[2024-09-29 11:26:09,860] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)

Authorized users only. All activities may be monitored and reported.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[2024-09-29 11:26:11,703] [INFO] [runner.py:463:main] Using IP address of 192.168.0.25 for node 192.168.0.25
[2024-09-29 11:26:11,704] [INFO] [multinode_runner.py:81:get_cmd] Running on the following workers: 192.168.0.25,192.168.0.149,192.168.0.89,192.168.0.13
[2024-09-29 11:26:11,704] [INFO] [runner.py:568:main] cmd = pdsh -S -f 1024 -w 192.168.0.25,192.168.0.149,192.168.0.89,192.168.0.13 export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/tools/aml/lib64:/usr/local/Ascend/ascend-toolkit/latest/tools/aml/lib64/plugin:/usr/local/Ascend/ascend-toolkit/latest/lib64:/usr/local/Ascend/ascend-toolkit/latest/lib64/plugin/opskernel:/usr/local/Ascend/ascend-toolkit/latest/lib64/plugin/nnengine:/usr/local/Ascend/ascend-toolkit/latest/opp/built-in/op_impl/ai_core/tbe/op_tiling/lib/linux/aarch64:/usr/local/Ascend/driver/lib64:/usr/local/Ascend/driver/lib64/common:/usr/local/Ascend/driver/lib64/driver:/usr/local/Ascend/ascend-toolkit/latest/tools/aml/lib64:/usr/local/Ascend/ascend-toolkit/latest/tools/aml/lib64/plugin:/usr/local/Ascend/ascend-toolkit/latest/lib64:/usr/local/Ascend/ascend-toolkit/latest/lib64/plugin/opskernel:/usr/local/Ascend/ascend-toolkit/latest/lib64/plugin/nnengine:/usr/local/Ascend/ascend-toolkit/latest/opp/built-in/op_impl/ai_core/tbe/op_tiling/lib/linux/aarch64:/usr/local/Ascend/driver/lib64:/usr/local/Ascend/driver/lib64/common:/usr/local/Ascend/driver/lib64/driver:/usr/local/Ascend/ascend-toolkit/latest/tools/aml/lib64:/usr/local/Ascend/ascend-toolkit/latest/tools/aml/lib64/plugin:/usr/local/Ascend/ascend-toolkit/latest/lib64:/usr/local/Ascend/ascend-toolkit/latest/lib64/plugin/opskernel:/usr/local/Ascend/ascend-toolkit/latest/lib64/plugin/nnengine:/usr/local/Ascend/ascend-toolkit/latest/opp/built-in/op_impl/ai_core/tbe/op_tiling/lib/linux/aarch64:/usr/local/Ascend/driver/lib64:/usr/local/Ascend/driver/lib64/common:/usr/local/Ascend/driver/lib64/driver:; export ASCEND_AICPU_PATH=/usr/local/Ascend/ascend-toolkit/latest; export ASCEND_TOOLKIT_HOME=/usr/local/Ascend/ascend-toolkit/latest; export ASCEND_OPP_PATH=/usr/local/Ascend/ascend-toolkit/latest/opp; export ASCEND_HOME_PATH=/usr/local/Ascend/ascend-toolkit/latest; export PYTHONPATH=/root:/usr/local/Ascend/ascend-toolkit/latest/python/site-packages:/usr/local/Ascend/ascend-toolkit/latest/opp/built-in/op_impl/ai_core/tbe:/usr/local/Ascend/ascend-toolkit/latest/python/site-packages:/usr/local/Ascend/ascend-toolkit/latest/opp/built-in/op_impl/ai_core/tbe:/usr/local/Ascend/ascend-toolkit/latest/python/site-packages:/usr/local/Ascend/ascend-toolkit/latest/opp/built-in/op_impl/ai_core/tbe:; export PATH=/usr/local/Ascend/ascend-toolkit/latest/bin:/usr/local/Ascend/ascend-toolkit/latest/compiler/ccec_compiler/bin:/usr/local/Ascend/ascend-toolkit/latest/tools/ccec_compiler/bin:/root/.vscode-server/cli/servers/Stable-38c31bc77e0dd6ae88a4e9cc93428cc27a56ba40/server/bin/remote-cli:/usr/local/Ascend/ascend-toolkit/latest/bin:/usr/local/Ascend/ascend-toolkit/latest/compiler/ccec_compiler/bin:/usr/local/Ascend/ascend-toolkit/latest/tools/ccec_compiler/bin:/root/miniconda3/envs/protein/bin:/root/miniconda3/condabin:/usr/local/Ascend/ascend-toolkit/latest/bin:/usr/local/Ascend/ascend-toolkit/latest/compiler/ccec_compiler/bin:/usr/local/Ascend/ascend-toolkit/latest/tools/ccec_compiler/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin;  cd /root; /root/miniconda3/envs/protein/bin/python -u -m deepspeed.launcher.launch --world_info=eyIxOTIuMTY4LjAuMjUiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN10sICIxOTIuMTY4LjAuMTQ5IjogWzAsIDEsIDIsIDMsIDQsIDUsIDYsIDddLCAiMTkyLjE2OC4wLjg5IjogWzAsIDEsIDIsIDMsIDQsIDUsIDYsIDddLCAiMTkyLjE2OC4wLjEzIjogWzAsIDEsIDIsIDMsIDQsIDUsIDYsIDddfQ== --node_rank=%n --master_addr=192.168.0.25 --master_port=29500 /root/fcl/LLaMA-Factory/src/train.py --deepspeed /root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json --stage pt --do_train --model_name_or_path /root/fcl/Meta-Llama-3-8B_512_tokenizer --dataset pt_512_c4_format --dataset_dir /root/fcl/LLaMA-Factory/data --finetuning_type full --resize_vocab --output_dir /root/fcl/LLaMA-Factory/saves/240929 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --max_samples 1000000 --preprocessing_num_workers 2 --per_device_train_batch_size 8 --per_device_eval_batch_size 1 --gradient_accumulation_steps 1 --lr_scheduler_type cosine --logging_steps 10 --warmup_steps 20 --warmup_ratio 0.1 --save_steps 1000 --eval_steps 500 --eval_strategy steps --learning_rate 1.0e-5 --num_train_epochs 3.0 --val_size 0.1 --ddp_timeout 180000000 --plot_loss --fp16
192.168.0.89: 
192.168.0.13: 
192.168.0.89: Authorized users only. All activities may be monitored and reported.
192.168.0.13: Authorized users only. All activities may be monitored and reported.
192.168.0.25: 
192.168.0.25: Authorized users only. All activities may be monitored and reported.
192.168.0.149: 
192.168.0.149: Authorized users only. All activities may be monitored and reported.
192.168.0.149: [2024-09-29 11:26:21,758] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.25: [2024-09-29 11:26:18,744] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.89: [2024-09-29 11:26:03,135] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.13: [2024-09-29 11:26:19,212] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.149: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.149: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.25: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.25: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.89: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.13: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.89: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.13: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.149: [2024-09-29 11:26:22,502] [INFO] [launch.py:146:main] WORLD INFO DICT: {'192.168.0.25': [0, 1, 2, 3, 4, 5, 6, 7], '192.168.0.149': [0, 1, 2, 3, 4, 5, 6, 7], '192.168.0.89': [0, 1, 2, 3, 4, 5, 6, 7], '192.168.0.13': [0, 1, 2, 3, 4, 5, 6, 7]}
192.168.0.149: [2024-09-29 11:26:22,502] [INFO] [launch.py:152:main] nnodes=4, num_local_procs=8, node_rank=1
192.168.0.149: [2024-09-29 11:26:22,502] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'192.168.0.25': [0, 1, 2, 3, 4, 5, 6, 7], '192.168.0.149': [8, 9, 10, 11, 12, 13, 14, 15], '192.168.0.89': [16, 17, 18, 19, 20, 21, 22, 23], '192.168.0.13': [24, 25, 26, 27, 28, 29, 30, 31]})
192.168.0.149: [2024-09-29 11:26:22,502] [INFO] [launch.py:164:main] dist_world_size=32
192.168.0.149: [2024-09-29 11:26:22,502] [INFO] [launch.py:168:main] Setting ASCEND_RT_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
192.168.0.149: [2024-09-29 11:26:22,503] [INFO] [launch.py:256:main] process 175635 spawned with command: ['/root/miniconda3/envs/protein/bin/python', '-u', '/root/fcl/LLaMA-Factory/src/train.py', '--local_rank=0', '--deepspeed', '/root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json', '--stage', 'pt', '--do_train', '--model_name_or_path', '/root/fcl/Meta-Llama-3-8B_512_tokenizer', '--dataset', 'pt_512_c4_format', '--dataset_dir', '/root/fcl/LLaMA-Factory/data', '--finetuning_type', 'full', '--resize_vocab', '--output_dir', '/root/fcl/LLaMA-Factory/saves/240929', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--max_samples', '1000000', '--preprocessing_num_workers', '2', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--warmup_ratio', '0.1', '--save_steps', '1000', '--eval_steps', '500', '--eval_strategy', 'steps', '--learning_rate', '1.0e-5', '--num_train_epochs', '3.0', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
192.168.0.149: [2024-09-29 11:26:22,503] [INFO] [launch.py:256:main] process 175636 spawned with command: ['/root/miniconda3/envs/protein/bin/python', '-u', '/root/fcl/LLaMA-Factory/src/train.py', '--local_rank=1', '--deepspeed', '/root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json', '--stage', 'pt', '--do_train', '--model_name_or_path', '/root/fcl/Meta-Llama-3-8B_512_tokenizer', '--dataset', 'pt_512_c4_format', '--dataset_dir', '/root/fcl/LLaMA-Factory/data', '--finetuning_type', 'full', '--resize_vocab', '--output_dir', '/root/fcl/LLaMA-Factory/saves/240929', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--max_samples', '1000000', '--preprocessing_num_workers', '2', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--warmup_ratio', '0.1', '--save_steps', '1000', '--eval_steps', '500', '--eval_strategy', 'steps', '--learning_rate', '1.0e-5', '--num_train_epochs', '3.0', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
192.168.0.149: [2024-09-29 11:26:22,504] [INFO] [launch.py:256:main] process 175637 spawned with command: ['/root/miniconda3/envs/protein/bin/python', '-u', '/root/fcl/LLaMA-Factory/src/train.py', '--local_rank=2', '--deepspeed', '/root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json', '--stage', 'pt', '--do_train', '--model_name_or_path', '/root/fcl/Meta-Llama-3-8B_512_tokenizer', '--dataset', 'pt_512_c4_format', '--dataset_dir', '/root/fcl/LLaMA-Factory/data', '--finetuning_type', 'full', '--resize_vocab', '--output_dir', '/root/fcl/LLaMA-Factory/saves/240929', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--max_samples', '1000000', '--preprocessing_num_workers', '2', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--warmup_ratio', '0.1', '--save_steps', '1000', '--eval_steps', '500', '--eval_strategy', 'steps', '--learning_rate', '1.0e-5', '--num_train_epochs', '3.0', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
192.168.0.149: [2024-09-29 11:26:22,504] [INFO] [launch.py:256:main] process 175638 spawned with command: ['/root/miniconda3/envs/protein/bin/python', '-u', '/root/fcl/LLaMA-Factory/src/train.py', '--local_rank=3', '--deepspeed', '/root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json', '--stage', 'pt', '--do_train', '--model_name_or_path', '/root/fcl/Meta-Llama-3-8B_512_tokenizer', '--dataset', 'pt_512_c4_format', '--dataset_dir', '/root/fcl/LLaMA-Factory/data', '--finetuning_type', 'full', '--resize_vocab', '--output_dir', '/root/fcl/LLaMA-Factory/saves/240929', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--max_samples', '1000000', '--preprocessing_num_workers', '2', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--warmup_ratio', '0.1', '--save_steps', '1000', '--eval_steps', '500', '--eval_strategy', 'steps', '--learning_rate', '1.0e-5', '--num_train_epochs', '3.0', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
192.168.0.149: [2024-09-29 11:26:22,505] [INFO] [launch.py:256:main] process 175639 spawned with command: ['/root/miniconda3/envs/protein/bin/python', '-u', '/root/fcl/LLaMA-Factory/src/train.py', '--local_rank=4', '--deepspeed', '/root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json', '--stage', 'pt', '--do_train', '--model_name_or_path', '/root/fcl/Meta-Llama-3-8B_512_tokenizer', '--dataset', 'pt_512_c4_format', '--dataset_dir', '/root/fcl/LLaMA-Factory/data', '--finetuning_type', 'full', '--resize_vocab', '--output_dir', '/root/fcl/LLaMA-Factory/saves/240929', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--max_samples', '1000000', '--preprocessing_num_workers', '2', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--warmup_ratio', '0.1', '--save_steps', '1000', '--eval_steps', '500', '--eval_strategy', 'steps', '--learning_rate', '1.0e-5', '--num_train_epochs', '3.0', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
192.168.0.149: [2024-09-29 11:26:22,505] [INFO] [launch.py:256:main] process 175640 spawned with command: ['/root/miniconda3/envs/protein/bin/python', '-u', '/root/fcl/LLaMA-Factory/src/train.py', '--local_rank=5', '--deepspeed', '/root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json', '--stage', 'pt', '--do_train', '--model_name_or_path', '/root/fcl/Meta-Llama-3-8B_512_tokenizer', '--dataset', 'pt_512_c4_format', '--dataset_dir', '/root/fcl/LLaMA-Factory/data', '--finetuning_type', 'full', '--resize_vocab', '--output_dir', '/root/fcl/LLaMA-Factory/saves/240929', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--max_samples', '1000000', '--preprocessing_num_workers', '2', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--warmup_ratio', '0.1', '--save_steps', '1000', '--eval_steps', '500', '--eval_strategy', 'steps', '--learning_rate', '1.0e-5', '--num_train_epochs', '3.0', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
192.168.0.149: [2024-09-29 11:26:22,506] [INFO] [launch.py:256:main] process 175641 spawned with command: ['/root/miniconda3/envs/protein/bin/python', '-u', '/root/fcl/LLaMA-Factory/src/train.py', '--local_rank=6', '--deepspeed', '/root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json', '--stage', 'pt', '--do_train', '--model_name_or_path', '/root/fcl/Meta-Llama-3-8B_512_tokenizer', '--dataset', 'pt_512_c4_format', '--dataset_dir', '/root/fcl/LLaMA-Factory/data', '--finetuning_type', 'full', '--resize_vocab', '--output_dir', '/root/fcl/LLaMA-Factory/saves/240929', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--max_samples', '1000000', '--preprocessing_num_workers', '2', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--warmup_ratio', '0.1', '--save_steps', '1000', '--eval_steps', '500', '--eval_strategy', 'steps', '--learning_rate', '1.0e-5', '--num_train_epochs', '3.0', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
192.168.0.149: [2024-09-29 11:26:22,506] [INFO] [launch.py:256:main] process 175642 spawned with command: ['/root/miniconda3/envs/protein/bin/python', '-u', '/root/fcl/LLaMA-Factory/src/train.py', '--local_rank=7', '--deepspeed', '/root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json', '--stage', 'pt', '--do_train', '--model_name_or_path', '/root/fcl/Meta-Llama-3-8B_512_tokenizer', '--dataset', 'pt_512_c4_format', '--dataset_dir', '/root/fcl/LLaMA-Factory/data', '--finetuning_type', 'full', '--resize_vocab', '--output_dir', '/root/fcl/LLaMA-Factory/saves/240929', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--max_samples', '1000000', '--preprocessing_num_workers', '2', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--warmup_ratio', '0.1', '--save_steps', '1000', '--eval_steps', '500', '--eval_strategy', 'steps', '--learning_rate', '1.0e-5', '--num_train_epochs', '3.0', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
192.168.0.25: [2024-09-29 11:26:19,520] [INFO] [launch.py:146:main] WORLD INFO DICT: {'192.168.0.25': [0, 1, 2, 3, 4, 5, 6, 7], '192.168.0.149': [0, 1, 2, 3, 4, 5, 6, 7], '192.168.0.89': [0, 1, 2, 3, 4, 5, 6, 7], '192.168.0.13': [0, 1, 2, 3, 4, 5, 6, 7]}
192.168.0.25: [2024-09-29 11:26:19,520] [INFO] [launch.py:152:main] nnodes=4, num_local_procs=8, node_rank=0
192.168.0.25: [2024-09-29 11:26:19,521] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'192.168.0.25': [0, 1, 2, 3, 4, 5, 6, 7], '192.168.0.149': [8, 9, 10, 11, 12, 13, 14, 15], '192.168.0.89': [16, 17, 18, 19, 20, 21, 22, 23], '192.168.0.13': [24, 25, 26, 27, 28, 29, 30, 31]})
192.168.0.25: [2024-09-29 11:26:19,521] [INFO] [launch.py:164:main] dist_world_size=32
192.168.0.25: [2024-09-29 11:26:19,521] [INFO] [launch.py:168:main] Setting ASCEND_RT_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
192.168.0.25: [2024-09-29 11:26:19,521] [INFO] [launch.py:256:main] process 50916 spawned with command: ['/root/miniconda3/envs/protein/bin/python', '-u', '/root/fcl/LLaMA-Factory/src/train.py', '--local_rank=0', '--deepspeed', '/root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json', '--stage', 'pt', '--do_train', '--model_name_or_path', '/root/fcl/Meta-Llama-3-8B_512_tokenizer', '--dataset', 'pt_512_c4_format', '--dataset_dir', '/root/fcl/LLaMA-Factory/data', '--finetuning_type', 'full', '--resize_vocab', '--output_dir', '/root/fcl/LLaMA-Factory/saves/240929', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--max_samples', '1000000', '--preprocessing_num_workers', '2', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--warmup_ratio', '0.1', '--save_steps', '1000', '--eval_steps', '500', '--eval_strategy', 'steps', '--learning_rate', '1.0e-5', '--num_train_epochs', '3.0', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
192.168.0.25: [2024-09-29 11:26:19,522] [INFO] [launch.py:256:main] process 50917 spawned with command: ['/root/miniconda3/envs/protein/bin/python', '-u', '/root/fcl/LLaMA-Factory/src/train.py', '--local_rank=1', '--deepspeed', '/root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json', '--stage', 'pt', '--do_train', '--model_name_or_path', '/root/fcl/Meta-Llama-3-8B_512_tokenizer', '--dataset', 'pt_512_c4_format', '--dataset_dir', '/root/fcl/LLaMA-Factory/data', '--finetuning_type', 'full', '--resize_vocab', '--output_dir', '/root/fcl/LLaMA-Factory/saves/240929', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--max_samples', '1000000', '--preprocessing_num_workers', '2', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--warmup_ratio', '0.1', '--save_steps', '1000', '--eval_steps', '500', '--eval_strategy', 'steps', '--learning_rate', '1.0e-5', '--num_train_epochs', '3.0', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
192.168.0.25: [2024-09-29 11:26:19,522] [INFO] [launch.py:256:main] process 50918 spawned with command: ['/root/miniconda3/envs/protein/bin/python', '-u', '/root/fcl/LLaMA-Factory/src/train.py', '--local_rank=2', '--deepspeed', '/root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json', '--stage', 'pt', '--do_train', '--model_name_or_path', '/root/fcl/Meta-Llama-3-8B_512_tokenizer', '--dataset', 'pt_512_c4_format', '--dataset_dir', '/root/fcl/LLaMA-Factory/data', '--finetuning_type', 'full', '--resize_vocab', '--output_dir', '/root/fcl/LLaMA-Factory/saves/240929', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--max_samples', '1000000', '--preprocessing_num_workers', '2', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--warmup_ratio', '0.1', '--save_steps', '1000', '--eval_steps', '500', '--eval_strategy', 'steps', '--learning_rate', '1.0e-5', '--num_train_epochs', '3.0', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
192.168.0.25: [2024-09-29 11:26:19,523] [INFO] [launch.py:256:main] process 50919 spawned with command: ['/root/miniconda3/envs/protein/bin/python', '-u', '/root/fcl/LLaMA-Factory/src/train.py', '--local_rank=3', '--deepspeed', '/root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json', '--stage', 'pt', '--do_train', '--model_name_or_path', '/root/fcl/Meta-Llama-3-8B_512_tokenizer', '--dataset', 'pt_512_c4_format', '--dataset_dir', '/root/fcl/LLaMA-Factory/data', '--finetuning_type', 'full', '--resize_vocab', '--output_dir', '/root/fcl/LLaMA-Factory/saves/240929', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--max_samples', '1000000', '--preprocessing_num_workers', '2', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--warmup_ratio', '0.1', '--save_steps', '1000', '--eval_steps', '500', '--eval_strategy', 'steps', '--learning_rate', '1.0e-5', '--num_train_epochs', '3.0', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
192.168.0.25: [2024-09-29 11:26:19,523] [INFO] [launch.py:256:main] process 50920 spawned with command: ['/root/miniconda3/envs/protein/bin/python', '-u', '/root/fcl/LLaMA-Factory/src/train.py', '--local_rank=4', '--deepspeed', '/root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json', '--stage', 'pt', '--do_train', '--model_name_or_path', '/root/fcl/Meta-Llama-3-8B_512_tokenizer', '--dataset', 'pt_512_c4_format', '--dataset_dir', '/root/fcl/LLaMA-Factory/data', '--finetuning_type', 'full', '--resize_vocab', '--output_dir', '/root/fcl/LLaMA-Factory/saves/240929', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--max_samples', '1000000', '--preprocessing_num_workers', '2', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--warmup_ratio', '0.1', '--save_steps', '1000', '--eval_steps', '500', '--eval_strategy', 'steps', '--learning_rate', '1.0e-5', '--num_train_epochs', '3.0', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
192.168.0.25: [2024-09-29 11:26:19,524] [INFO] [launch.py:256:main] process 50921 spawned with command: ['/root/miniconda3/envs/protein/bin/python', '-u', '/root/fcl/LLaMA-Factory/src/train.py', '--local_rank=5', '--deepspeed', '/root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json', '--stage', 'pt', '--do_train', '--model_name_or_path', '/root/fcl/Meta-Llama-3-8B_512_tokenizer', '--dataset', 'pt_512_c4_format', '--dataset_dir', '/root/fcl/LLaMA-Factory/data', '--finetuning_type', 'full', '--resize_vocab', '--output_dir', '/root/fcl/LLaMA-Factory/saves/240929', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--max_samples', '1000000', '--preprocessing_num_workers', '2', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--warmup_ratio', '0.1', '--save_steps', '1000', '--eval_steps', '500', '--eval_strategy', 'steps', '--learning_rate', '1.0e-5', '--num_train_epochs', '3.0', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
192.168.0.25: [2024-09-29 11:26:19,524] [INFO] [launch.py:256:main] process 50922 spawned with command: ['/root/miniconda3/envs/protein/bin/python', '-u', '/root/fcl/LLaMA-Factory/src/train.py', '--local_rank=6', '--deepspeed', '/root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json', '--stage', 'pt', '--do_train', '--model_name_or_path', '/root/fcl/Meta-Llama-3-8B_512_tokenizer', '--dataset', 'pt_512_c4_format', '--dataset_dir', '/root/fcl/LLaMA-Factory/data', '--finetuning_type', 'full', '--resize_vocab', '--output_dir', '/root/fcl/LLaMA-Factory/saves/240929', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--max_samples', '1000000', '--preprocessing_num_workers', '2', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--warmup_ratio', '0.1', '--save_steps', '1000', '--eval_steps', '500', '--eval_strategy', 'steps', '--learning_rate', '1.0e-5', '--num_train_epochs', '3.0', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
192.168.0.25: [2024-09-29 11:26:19,525] [INFO] [launch.py:256:main] process 50923 spawned with command: ['/root/miniconda3/envs/protein/bin/python', '-u', '/root/fcl/LLaMA-Factory/src/train.py', '--local_rank=7', '--deepspeed', '/root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json', '--stage', 'pt', '--do_train', '--model_name_or_path', '/root/fcl/Meta-Llama-3-8B_512_tokenizer', '--dataset', 'pt_512_c4_format', '--dataset_dir', '/root/fcl/LLaMA-Factory/data', '--finetuning_type', 'full', '--resize_vocab', '--output_dir', '/root/fcl/LLaMA-Factory/saves/240929', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--max_samples', '1000000', '--preprocessing_num_workers', '2', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--warmup_ratio', '0.1', '--save_steps', '1000', '--eval_steps', '500', '--eval_strategy', 'steps', '--learning_rate', '1.0e-5', '--num_train_epochs', '3.0', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
192.168.0.89: [2024-09-29 11:26:03,908] [INFO] [launch.py:146:main] WORLD INFO DICT: {'192.168.0.25': [0, 1, 2, 3, 4, 5, 6, 7], '192.168.0.149': [0, 1, 2, 3, 4, 5, 6, 7], '192.168.0.89': [0, 1, 2, 3, 4, 5, 6, 7], '192.168.0.13': [0, 1, 2, 3, 4, 5, 6, 7]}
192.168.0.89: [2024-09-29 11:26:03,908] [INFO] [launch.py:152:main] nnodes=4, num_local_procs=8, node_rank=2
192.168.0.89: [2024-09-29 11:26:03,908] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'192.168.0.25': [0, 1, 2, 3, 4, 5, 6, 7], '192.168.0.149': [8, 9, 10, 11, 12, 13, 14, 15], '192.168.0.89': [16, 17, 18, 19, 20, 21, 22, 23], '192.168.0.13': [24, 25, 26, 27, 28, 29, 30, 31]})
192.168.0.89: [2024-09-29 11:26:03,908] [INFO] [launch.py:164:main] dist_world_size=32
192.168.0.89: [2024-09-29 11:26:03,908] [INFO] [launch.py:168:main] Setting ASCEND_RT_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
192.168.0.89: [2024-09-29 11:26:03,909] [INFO] [launch.py:256:main] process 24153 spawned with command: ['/root/miniconda3/envs/protein/bin/python', '-u', '/root/fcl/LLaMA-Factory/src/train.py', '--local_rank=0', '--deepspeed', '/root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json', '--stage', 'pt', '--do_train', '--model_name_or_path', '/root/fcl/Meta-Llama-3-8B_512_tokenizer', '--dataset', 'pt_512_c4_format', '--dataset_dir', '/root/fcl/LLaMA-Factory/data', '--finetuning_type', 'full', '--resize_vocab', '--output_dir', '/root/fcl/LLaMA-Factory/saves/240929', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--max_samples', '1000000', '--preprocessing_num_workers', '2', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--warmup_ratio', '0.1', '--save_steps', '1000', '--eval_steps', '500', '--eval_strategy', 'steps', '--learning_rate', '1.0e-5', '--num_train_epochs', '3.0', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
192.168.0.89: [2024-09-29 11:26:03,910] [INFO] [launch.py:256:main] process 24154 spawned with command: ['/root/miniconda3/envs/protein/bin/python', '-u', '/root/fcl/LLaMA-Factory/src/train.py', '--local_rank=1', '--deepspeed', '/root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json', '--stage', 'pt', '--do_train', '--model_name_or_path', '/root/fcl/Meta-Llama-3-8B_512_tokenizer', '--dataset', 'pt_512_c4_format', '--dataset_dir', '/root/fcl/LLaMA-Factory/data', '--finetuning_type', 'full', '--resize_vocab', '--output_dir', '/root/fcl/LLaMA-Factory/saves/240929', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--max_samples', '1000000', '--preprocessing_num_workers', '2', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--warmup_ratio', '0.1', '--save_steps', '1000', '--eval_steps', '500', '--eval_strategy', 'steps', '--learning_rate', '1.0e-5', '--num_train_epochs', '3.0', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
192.168.0.89: [2024-09-29 11:26:03,910] [INFO] [launch.py:256:main] process 24155 spawned with command: ['/root/miniconda3/envs/protein/bin/python', '-u', '/root/fcl/LLaMA-Factory/src/train.py', '--local_rank=2', '--deepspeed', '/root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json', '--stage', 'pt', '--do_train', '--model_name_or_path', '/root/fcl/Meta-Llama-3-8B_512_tokenizer', '--dataset', 'pt_512_c4_format', '--dataset_dir', '/root/fcl/LLaMA-Factory/data', '--finetuning_type', 'full', '--resize_vocab', '--output_dir', '/root/fcl/LLaMA-Factory/saves/240929', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--max_samples', '1000000', '--preprocessing_num_workers', '2', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--warmup_ratio', '0.1', '--save_steps', '1000', '--eval_steps', '500', '--eval_strategy', 'steps', '--learning_rate', '1.0e-5', '--num_train_epochs', '3.0', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
192.168.0.89: [2024-09-29 11:26:03,910] [INFO] [launch.py:256:main] process 24156 spawned with command: ['/root/miniconda3/envs/protein/bin/python', '-u', '/root/fcl/LLaMA-Factory/src/train.py', '--local_rank=3', '--deepspeed', '/root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json', '--stage', 'pt', '--do_train', '--model_name_or_path', '/root/fcl/Meta-Llama-3-8B_512_tokenizer', '--dataset', 'pt_512_c4_format', '--dataset_dir', '/root/fcl/LLaMA-Factory/data', '--finetuning_type', 'full', '--resize_vocab', '--output_dir', '/root/fcl/LLaMA-Factory/saves/240929', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--max_samples', '1000000', '--preprocessing_num_workers', '2', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--warmup_ratio', '0.1', '--save_steps', '1000', '--eval_steps', '500', '--eval_strategy', 'steps', '--learning_rate', '1.0e-5', '--num_train_epochs', '3.0', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
192.168.0.89: [2024-09-29 11:26:03,911] [INFO] [launch.py:256:main] process 24157 spawned with command: ['/root/miniconda3/envs/protein/bin/python', '-u', '/root/fcl/LLaMA-Factory/src/train.py', '--local_rank=4', '--deepspeed', '/root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json', '--stage', 'pt', '--do_train', '--model_name_or_path', '/root/fcl/Meta-Llama-3-8B_512_tokenizer', '--dataset', 'pt_512_c4_format', '--dataset_dir', '/root/fcl/LLaMA-Factory/data', '--finetuning_type', 'full', '--resize_vocab', '--output_dir', '/root/fcl/LLaMA-Factory/saves/240929', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--max_samples', '1000000', '--preprocessing_num_workers', '2', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--warmup_ratio', '0.1', '--save_steps', '1000', '--eval_steps', '500', '--eval_strategy', 'steps', '--learning_rate', '1.0e-5', '--num_train_epochs', '3.0', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
192.168.0.89: [2024-09-29 11:26:03,911] [INFO] [launch.py:256:main] process 24158 spawned with command: ['/root/miniconda3/envs/protein/bin/python', '-u', '/root/fcl/LLaMA-Factory/src/train.py', '--local_rank=5', '--deepspeed', '/root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json', '--stage', 'pt', '--do_train', '--model_name_or_path', '/root/fcl/Meta-Llama-3-8B_512_tokenizer', '--dataset', 'pt_512_c4_format', '--dataset_dir', '/root/fcl/LLaMA-Factory/data', '--finetuning_type', 'full', '--resize_vocab', '--output_dir', '/root/fcl/LLaMA-Factory/saves/240929', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--max_samples', '1000000', '--preprocessing_num_workers', '2', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--warmup_ratio', '0.1', '--save_steps', '1000', '--eval_steps', '500', '--eval_strategy', 'steps', '--learning_rate', '1.0e-5', '--num_train_epochs', '3.0', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
192.168.0.89: [2024-09-29 11:26:03,912] [INFO] [launch.py:256:main] process 24159 spawned with command: ['/root/miniconda3/envs/protein/bin/python', '-u', '/root/fcl/LLaMA-Factory/src/train.py', '--local_rank=6', '--deepspeed', '/root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json', '--stage', 'pt', '--do_train', '--model_name_or_path', '/root/fcl/Meta-Llama-3-8B_512_tokenizer', '--dataset', 'pt_512_c4_format', '--dataset_dir', '/root/fcl/LLaMA-Factory/data', '--finetuning_type', 'full', '--resize_vocab', '--output_dir', '/root/fcl/LLaMA-Factory/saves/240929', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--max_samples', '1000000', '--preprocessing_num_workers', '2', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--warmup_ratio', '0.1', '--save_steps', '1000', '--eval_steps', '500', '--eval_strategy', 'steps', '--learning_rate', '1.0e-5', '--num_train_epochs', '3.0', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
192.168.0.89: [2024-09-29 11:26:03,912] [INFO] [launch.py:256:main] process 24160 spawned with command: ['/root/miniconda3/envs/protein/bin/python', '-u', '/root/fcl/LLaMA-Factory/src/train.py', '--local_rank=7', '--deepspeed', '/root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json', '--stage', 'pt', '--do_train', '--model_name_or_path', '/root/fcl/Meta-Llama-3-8B_512_tokenizer', '--dataset', 'pt_512_c4_format', '--dataset_dir', '/root/fcl/LLaMA-Factory/data', '--finetuning_type', 'full', '--resize_vocab', '--output_dir', '/root/fcl/LLaMA-Factory/saves/240929', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--max_samples', '1000000', '--preprocessing_num_workers', '2', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--warmup_ratio', '0.1', '--save_steps', '1000', '--eval_steps', '500', '--eval_strategy', 'steps', '--learning_rate', '1.0e-5', '--num_train_epochs', '3.0', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
192.168.0.13: [2024-09-29 11:26:19,991] [INFO] [launch.py:146:main] WORLD INFO DICT: {'192.168.0.25': [0, 1, 2, 3, 4, 5, 6, 7], '192.168.0.149': [0, 1, 2, 3, 4, 5, 6, 7], '192.168.0.89': [0, 1, 2, 3, 4, 5, 6, 7], '192.168.0.13': [0, 1, 2, 3, 4, 5, 6, 7]}
192.168.0.13: [2024-09-29 11:26:19,991] [INFO] [launch.py:152:main] nnodes=4, num_local_procs=8, node_rank=3
192.168.0.13: [2024-09-29 11:26:19,991] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'192.168.0.25': [0, 1, 2, 3, 4, 5, 6, 7], '192.168.0.149': [8, 9, 10, 11, 12, 13, 14, 15], '192.168.0.89': [16, 17, 18, 19, 20, 21, 22, 23], '192.168.0.13': [24, 25, 26, 27, 28, 29, 30, 31]})
192.168.0.13: [2024-09-29 11:26:19,991] [INFO] [launch.py:164:main] dist_world_size=32
192.168.0.13: [2024-09-29 11:26:19,991] [INFO] [launch.py:168:main] Setting ASCEND_RT_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
192.168.0.13: [2024-09-29 11:26:19,992] [INFO] [launch.py:256:main] process 85536 spawned with command: ['/root/miniconda3/envs/protein/bin/python', '-u', '/root/fcl/LLaMA-Factory/src/train.py', '--local_rank=0', '--deepspeed', '/root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json', '--stage', 'pt', '--do_train', '--model_name_or_path', '/root/fcl/Meta-Llama-3-8B_512_tokenizer', '--dataset', 'pt_512_c4_format', '--dataset_dir', '/root/fcl/LLaMA-Factory/data', '--finetuning_type', 'full', '--resize_vocab', '--output_dir', '/root/fcl/LLaMA-Factory/saves/240929', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--max_samples', '1000000', '--preprocessing_num_workers', '2', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--warmup_ratio', '0.1', '--save_steps', '1000', '--eval_steps', '500', '--eval_strategy', 'steps', '--learning_rate', '1.0e-5', '--num_train_epochs', '3.0', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
192.168.0.13: [2024-09-29 11:26:19,992] [INFO] [launch.py:256:main] process 85537 spawned with command: ['/root/miniconda3/envs/protein/bin/python', '-u', '/root/fcl/LLaMA-Factory/src/train.py', '--local_rank=1', '--deepspeed', '/root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json', '--stage', 'pt', '--do_train', '--model_name_or_path', '/root/fcl/Meta-Llama-3-8B_512_tokenizer', '--dataset', 'pt_512_c4_format', '--dataset_dir', '/root/fcl/LLaMA-Factory/data', '--finetuning_type', 'full', '--resize_vocab', '--output_dir', '/root/fcl/LLaMA-Factory/saves/240929', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--max_samples', '1000000', '--preprocessing_num_workers', '2', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--warmup_ratio', '0.1', '--save_steps', '1000', '--eval_steps', '500', '--eval_strategy', 'steps', '--learning_rate', '1.0e-5', '--num_train_epochs', '3.0', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
192.168.0.13: [2024-09-29 11:26:19,993] [INFO] [launch.py:256:main] process 85538 spawned with command: ['/root/miniconda3/envs/protein/bin/python', '-u', '/root/fcl/LLaMA-Factory/src/train.py', '--local_rank=2', '--deepspeed', '/root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json', '--stage', 'pt', '--do_train', '--model_name_or_path', '/root/fcl/Meta-Llama-3-8B_512_tokenizer', '--dataset', 'pt_512_c4_format', '--dataset_dir', '/root/fcl/LLaMA-Factory/data', '--finetuning_type', 'full', '--resize_vocab', '--output_dir', '/root/fcl/LLaMA-Factory/saves/240929', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--max_samples', '1000000', '--preprocessing_num_workers', '2', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--warmup_ratio', '0.1', '--save_steps', '1000', '--eval_steps', '500', '--eval_strategy', 'steps', '--learning_rate', '1.0e-5', '--num_train_epochs', '3.0', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
192.168.0.13: [2024-09-29 11:26:19,993] [INFO] [launch.py:256:main] process 85539 spawned with command: ['/root/miniconda3/envs/protein/bin/python', '-u', '/root/fcl/LLaMA-Factory/src/train.py', '--local_rank=3', '--deepspeed', '/root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json', '--stage', 'pt', '--do_train', '--model_name_or_path', '/root/fcl/Meta-Llama-3-8B_512_tokenizer', '--dataset', 'pt_512_c4_format', '--dataset_dir', '/root/fcl/LLaMA-Factory/data', '--finetuning_type', 'full', '--resize_vocab', '--output_dir', '/root/fcl/LLaMA-Factory/saves/240929', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--max_samples', '1000000', '--preprocessing_num_workers', '2', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--warmup_ratio', '0.1', '--save_steps', '1000', '--eval_steps', '500', '--eval_strategy', 'steps', '--learning_rate', '1.0e-5', '--num_train_epochs', '3.0', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
192.168.0.13: [2024-09-29 11:26:19,994] [INFO] [launch.py:256:main] process 85540 spawned with command: ['/root/miniconda3/envs/protein/bin/python', '-u', '/root/fcl/LLaMA-Factory/src/train.py', '--local_rank=4', '--deepspeed', '/root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json', '--stage', 'pt', '--do_train', '--model_name_or_path', '/root/fcl/Meta-Llama-3-8B_512_tokenizer', '--dataset', 'pt_512_c4_format', '--dataset_dir', '/root/fcl/LLaMA-Factory/data', '--finetuning_type', 'full', '--resize_vocab', '--output_dir', '/root/fcl/LLaMA-Factory/saves/240929', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--max_samples', '1000000', '--preprocessing_num_workers', '2', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--warmup_ratio', '0.1', '--save_steps', '1000', '--eval_steps', '500', '--eval_strategy', 'steps', '--learning_rate', '1.0e-5', '--num_train_epochs', '3.0', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
192.168.0.13: [2024-09-29 11:26:19,994] [INFO] [launch.py:256:main] process 85541 spawned with command: ['/root/miniconda3/envs/protein/bin/python', '-u', '/root/fcl/LLaMA-Factory/src/train.py', '--local_rank=5', '--deepspeed', '/root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json', '--stage', 'pt', '--do_train', '--model_name_or_path', '/root/fcl/Meta-Llama-3-8B_512_tokenizer', '--dataset', 'pt_512_c4_format', '--dataset_dir', '/root/fcl/LLaMA-Factory/data', '--finetuning_type', 'full', '--resize_vocab', '--output_dir', '/root/fcl/LLaMA-Factory/saves/240929', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--max_samples', '1000000', '--preprocessing_num_workers', '2', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--warmup_ratio', '0.1', '--save_steps', '1000', '--eval_steps', '500', '--eval_strategy', 'steps', '--learning_rate', '1.0e-5', '--num_train_epochs', '3.0', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
192.168.0.13: [2024-09-29 11:26:19,995] [INFO] [launch.py:256:main] process 85542 spawned with command: ['/root/miniconda3/envs/protein/bin/python', '-u', '/root/fcl/LLaMA-Factory/src/train.py', '--local_rank=6', '--deepspeed', '/root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json', '--stage', 'pt', '--do_train', '--model_name_or_path', '/root/fcl/Meta-Llama-3-8B_512_tokenizer', '--dataset', 'pt_512_c4_format', '--dataset_dir', '/root/fcl/LLaMA-Factory/data', '--finetuning_type', 'full', '--resize_vocab', '--output_dir', '/root/fcl/LLaMA-Factory/saves/240929', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--max_samples', '1000000', '--preprocessing_num_workers', '2', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--warmup_ratio', '0.1', '--save_steps', '1000', '--eval_steps', '500', '--eval_strategy', 'steps', '--learning_rate', '1.0e-5', '--num_train_epochs', '3.0', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
192.168.0.13: [2024-09-29 11:26:19,995] [INFO] [launch.py:256:main] process 85543 spawned with command: ['/root/miniconda3/envs/protein/bin/python', '-u', '/root/fcl/LLaMA-Factory/src/train.py', '--local_rank=7', '--deepspeed', '/root/fcl/LLaMA-Factory/examples/deepspeed/ds_z3_config.json', '--stage', 'pt', '--do_train', '--model_name_or_path', '/root/fcl/Meta-Llama-3-8B_512_tokenizer', '--dataset', 'pt_512_c4_format', '--dataset_dir', '/root/fcl/LLaMA-Factory/data', '--finetuning_type', 'full', '--resize_vocab', '--output_dir', '/root/fcl/LLaMA-Factory/saves/240929', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--max_samples', '1000000', '--preprocessing_num_workers', '2', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--warmup_ratio', '0.1', '--save_steps', '1000', '--eval_steps', '500', '--eval_strategy', 'steps', '--learning_rate', '1.0e-5', '--num_train_epochs', '3.0', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
192.168.0.89: [2024-09-29 11:26:24,912] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.13: [2024-09-29 11:26:41,078] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.89: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.89: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.13: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.13: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.25: [2024-09-29 11:26:41,445] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.149: [2024-09-29 11:26:44,573] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.25: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.25: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.13: [2024-09-29 11:26:42,135] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.149: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.149: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.13: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.13: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.89: [2024-09-29 11:26:26,224] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.89: [2024-09-29 11:26:26,263] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.89: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.89: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.89: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.89: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.89: [2024-09-29 11:26:26,556] [INFO] [comm.py:637:init_distributed] cdb=None
192.168.0.13: [2024-09-29 11:26:42,786] [INFO] [comm.py:637:init_distributed] cdb=None
192.168.0.13: [2024-09-29 11:26:42,814] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.13: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.13: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.25: [2024-09-29 11:26:42,887] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.25: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.25: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.89: [2024-09-29 11:26:27,943] [INFO] [comm.py:637:init_distributed] cdb=None
192.168.0.13: [2024-09-29 11:26:44,329] [INFO] [comm.py:637:init_distributed] cdb=None
192.168.0.25: <frozen importlib._bootstrap>:914: ImportWarning: TEMetaPathFinder.find_spec() not found; falling back to find_module()
192.168.0.25: [2024-09-29 11:26:44,011] [INFO] [comm.py:637:init_distributed] cdb=None
192.168.0.13: [2024-09-29 11:26:44,494] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.25: [2024-09-29 11:26:44,080] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.13: [2024-09-29 11:26:44,567] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.89: [2024-09-29 11:26:28,494] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.89: [2024-09-29 11:26:28,532] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.13: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.13: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.25: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.25: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.13: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.89: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.13: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.89: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.89: [2024-09-29 11:26:28,682] [INFO] [comm.py:637:init_distributed] cdb=None
192.168.0.89: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.149: [2024-09-29 11:26:47,356] [INFO] [comm.py:637:init_distributed] cdb=None
192.168.0.89: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.13: [2024-09-29 11:26:44,813] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.149: [2024-09-29 11:26:47,419] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.25: [2024-09-29 11:26:44,402] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.13: [2024-09-29 11:26:44,910] [INFO] [comm.py:637:init_distributed] cdb=None
192.168.0.13: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.13: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.149: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.25: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.149: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.25: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.149: [2024-09-29 11:26:47,709] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.149: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.149: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.13: [2024-09-29 11:26:45,364] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.149: [2024-09-29 11:26:47,980] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.13: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.25: <frozen importlib._bootstrap>:914: ImportWarning: TEMetaPathFinder.find_spec() not found; falling back to find_module()
192.168.0.25: [2024-09-29 11:26:45,067] [INFO] [comm.py:637:init_distributed] cdb=None
192.168.0.13: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.149: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.149: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.89: [2024-09-29 11:26:29,641] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.13: [2024-09-29 11:26:45,750] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.89: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.89: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.13: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.13: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.13: [2024-09-29 11:26:46,202] [INFO] [comm.py:637:init_distributed] cdb=None
192.168.0.25: [2024-09-29 11:26:45,766] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.89: [2024-09-29 11:26:30,204] [INFO] [comm.py:637:init_distributed] cdb=None
192.168.0.89: [2024-09-29 11:26:30,250] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.25: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.25: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.13: [2024-09-29 11:26:46,433] [INFO] [comm.py:637:init_distributed] cdb=None
192.168.0.89: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.89: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.13: [2024-09-29 11:26:46,494] [INFO] [comm.py:637:init_distributed] cdb=None
192.168.0.25: [2024-09-29 11:26:46,082] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.25: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.25: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.25: [2024-09-29 11:26:46,341] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.89: [2024-09-29 11:26:30,719] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.89: [2024-09-29 11:26:30,808] [INFO] [comm.py:637:init_distributed] cdb=None
192.168.0.25: <frozen importlib._bootstrap>:914: ImportWarning: TEMetaPathFinder.find_spec() not found; falling back to find_module()
192.168.0.25: [2024-09-29 11:26:46,461] [INFO] [comm.py:637:init_distributed] cdb=None
192.168.0.25: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.89: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.25: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.89: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.25: <frozen importlib._bootstrap>:914: ImportWarning: TEMetaPathFinder.find_spec() not found; falling back to find_module()
192.168.0.25: [2024-09-29 11:26:46,525] [INFO] [comm.py:637:init_distributed] cdb=None
192.168.0.25: [2024-09-29 11:26:46,571] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.149: [2024-09-29 11:26:49,598] [INFO] [comm.py:637:init_distributed] cdb=None
192.168.0.149: [2024-09-29 11:26:49,614] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.13: [2024-09-29 11:26:47,114] [INFO] [comm.py:637:init_distributed] cdb=None
192.168.0.25: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.25: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.149: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.149: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.149: [2024-09-29 11:26:49,901] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.149: [2024-09-29 11:26:49,906] [INFO] [comm.py:637:init_distributed] cdb=None
192.168.0.89: [2024-09-29 11:26:31,263] [INFO] [comm.py:637:init_distributed] cdb=None
192.168.0.149: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.149: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.149: [2024-09-29 11:26:50,069] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.149: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.149: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.149: [2024-09-29 11:26:50,472] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
192.168.0.89: [2024-09-29 11:26:31,898] [INFO] [comm.py:637:init_distributed] cdb=None
192.168.0.13: [2024-09-29 11:26:48,007] [INFO] [comm.py:637:init_distributed] cdb=None
192.168.0.25: <frozen importlib._bootstrap>:914: ImportWarning: TEMetaPathFinder.find_spec() not found; falling back to find_module()
192.168.0.149: [2024-09-29 11:26:50,593] [INFO] [comm.py:637:init_distributed] cdb=None
192.168.0.25: [2024-09-29 11:26:47,573] [INFO] [comm.py:637:init_distributed] cdb=None
192.168.0.149: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
192.168.0.149: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
192.168.0.25: /root/miniconda3/envs/protein/lib/python3.10/site-packages/torch_npu/contrib/transfer_to_npu.py:209: ImportWarning: 
192.168.0.25:     *************************************************************************************************************
192.168.0.25:     The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
192.168.0.25:     The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
192.168.0.25:     The backend in torch.distributed.init_process_group set to hccl now..
192.168.0.25:     The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
192.168.0.25:     The device parameters have been replaced with npu in the function below:
192.168.0.25:     torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.autocast, torch.load, torch.Generator, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.nn.Module.to, torch.nn.Module.to_empty
192.168.0.25:     *************************************************************************************************************
192.168.0.25:     
192.168.0.25:   warnings.warn(msg, ImportWarning)
192.168.0.25: <frozen importlib._bootstrap>:914: ImportWarning: TEMetaPathFinder.find_spec() not found; falling back to find_module()
192.168.0.25: [2024-09-29 11:26:48,038] [INFO] [comm.py:637:init_distributed] cdb=None
192.168.0.25: [2024-09-29 11:26:48,038] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend hccl
192.168.0.25: <frozen importlib._bootstrap>:914: ImportWarning: TEMetaPathFinder.find_spec() not found; falling back to find_module()
192.168.0.25: [2024-09-29 11:26:48,241] [INFO] [comm.py:637:init_distributed] cdb=None
192.168.0.13: 09/29/2024 11:26:48 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
192.168.0.25: 09/29/2024 11:26:48 - INFO - llamafactory.hparams.parser - Process rank: 7, device: npu:7, n_gpu: 1, distributed training: True, compute dtype: torch.float16
192.168.0.89: [2024-09-29 11:26:32,786] [INFO] [comm.py:637:init_distributed] cdb=None
192.168.0.89: 09/29/2024 11:26:32 - INFO - llamafactory.hparams.parser - Process rank: 2, device: npu:2, n_gpu: 1, distributed training: True, compute dtype: torch.float16
192.168.0.13: 09/29/2024 11:26:48 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
192.168.0.13: [INFO|configuration_utils.py:731] 2024-09-29 11:26:48,939 >> loading configuration file /root/fcl/Meta-Llama-3-8B_512_tokenizer/config.json
192.168.0.13: [INFO|configuration_utils.py:800] 2024-09-29 11:26:48,941 >> Model config LlamaConfig {
192.168.0.13:   "_name_or_path": "/root/fcl/Meta-Llama-3-8B_512_tokenizer",
192.168.0.13:   "architectures": [
192.168.0.13:     "LlamaForCausalLM"
192.168.0.13:   ],
192.168.0.13:   "attention_bias": false,
192.168.0.13:   "attention_dropout": 0.0,
192.168.0.13:   "bos_token_id": 128000,
192.168.0.13:   "eos_token_id": 128001,
192.168.0.13:   "hidden_act": "silu",
192.168.0.13:   "hidden_size": 4096,
192.168.0.13:   "initializer_range": 0.02,
192.168.0.13:   "intermediate_size": 14336,
192.168.0.13:   "max_position_embeddings": 8192,
192.168.0.13:   "mlp_bias": false,
192.168.0.13:   "model_type": "llama",
192.168.0.13:   "num_attention_heads": 32,
192.168.0.13:   "num_hidden_layers": 32,
192.168.0.13:   "num_key_value_heads": 8,
192.168.0.13:   "pretraining_tp": 1,
192.168.0.13:   "rms_norm_eps": 1e-05,
192.168.0.13:   "rope_scaling": null,
192.168.0.13:   "rope_theta": 500000.0,
192.168.0.13:   "tie_word_embeddings": false,
192.168.0.13:   "torch_dtype": "bfloat16",
192.168.0.13:   "transformers_version": "4.44.2",
192.168.0.13:   "use_cache": true,
192.168.0.13:   "vocab_size": 128256
192.168.0.13: }
192.168.0.13: 
192.168.0.13: [INFO|tokenization_utils_base.py:2267] 2024-09-29 11:26:48,948 >> loading file tokenizer.json
192.168.0.13: [INFO|tokenization_utils_base.py:2267] 2024-09-29 11:26:48,948 >> loading file added_tokens.json
192.168.0.13: [INFO|tokenization_utils_base.py:2267] 2024-09-29 11:26:48,948 >> loading file special_tokens_map.json
192.168.0.13: [INFO|tokenization_utils_base.py:2267] 2024-09-29 11:26:48,948 >> loading file tokenizer_config.json
192.168.0.25: <frozen importlib._bootstrap>:914: ImportWarning: TEMetaPathFinder.find_spec() not found; falling back to find_module()
192.168.0.25: 09/29/2024 11:26:48 - INFO - llamafactory.hparams.parser - Process rank: 2, device: npu:2, n_gpu: 1, distributed training: True, compute dtype: torch.float16
192.168.0.25: [2024-09-29 11:26:48,520] [INFO] [comm.py:637:init_distributed] cdb=None
192.168.0.89: 09/29/2024 11:26:32 - INFO - llamafactory.hparams.parser - Process rank: 5, device: npu:5, n_gpu: 1, distributed training: True, compute dtype: torch.float16
192.168.0.149: 09/29/2024 11:26:51 - INFO - llamafactory.hparams.parser - Process rank: 6, device: npu:6, n_gpu: 1, distributed training: True, compute dtype: torch.float16
192.168.0.13: 09/29/2024 11:26:49 - INFO - llamafactory.hparams.parser - Process rank: 6, device: npu:6, n_gpu: 1, distributed training: True, compute dtype: torch.float16
192.168.0.89: 09/29/2024 11:26:33 - INFO - llamafactory.hparams.parser - Process rank: 4, device: npu:4, n_gpu: 1, distributed training: True, compute dtype: torch.float16
192.168.0.149: [2024-09-29 11:26:51,748] [INFO] [comm.py:637:init_distributed] cdb=None
192.168.0.89: 09/29/2024 11:26:33 - INFO - llamafactory.hparams.parser - Process rank: 7, device: npu:7, n_gpu: 1, distributed training: True, compute dtype: torch.float16
192.168.0.25: 09/29/2024 11:26:48 - INFO - llamafactory.hparams.parser - Process rank: 3, device: npu:3, n_gpu: 1, distributed training: True, compute dtype: torch.float16
192.168.0.25: 09/29/2024 11:26:48 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
192.168.0.89: 09/29/2024 11:26:33 - INFO - llamafactory.hparams.parser - Process rank: 6, device: npu:6, n_gpu: 1, distributed training: True, compute dtype: torch.float16
192.168.0.25: 09/29/2024 11:26:48 - INFO - llamafactory.hparams.parser - Process rank: 4, device: npu:4, n_gpu: 1, distributed training: True, compute dtype: torch.float16
192.168.0.149: 09/29/2024 11:26:51 - INFO - llamafactory.hparams.parser - Process rank: 3, device: npu:3, n_gpu: 1, distributed training: True, compute dtype: torch.float16
192.168.0.13: 09/29/2024 11:26:49 - INFO - llamafactory.hparams.parser - Process rank: 3, device: npu:3, n_gpu: 1, distributed training: True, compute dtype: torch.float16
192.168.0.149: 09/29/2024 11:26:51 - INFO - llamafactory.hparams.parser - Process rank: 5, device: npu:5, n_gpu: 1, distributed training: True, compute dtype: torch.float16
192.168.0.25: 09/29/2024 11:26:48 - INFO - llamafactory.hparams.parser - Process rank: 6, device: npu:6, n_gpu: 1, distributed training: True, compute dtype: torch.float16
192.168.0.89: 09/29/2024 11:26:33 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
192.168.0.89: 09/29/2024 11:26:33 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
192.168.0.89: [INFO|configuration_utils.py:731] 2024-09-29 11:26:33,290 >> loading configuration file /root/fcl/Meta-Llama-3-8B_512_tokenizer/config.json
192.168.0.89: [INFO|configuration_utils.py:800] 2024-09-29 11:26:33,292 >> Model config LlamaConfig {
192.168.0.89:   "_name_or_path": "/root/fcl/Meta-Llama-3-8B_512_tokenizer",
192.168.0.89:   "architectures": [
192.168.0.89:     "LlamaForCausalLM"
192.168.0.89:   ],
192.168.0.89:   "attention_bias": false,
192.168.0.89:   "attention_dropout": 0.0,
192.168.0.89:   "bos_token_id": 128000,
192.168.0.89:   "eos_token_id": 128001,
192.168.0.89:   "hidden_act": "silu",
192.168.0.89:   "hidden_size": 4096,
192.168.0.89:   "initializer_range": 0.02,
192.168.0.89:   "intermediate_size": 14336,
192.168.0.89:   "max_position_embeddings": 8192,
192.168.0.89:   "mlp_bias": false,
192.168.0.89:   "model_type": "llama",
192.168.0.89:   "num_attention_heads": 32,
192.168.0.89:   "num_hidden_layers": 32,
192.168.0.89:   "num_key_value_heads": 8,
192.168.0.89:   "pretraining_tp": 1,
192.168.0.89:   "rms_norm_eps": 1e-05,
192.168.0.89:   "rope_scaling": null,
192.168.0.89:   "rope_theta": 500000.0,
192.168.0.89:   "tie_word_embeddings": false,
192.168.0.89:   "torch_dtype": "bfloat16",
192.168.0.89:   "transformers_version": "4.44.2",
192.168.0.89:   "use_cache": true,
192.168.0.89:   "vocab_size": 128256
192.168.0.89: }
192.168.0.89: 
192.168.0.149: [2024-09-29 11:26:51,949] [INFO] [comm.py:637:init_distributed] cdb=None
192.168.0.89: [INFO|tokenization_utils_base.py:2267] 2024-09-29 11:26:33,298 >> loading file tokenizer.json
192.168.0.89: [INFO|tokenization_utils_base.py:2267] 2024-09-29 11:26:33,299 >> loading file added_tokens.json
192.168.0.89: [INFO|tokenization_utils_base.py:2267] 2024-09-29 11:26:33,299 >> loading file special_tokens_map.json
192.168.0.89: [INFO|tokenization_utils_base.py:2267] 2024-09-29 11:26:33,299 >> loading file tokenizer_config.json
192.168.0.13: 09/29/2024 11:26:49 - INFO - llamafactory.hparams.parser - Process rank: 7, device: npu:7, n_gpu: 1, distributed training: True, compute dtype: torch.float16
192.168.0.149: 09/29/2024 11:26:52 - INFO - llamafactory.hparams.parser - Process rank: 2, device: npu:2, n_gpu: 1, distributed training: True, compute dtype: torch.float16
192.168.0.13: 09/29/2024 11:26:49 - INFO - llamafactory.hparams.parser - Process rank: 4, device: npu:4, n_gpu: 1, distributed training: True, compute dtype: torch.float16
192.168.0.149: [2024-09-29 11:26:52,100] [INFO] [comm.py:637:init_distributed] cdb=None
192.168.0.13: [INFO|tokenization_utils_base.py:2513] 2024-09-29 11:26:49,524 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
192.168.0.89: 09/29/2024 11:26:33 - INFO - llamafactory.hparams.parser - Process rank: 3, device: npu:3, n_gpu: 1, distributed training: True, compute dtype: torch.float16
192.168.0.13: [INFO|configuration_utils.py:731] 2024-09-29 11:26:49,530 >> loading configuration file /root/fcl/Meta-Llama-3-8B_512_tokenizer/config.json
192.168.0.13: [INFO|configuration_utils.py:800] 2024-09-29 11:26:49,533 >> Model config LlamaConfig {
192.168.0.13:   "_name_or_path": "/root/fcl/Meta-Llama-3-8B_512_tokenizer",
192.168.0.13:   "architectures": [
192.168.0.13:     "LlamaForCausalLM"
192.168.0.13:   ],
192.168.0.13:   "attention_bias": false,
192.168.0.13:   "attention_dropout": 0.0,
192.168.0.13:   "bos_token_id": 128000,
192.168.0.13:   "eos_token_id": 128001,
192.168.0.13:   "hidden_act": "silu",
192.168.0.13:   "hidden_size": 4096,
192.168.0.13:   "initializer_range": 0.02,
192.168.0.13:   "intermediate_size": 14336,
192.168.0.13:   "max_position_embeddings": 8192,
192.168.0.13:   "mlp_bias": false,
192.168.0.13:   "model_type": "llama",
192.168.0.13:   "num_attention_heads": 32,
192.168.0.13:   "num_hidden_layers": 32,
192.168.0.13:   "num_key_value_heads": 8,
192.168.0.13:   "pretraining_tp": 1,
192.168.0.13:   "rms_norm_eps": 1e-05,
192.168.0.13:   "rope_scaling": null,
192.168.0.13:   "rope_theta": 500000.0,
192.168.0.13:   "tie_word_embeddings": false,
192.168.0.13:   "torch_dtype": "bfloat16",
192.168.0.13:   "transformers_version": "4.44.2",
192.168.0.13:   "use_cache": true,
192.168.0.13:   "vocab_size": 128256
192.168.0.13: }
192.168.0.13: 
192.168.0.13: [INFO|tokenization_utils_base.py:2267] 2024-09-29 11:26:49,538 >> loading file tokenizer.json
192.168.0.13: [INFO|tokenization_utils_base.py:2267] 2024-09-29 11:26:49,539 >> loading file added_tokens.json
192.168.0.13: [INFO|tokenization_utils_base.py:2267] 2024-09-29 11:26:49,539 >> loading file special_tokens_map.json
192.168.0.13: [INFO|tokenization_utils_base.py:2267] 2024-09-29 11:26:49,539 >> loading file tokenizer_config.json
192.168.0.13: 09/29/2024 11:26:49 - INFO - llamafactory.hparams.parser - Process rank: 5, device: npu:5, n_gpu: 1, distributed training: True, compute dtype: torch.float16
192.168.0.149: 09/29/2024 11:26:52 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
192.168.0.149: 09/29/2024 11:26:52 - INFO - llamafactory.hparams.parser - Process rank: 4, device: npu:4, n_gpu: 1, distributed training: True, compute dtype: torch.float16
192.168.0.13: 09/29/2024 11:26:49 - INFO - llamafactory.hparams.parser - Process rank: 2, device: npu:2, n_gpu: 1, distributed training: True, compute dtype: torch.float16
192.168.0.25: 09/29/2024 11:26:49 - INFO - llamafactory.hparams.parser - Process rank: 5, device: npu:5, n_gpu: 1, distributed training: True, compute dtype: torch.float16
192.168.0.149: 09/29/2024 11:26:52 - INFO - llamafactory.hparams.parser - Process rank: 7, device: npu:7, n_gpu: 1, distributed training: True, compute dtype: torch.float16
192.168.0.149: [2024-09-29 11:26:52,481] [INFO] [comm.py:637:init_distributed] cdb=None
192.168.0.149: 09/29/2024 11:26:52 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
192.168.0.149: [INFO|configuration_utils.py:731] 2024-09-29 11:26:52,502 >> loading configuration file /root/fcl/Meta-Llama-3-8B_512_tokenizer/config.json
192.168.0.149: [INFO|configuration_utils.py:800] 2024-09-29 11:26:52,504 >> Model config LlamaConfig {
192.168.0.149:   "_name_or_path": "/root/fcl/Meta-Llama-3-8B_512_tokenizer",
192.168.0.149:   "architectures": [
192.168.0.149:     "LlamaForCausalLM"
192.168.0.149:   ],
192.168.0.149:   "attention_bias": false,
192.168.0.149:   "attention_dropout": 0.0,
192.168.0.149:   "bos_token_id": 128000,
192.168.0.149:   "eos_token_id": 128001,
192.168.0.149:   "hidden_act": "silu",
192.168.0.149:   "hidden_size": 4096,
192.168.0.149:   "initializer_range": 0.02,
192.168.0.149:   "intermediate_size": 14336,
192.168.0.149:   "max_position_embeddings": 8192,
192.168.0.149:   "mlp_bias": false,
192.168.0.149:   "model_type": "llama",
192.168.0.149:   "num_attention_heads": 32,
192.168.0.149:   "num_hidden_layers": 32,
192.168.0.149:   "num_key_value_heads": 8,
192.168.0.149:   "pretraining_tp": 1,
192.168.0.149:   "rms_norm_eps": 1e-05,
192.168.0.149:   "rope_scaling": null,
192.168.0.149:   "rope_theta": 500000.0,
192.168.0.149:   "tie_word_embeddings": false,
192.168.0.149:   "torch_dtype": "bfloat16",
192.168.0.149:   "transformers_version": "4.44.2",
192.168.0.149:   "use_cache": true,
192.168.0.149:   "vocab_size": 128256
192.168.0.149: }
192.168.0.149: 
192.168.0.25: 09/29/2024 11:26:49 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
192.168.0.25: [INFO|configuration_utils.py:731] 2024-09-29 11:26:49,487 >> loading configuration file /root/fcl/Meta-Llama-3-8B_512_tokenizer/config.json
192.168.0.149: [INFO|tokenization_utils_base.py:2267] 2024-09-29 11:26:52,511 >> loading file tokenizer.json
192.168.0.149: [INFO|tokenization_utils_base.py:2267] 2024-09-29 11:26:52,511 >> loading file added_tokens.json
192.168.0.149: [INFO|tokenization_utils_base.py:2267] 2024-09-29 11:26:52,511 >> loading file special_tokens_map.json
192.168.0.25: [INFO|configuration_utils.py:800] 2024-09-29 11:26:49,489 >> Model config LlamaConfig {
192.168.0.25:   "_name_or_path": "/root/fcl/Meta-Llama-3-8B_512_tokenizer",
192.168.0.25:   "architectures": [
192.168.0.149: [INFO|tokenization_utils_base.py:2267] 2024-09-29 11:26:52,511 >> loading file tokenizer_config.json
192.168.0.25:     "LlamaForCausalLM"
192.168.0.25:   ],
192.168.0.25:   "attention_bias": false,
192.168.0.25:   "attention_dropout": 0.0,
192.168.0.25:   "bos_token_id": 128000,
192.168.0.25:   "eos_token_id": 128001,
192.168.0.25:   "hidden_act": "silu",
192.168.0.25:   "hidden_size": 4096,
192.168.0.25:   "initializer_range": 0.02,
192.168.0.25:   "intermediate_size": 14336,
192.168.0.25:   "max_position_embeddings": 8192,
192.168.0.25:   "mlp_bias": false,
192.168.0.25:   "model_type": "llama",
192.168.0.25:   "num_attention_heads": 32,
192.168.0.25:   "num_hidden_layers": 32,
192.168.0.25:   "num_key_value_heads": 8,
192.168.0.25:   "pretraining_tp": 1,
192.168.0.25:   "rms_norm_eps": 1e-05,
192.168.0.25:   "rope_scaling": null,
192.168.0.25:   "rope_theta": 500000.0,
192.168.0.25:   "tie_word_embeddings": false,
192.168.0.25:   "torch_dtype": "bfloat16",
192.168.0.25:   "transformers_version": "4.44.2",
192.168.0.25:   "use_cache": true,
192.168.0.25:   "vocab_size": 128256
192.168.0.25: }
192.168.0.25: 
192.168.0.25: [INFO|tokenization_utils_base.py:2267] 2024-09-29 11:26:49,497 >> loading file tokenizer.json
192.168.0.25: [INFO|tokenization_utils_base.py:2267] 2024-09-29 11:26:49,497 >> loading file added_tokens.json
192.168.0.25: [INFO|tokenization_utils_base.py:2267] 2024-09-29 11:26:49,497 >> loading file special_tokens_map.json
192.168.0.25: [INFO|tokenization_utils_base.py:2267] 2024-09-29 11:26:49,497 >> loading file tokenizer_config.json
192.168.0.89: [INFO|tokenization_utils_base.py:2513] 2024-09-29 11:26:33,909 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
192.168.0.89: [INFO|configuration_utils.py:731] 2024-09-29 11:26:33,913 >> loading configuration file /root/fcl/Meta-Llama-3-8B_512_tokenizer/config.json
192.168.0.89: [INFO|configuration_utils.py:800] 2024-09-29 11:26:33,915 >> Model config LlamaConfig {
192.168.0.89:   "_name_or_path": "/root/fcl/Meta-Llama-3-8B_512_tokenizer",
192.168.0.89:   "architectures": [
192.168.0.89:     "LlamaForCausalLM"
192.168.0.89:   ],
192.168.0.89:   "attention_bias": false,
192.168.0.89:   "attention_dropout": 0.0,
192.168.0.89:   "bos_token_id": 128000,
192.168.0.89:   "eos_token_id": 128001,
192.168.0.89:   "hidden_act": "silu",
192.168.0.89:   "hidden_size": 4096,
192.168.0.89:   "initializer_range": 0.02,
192.168.0.89:   "intermediate_size": 14336,
192.168.0.89:   "max_position_embeddings": 8192,
192.168.0.89:   "mlp_bias": false,
192.168.0.89:   "model_type": "llama",
192.168.0.89:   "num_attention_heads": 32,
192.168.0.89:   "num_hidden_layers": 32,
192.168.0.89:   "num_key_value_heads": 8,
192.168.0.89:   "pretraining_tp": 1,
192.168.0.89:   "rms_norm_eps": 1e-05,
192.168.0.89:   "rope_scaling": null,
192.168.0.89:   "rope_theta": 500000.0,
192.168.0.89:   "tie_word_embeddings": false,
192.168.0.89:   "torch_dtype": "bfloat16",
192.168.0.89:   "transformers_version": "4.44.2",
192.168.0.89:   "use_cache": true,
192.168.0.89:   "vocab_size": 128256
192.168.0.89: }
192.168.0.89: 
192.168.0.89: [INFO|tokenization_utils_base.py:2267] 2024-09-29 11:26:33,921 >> loading file tokenizer.json
192.168.0.89: [INFO|tokenization_utils_base.py:2267] 2024-09-29 11:26:33,921 >> loading file added_tokens.json
192.168.0.89: [INFO|tokenization_utils_base.py:2267] 2024-09-29 11:26:33,921 >> loading file special_tokens_map.json
192.168.0.89: [INFO|tokenization_utils_base.py:2267] 2024-09-29 11:26:33,921 >> loading file tokenizer_config.json
192.168.0.13: 09/29/2024 11:26:50 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
192.168.0.89: 09/29/2024 11:26:34 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
192.168.0.13: [INFO|tokenization_utils_base.py:2513] 2024-09-29 11:26:50,130 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
192.168.0.25: 09/29/2024 11:26:49 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
192.168.0.13: 09/29/2024 11:26:50 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
192.168.0.13: 09/29/2024 11:26:50 - INFO - llamafactory.data.loader - Loading dataset pt_512_c4_format.json...
192.168.0.25: 09/29/2024 11:26:49 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
192.168.0.149: 09/29/2024 11:26:52 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
192.168.0.89: 09/29/2024 11:26:34 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
192.168.0.13: 09/29/2024 11:26:50 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
192.168.0.89: 09/29/2024 11:26:34 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
192.168.0.89: 09/29/2024 11:26:34 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
192.168.0.89: 09/29/2024 11:26:34 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
192.168.0.25: 09/29/2024 11:26:49 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
192.168.0.25: 09/29/2024 11:26:49 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
192.168.0.149: 09/29/2024 11:26:53 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
192.168.0.13: 09/29/2024 11:26:50 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
192.168.0.149: 09/29/2024 11:26:53 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
192.168.0.149: [INFO|tokenization_utils_base.py:2513] 2024-09-29 11:26:53,096 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
192.168.0.149: [INFO|configuration_utils.py:731] 2024-09-29 11:26:53,103 >> loading configuration file /root/fcl/Meta-Llama-3-8B_512_tokenizer/config.json
192.168.0.149: [INFO|configuration_utils.py:800] 2024-09-29 11:26:53,105 >> Model config LlamaConfig {
192.168.0.149:   "_name_or_path": "/root/fcl/Meta-Llama-3-8B_512_tokenizer",
192.168.0.149:   "architectures": [
192.168.0.149:     "LlamaForCausalLM"
192.168.0.149:   ],
192.168.0.149:   "attention_bias": false,
192.168.0.149:   "attention_dropout": 0.0,
192.168.0.149:   "bos_token_id": 128000,
192.168.0.149:   "eos_token_id": 128001,
192.168.0.149:   "hidden_act": "silu",
192.168.0.149:   "hidden_size": 4096,
192.168.0.149:   "initializer_range": 0.02,
192.168.0.149:   "intermediate_size": 14336,
192.168.0.149:   "max_position_embeddings": 8192,
192.168.0.149:   "mlp_bias": false,
192.168.0.149:   "model_type": "llama",
192.168.0.149:   "num_attention_heads": 32,
192.168.0.149:   "num_hidden_layers": 32,
192.168.0.149:   "num_key_value_heads": 8,
192.168.0.149:   "pretraining_tp": 1,
192.168.0.149:   "rms_norm_eps": 1e-05,
192.168.0.149:   "rope_scaling": null,
192.168.0.149:   "rope_theta": 500000.0,
192.168.0.149:   "tie_word_embeddings": false,
192.168.0.149:   "torch_dtype": "bfloat16",
192.168.0.149:   "transformers_version": "4.44.2",
192.168.0.149:   "use_cache": true,
192.168.0.149:   "vocab_size": 128256
192.168.0.149: }
192.168.0.149: 
192.168.0.149: [INFO|tokenization_utils_base.py:2267] 2024-09-29 11:26:53,111 >> loading file tokenizer.json
192.168.0.149: [INFO|tokenization_utils_base.py:2267] 2024-09-29 11:26:53,111 >> loading file added_tokens.json
192.168.0.149: [INFO|tokenization_utils_base.py:2267] 2024-09-29 11:26:53,111 >> loading file special_tokens_map.json
192.168.0.149: [INFO|tokenization_utils_base.py:2267] 2024-09-29 11:26:53,111 >> loading file tokenizer_config.json
192.168.0.25: [INFO|tokenization_utils_base.py:2513] 2024-09-29 11:26:50,092 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
192.168.0.25: [INFO|configuration_utils.py:731] 2024-09-29 11:26:50,098 >> loading configuration file /root/fcl/Meta-Llama-3-8B_512_tokenizer/config.json
192.168.0.25: [INFO|configuration_utils.py:800] 2024-09-29 11:26:50,100 >> Model config LlamaConfig {
192.168.0.25:   "_name_or_path": "/root/fcl/Meta-Llama-3-8B_512_tokenizer",
192.168.0.25:   "architectures": [
192.168.0.25:     "LlamaForCausalLM"
192.168.0.25:   ],
192.168.0.25:   "attention_bias": false,
192.168.0.25:   "attention_dropout": 0.0,
192.168.0.25:   "bos_token_id": 128000,
192.168.0.25:   "eos_token_id": 128001,
192.168.0.25:   "hidden_act": "silu",
192.168.0.25:   "hidden_size": 4096,
192.168.0.25:   "initializer_range": 0.02,
192.168.0.25:   "intermediate_size": 14336,
192.168.0.25:   "max_position_embeddings": 8192,
192.168.0.25:   "mlp_bias": false,
192.168.0.25:   "model_type": "llama",
192.168.0.25:   "num_attention_heads": 32,
192.168.0.25:   "num_hidden_layers": 32,
192.168.0.25:   "num_key_value_heads": 8,
192.168.0.25:   "pretraining_tp": 1,
192.168.0.25:   "rms_norm_eps": 1e-05,
192.168.0.25:   "rope_scaling": null,
192.168.0.25:   "rope_theta": 500000.0,
192.168.0.25:   "tie_word_embeddings": false,
192.168.0.25:   "torch_dtype": "bfloat16",
192.168.0.25:   "transformers_version": "4.44.2",
192.168.0.25:   "use_cache": true,
192.168.0.25:   "vocab_size": 128256
192.168.0.25: }
192.168.0.25: 
192.168.0.25: [INFO|tokenization_utils_base.py:2267] 2024-09-29 11:26:50,106 >> loading file tokenizer.json
192.168.0.25: [INFO|tokenization_utils_base.py:2267] 2024-09-29 11:26:50,106 >> loading file added_tokens.json
192.168.0.25: [INFO|tokenization_utils_base.py:2267] 2024-09-29 11:26:50,106 >> loading file special_tokens_map.json
192.168.0.25: [INFO|tokenization_utils_base.py:2267] 2024-09-29 11:26:50,106 >> loading file tokenizer_config.json
192.168.0.89: 09/29/2024 11:26:34 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
192.168.0.89: [INFO|tokenization_utils_base.py:2513] 2024-09-29 11:26:34,551 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
192.168.0.149: 09/29/2024 11:26:53 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
192.168.0.25: 09/29/2024 11:26:50 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
192.168.0.25: 09/29/2024 11:26:50 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
192.168.0.89: 09/29/2024 11:26:34 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
192.168.0.89: 09/29/2024 11:26:34 - INFO - llamafactory.data.loader - Loading dataset pt_512_c4_format.json...
192.168.0.89: 09/29/2024 11:26:34 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
192.168.0.13: 09/29/2024 11:26:50 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
192.168.0.13: 09/29/2024 11:26:50 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
192.168.0.13: 09/29/2024 11:26:50 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
192.168.0.149: 09/29/2024 11:26:53 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
192.168.0.25: 09/29/2024 11:26:50 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
192.168.0.149: 09/29/2024 11:26:53 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
192.168.0.13: 09/29/2024 11:26:50 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
192.168.0.25: [INFO|tokenization_utils_base.py:2513] 2024-09-29 11:26:50,670 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
192.168.0.25: 09/29/2024 11:26:50 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
192.168.0.25: 09/29/2024 11:26:50 - INFO - llamafactory.data.loader - Loading dataset pt_512_c4_format.json...
192.168.0.149: [INFO|tokenization_utils_base.py:2513] 2024-09-29 11:26:53,759 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
192.168.0.149: 09/29/2024 11:26:53 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
192.168.0.149: 09/29/2024 11:26:53 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
192.168.0.149: 09/29/2024 11:26:53 - INFO - llamafactory.data.loader - Loading dataset pt_512_c4_format.json...
192.168.0.89: Converting format of dataset (num_proc=2):   0%|          | 0/300001 [00:00<?, ? examples/s]Converting format of dataset (num_proc=2):   0%|          | 556/300001 [00:00<01:03, 4702.57 examples/s]Converting format of dataset (num_proc=2):   1%|          | 2600/300001 [00:00<00:25, 11845.20 examples/s]Converting format of dataset (num_proc=2):   2%|▏         | 4594/300001 [00:00<00:19, 14967.47 examples/s]Converting format of dataset (num_proc=2):   2%|▏         | 6589/300001 [00:00<00:17, 16617.37 examples/s]Converting format of dataset (num_proc=2):   3%|▎         | 8594/300001 [00:00<00:16, 17544.09 examples/s]Converting format of dataset (num_proc=2):   4%|▎         | 10587/300001 [00:00<00:15, 18169.34 examples/s]Converting format of dataset (num_proc=2):   4%|▍         | 12591/300001 [00:00<00:15, 18548.54 examples/s]Converting format of dataset (num_proc=2):   5%|▍         | 14568/300001 [00:00<00:15, 18703.04 examples/s]Converting format of dataset (num_proc=2):   6%|▌         | 16589/300001 [00:00<00:15, 18588.12 examples/s]Converting format of dataset (num_proc=2):   6%|▌         | 18592/300001 [00:01<00:15, 18734.47 examples/s]Converting format of dataset (num_proc=2):   7%|▋         | 20590/300001 [00:01<00:14, 18867.14 examples/s]Converting format of dataset (num_proc=2):   8%|▊         | 22590/300001 [00:01<00:14, 18986.11 examples/s]Converting format of dataset (num_proc=2):   8%|▊         | 24588/300001 [00:01<00:14, 19058.20 examples/s]Converting format of dataset (num_proc=2):   9%|▉         | 26589/300001 [00:01<00:14, 19115.83 examples/s]Converting format of dataset (num_proc=2):  10%|▉         | 28591/300001 [00:01<00:14, 19158.66 examples/s]Converting format of dataset (num_proc=2):  10%|█         | 30591/300001 [00:01<00:14, 19126.69 examples/s]Converting format of dataset (num_proc=2):  11%|█         | 32590/300001 [00:01<00:13, 19190.01 examples/s]Converting format of dataset (num_proc=2):  12%|█▏        | 34584/300001 [00:01<00:14, 18677.57 examples/s]Converting format of dataset (num_proc=2):  12%|█▏        | 36591/300001 [00:02<00:14, 18779.35 examples/s]Converting format of dataset (num_proc=2):  13%|█▎        | 38594/300001 [00:02<00:13, 18882.55 examples/s]Converting format of dataset (num_proc=2):  14%|█▎        | 40511/300001 [00:02<00:14, 18220.80 examples/s]Converting format of dataset (num_proc=2):  14%|█▍        | 42588/300001 [00:02<00:13, 18659.61 examples/s]Converting format of dataset (num_proc=2):  15%|█▍        | 44590/300001 [00:02<00:13, 18785.46 examples/s]Converting format of dataset (num_proc=2):  16%|█▌        | 47000/300001 [00:02<00:14, 17022.93 examples/s]Converting format of dataset (num_proc=2):  16%|█▋        | 49000/300001 [00:02<00:14, 17626.07 examples/s]Converting format of dataset (num_proc=2):  17%|█▋        | 51000/300001 [00:02<00:13, 18050.52 examples/s]Converting format of dataset (num_proc=2):  18%|█▊        | 53000/300001 [00:02<00:13, 18412.80 examples/s]Converting format of dataset (num_proc=2):  18%|█▊        | 55000/300001 [00:03<00:13, 18536.03 examples/s]Converting format of dataset (num_proc=2):  19%|█▉        | 57000/300001 [00:03<00:13, 18604.05 examples/s]Converting format of dataset (num_proc=2):  20%|█▉        | 59000/300001 [00:03<00:12, 18950.73 examples/s]Converting format of dataset (num_proc=2):  20%|██        | 61000/300001 [00:03<00:12, 19238.91 examples/s]Converting format of dataset (num_proc=2):  21%|██        | 63000/300001 [00:03<00:12, 19404.85 examples/s]Converting format of dataset (num_proc=2):  22%|██▏       | 65000/300001 [00:03<00:12, 19569.76 examples/s]Converting format of dataset (num_proc=2):  22%|██▏       | 67000/300001 [00:03<00:12, 18760.30 examples/s]Converting format of dataset (num_proc=2):  23%|██▎       | 69000/300001 [00:03<00:12, 18771.11 examples/s]Converting format of dataset (num_proc=2):  24%|██▎       | 71000/300001 [00:03<00:12, 17957.19 examples/s]Converting format of dataset (num_proc=2):  24%|██▍       | 73000/300001 [00:03<00:12, 18363.61 examples/s]Converting format of dataset (num_proc=2):  25%|██▍       | 75000/300001 [00:04<00:12, 18609.02 examples/s]Converting format of dataset (num_proc=2):  26%|██▌       | 77000/300001 [00:04<00:12, 18214.17 examples/s]Converting format of dataset (num_proc=2):  26%|██▋       | 79000/300001 [00:04<00:11, 18580.84 examples/s]Converting format of dataset (num_proc=2):  27%|██▋       | 81000/300001 [00:04<00:11, 18822.95 examples/s]Converting format of dataset (num_proc=2):  28%|██▊       | 83000/300001 [00:04<00:11, 18974.75 examples/s]Converting format of dataset (num_proc=2):  28%|██▊       | 85000/300001 [00:04<00:11, 19137.24 examples/s]Converting format of dataset (num_proc=2):  29%|██▉       | 87000/300001 [00:04<00:11, 19228.56 examples/s]Converting format of dataset (num_proc=2):  30%|██▉       | 89000/300001 [00:04<00:10, 19318.74 examples/s]Converting format of dataset (num_proc=2):  30%|███       | 91000/300001 [00:04<00:10, 19318.15 examples/s]Converting format of dataset (num_proc=2):  31%|███       | 93000/300001 [00:05<00:10, 19175.78 examples/s]Converting format of dataset (num_proc=2):  32%|███▏      | 95000/300001 [00:05<00:10, 19245.37 examples/s]Converting format of dataset (num_proc=2):  32%|███▏      | 97000/300001 [00:05<00:11, 18256.88 examples/s]Converting format of dataset (num_proc=2):  33%|███▎      | 99000/300001 [00:05<00:10, 18613.68 examples/s]Converting format of dataset (num_proc=2):  34%|███▎      | 101000/300001 [00:05<00:10, 18875.10 examples/s]Converting format of dataset (num_proc=2):  34%|███▍      | 103000/300001 [00:05<00:10, 19134.85 examples/s]Converting format of dataset (num_proc=2):  35%|███▍      | 105000/300001 [00:05<00:10, 19328.01 examples/s]Converting format of dataset (num_proc=2):  36%|███▌      | 107000/300001 [00:05<00:09, 19465.43 examples/s]Converting format of dataset (num_proc=2):  36%|███▋      | 109000/300001 [00:05<00:09, 19560.08 examples/s]Converting format of dataset (num_proc=2):  37%|███▋      | 111000/300001 [00:05<00:09, 19607.86 examples/s]Converting format of dataset (num_proc=2):  38%|███▊      | 113000/300001 [00:06<00:09, 19594.82 examples/s]Converting format of dataset (num_proc=2):  38%|███▊      | 115000/300001 [00:06<00:09, 19634.13 examples/s]Converting format of dataset (num_proc=2):  39%|███▉      | 117000/300001 [00:06<00:09, 19624.53 examples/s]Converting format of dataset (num_proc=2):  40%|███▉      | 119000/300001 [00:06<00:09, 19461.99 examples/s]Converting format of dataset (num_proc=2):  40%|████      | 121262/300001 [00:06<00:09, 17892.52 examples/s]Converting format of dataset (num_proc=2):  41%|████      | 123211/300001 [00:06<00:09, 18314.63 examples/s]Converting format of dataset (num_proc=2):  42%|████▏     | 125163/300001 [00:06<00:09, 18645.41 examples/s]Converting format of dataset (num_proc=2):  42%|████▏     | 127114/300001 [00:06<00:09, 18889.98 examples/s]Converting format of dataset (num_proc=2):  43%|████▎     | 129047/300001 [00:06<00:08, 19015.23 examples/s]Converting format of dataset (num_proc=2):  44%|████▎     | 131002/300001 [00:07<00:08, 19168.75 examples/s]Converting format of dataset (num_proc=2):  44%|████▍     | 133000/300001 [00:07<00:08, 19169.19 examples/s]Converting format of dataset (num_proc=2):  45%|████▍     | 135000/300001 [00:07<00:08, 19193.04 examples/s]Converting format of dataset (num_proc=2):  46%|████▌     | 137000/300001 [00:07<00:08, 19228.31 examples/s]Converting format of dataset (num_proc=2):  46%|████▋     | 139000/300001 [00:07<00:08, 19253.59 examples/s]Converting format of dataset (num_proc=2):  47%|████▋     | 141000/300001 [00:07<00:08, 19214.63 examples/s]Converting format of dataset (num_proc=2):  48%|████▊     | 143000/300001 [00:07<00:08, 19222.74 examples/s]Converting format of dataset (num_proc=2):  48%|████▊     | 145000/300001 [00:07<00:08, 19139.50 examples/s]Converting format of dataset (num_proc=2):  49%|████▉     | 147000/300001 [00:07<00:07, 19131.49 examples/s]Converting format of dataset (num_proc=2):  50%|████▉     | 149000/300001 [00:07<00:07, 19155.29 examples/s]Converting format of dataset (num_proc=2):  50%|█████     | 151000/300001 [00:08<00:07, 19199.31 examples/s]Converting format of dataset (num_proc=2):  51%|█████     | 153000/300001 [00:08<00:07, 19259.73 examples/s]Converting format of dataset (num_proc=2):  52%|█████▏    | 155000/300001 [00:08<00:07, 19127.18 examples/s]Converting format of dataset (num_proc=2):  52%|█████▏    | 157000/300001 [00:08<00:07, 19227.72 examples/s]Converting format of dataset (num_proc=2):  53%|█████▎    | 159000/300001 [00:08<00:07, 19284.81 examples/s]Converting format of dataset (num_proc=2):  54%|█████▎    | 161000/300001 [00:08<00:07, 19303.63 examples/s]Converting format of dataset (num_proc=2):  54%|█████▍    | 163000/300001 [00:08<00:07, 19267.20 examples/s]Converting format of dataset (num_proc=2):  55%|█████▍    | 165000/300001 [00:08<00:07, 19070.88 examples/s]Converting format of dataset (num_proc=2):  56%|█████▌    | 167000/300001 [00:08<00:06, 19088.63 examples/s]Converting format of dataset (num_proc=2):  56%|█████▋    | 169000/300001 [00:09<00:06, 19181.12 examples/s]Converting format of dataset (num_proc=2):  57%|█████▋    | 171000/300001 [00:09<00:06, 19169.04 examples/s]Converting format of dataset (num_proc=2):  58%|█████▊    | 173000/300001 [00:09<00:06, 19199.79 examples/s]Converting format of dataset (num_proc=2):  58%|█████▊    | 175000/300001 [00:09<00:06, 19218.55 examples/s]Converting format of dataset (num_proc=2):  59%|█████▉    | 177000/300001 [00:09<00:06, 19234.48 examples/s]Converting format of dataset (num_proc=2):  60%|█████▉    | 179000/300001 [00:09<00:06, 19233.61 examples/s]Converting format of dataset (num_proc=2):  60%|██████    | 181000/300001 [00:09<00:06, 19247.34 examples/s]Converting format of dataset (num_proc=2):  61%|██████    | 183000/300001 [00:09<00:06, 19247.04 examples/s]Converting format of dataset (num_proc=2):  62%|██████▏   | 185000/300001 [00:09<00:06, 19156.75 examples/s]Converting format of dataset (num_proc=2):  62%|██████▏   | 187000/300001 [00:09<00:05, 19173.03 examples/s]Converting format of dataset (num_proc=2):  63%|██████▎   | 189000/300001 [00:10<00:05, 19231.55 examples/s]Converting format of dataset (num_proc=2):  64%|██████▎   | 191000/300001 [00:10<00:05, 19289.18 examples/s]Converting format of dataset (num_proc=2):  64%|██████▍   | 193000/300001 [00:10<00:05, 19315.28 examples/s]Converting format of dataset (num_proc=2):  65%|██████▌   | 195485/300001 [00:10<00:05, 18625.31 examples/s]Converting format of dataset (num_proc=2):  66%|██████▌   | 197421/300001 [00:10<00:05, 18717.95 examples/s]Converting format of dataset (num_proc=2):  66%|██████▋   | 199368/300001 [00:10<00:05, 18849.26 examples/s]Converting format of dataset (num_proc=2):  67%|██████▋   | 201322/300001 [00:10<00:05, 18979.61 examples/s]Converting format of dataset (num_proc=2):  68%|██████▊   | 203281/300001 [00:10<00:05, 19091.92 examples/s]Converting format of dataset (num_proc=2):  68%|██████▊   | 205236/300001 [00:10<00:04, 19131.17 examples/s]Converting format of dataset (num_proc=2):  69%|██████▉   | 207192/300001 [00:11<00:04, 19156.87 examples/s]Converting format of dataset (num_proc=2):  70%|██████▉   | 209184/300001 [00:11<00:04, 19326.39 examples/s]Converting format of dataset (num_proc=2):  70%|███████   | 211179/300001 [00:11<00:04, 19455.54 examples/s]Converting format of dataset (num_proc=2):  71%|███████   | 213180/300001 [00:11<00:04, 19566.48 examples/s]Converting format of dataset (num_proc=2):  72%|███████▏  | 215180/300001 [00:11<00:04, 19439.29 examples/s]Converting format of dataset (num_proc=2):  72%|███████▏  | 217177/300001 [00:11<00:05, 16425.15 examples/s]Converting format of dataset (num_proc=2):  73%|███████▎  | 219074/300001 [00:12<00:16, 5022.81 examples/s] Converting format of dataset (num_proc=2):  74%|███████▎  | 220975/300001 [00:12<00:12, 6403.23 examples/s]Converting format of dataset (num_proc=2):  74%|███████▍  | 222882/300001 [00:12<00:09, 7967.66 examples/s]Converting format of dataset (num_proc=2):  75%|███████▍  | 224773/300001 [00:12<00:07, 9609.45 examples/s]Converting format of dataset (num_proc=2):  76%|███████▌  | 226684/300001 [00:13<00:06, 11283.95 examples/s]Converting format of dataset (num_proc=2):  76%|███████▌  | 228646/300001 [00:13<00:05, 12826.35 examples/s]Converting format of dataset (num_proc=2):  77%|███████▋  | 230575/300001 [00:13<00:04, 14118.71 examples/s]Converting format of dataset (num_proc=2):  78%|███████▊  | 232573/300001 [00:13<00:04, 15362.07 examples/s]Converting format of dataset (num_proc=2):  78%|███████▊  | 234575/300001 [00:13<00:04, 16350.84 examples/s]Converting format of dataset (num_proc=2):  79%|███████▉  | 236571/300001 [00:13<00:03, 17108.20 examples/s]Converting format of dataset (num_proc=2):  80%|███████▉  | 238572/300001 [00:13<00:03, 17635.51 examples/s]Converting format of dataset (num_proc=2):  80%|████████  | 240574/300001 [00:13<00:03, 18031.44 examples/s]Converting format of dataset (num_proc=2):  81%|████████  | 242573/300001 [00:13<00:03, 18305.56 examples/s]Converting format of dataset (num_proc=2):  82%|████████▏ | 244573/300001 [00:13<00:02, 18520.16 examples/s]Converting format of dataset (num_proc=2):  82%|████████▏ | 246572/300001 [00:14<00:02, 18647.59 examples/s]Converting format of dataset (num_proc=2):  83%|████████▎ | 248587/300001 [00:14<00:02, 18859.15 examples/s]Converting format of dataset (num_proc=2):  84%|████████▎ | 250587/300001 [00:14<00:02, 19016.11 examples/s]Converting format of dataset (num_proc=2):  84%|████████▍ | 252586/300001 [00:14<00:02, 19120.50 examples/s]Converting format of dataset (num_proc=2):  85%|████████▍ | 254584/300001 [00:14<00:02, 19151.29 examples/s]Converting format of dataset (num_proc=2):  86%|████████▌ | 256588/300001 [00:14<00:02, 19208.67 examples/s]Converting format of dataset (num_proc=2):  86%|████████▌ | 258582/300001 [00:14<00:02, 19244.92 examples/s]Converting format of dataset (num_proc=2):  87%|████████▋ | 260584/300001 [00:14<00:02, 19264.91 examples/s]Converting format of dataset (num_proc=2):  88%|████████▊ | 262586/300001 [00:14<00:01, 19298.89 examples/s]Converting format of dataset (num_proc=2):  88%|████████▊ | 264582/300001 [00:14<00:01, 19328.85 examples/s]Converting format of dataset (num_proc=2):  89%|████████▉ | 266585/300001 [00:15<00:01, 19308.55 examples/s]Converting format of dataset (num_proc=2):  90%|████████▉ | 268590/300001 [00:15<00:01, 19144.81 examples/s]Converting format of dataset (num_proc=2):  90%|█████████ | 270586/300001 [00:15<00:01, 19249.82 examples/s]Converting format of dataset (num_proc=2):  91%|█████████ | 272590/300001 [00:15<00:01, 19301.29 examples/s]Converting format of dataset (num_proc=2):  92%|█████████▏| 274587/300001 [00:15<00:01, 19373.88 examples/s]Converting format of dataset (num_proc=2):  92%|█████████▏| 276587/300001 [00:15<00:01, 19374.80 examples/s]Converting format of dataset (num_proc=2):  93%|█████████▎| 278583/300001 [00:15<00:01, 19400.56 examples/s]Converting format of dataset (num_proc=2):  94%|█████████▎| 280592/300001 [00:15<00:01, 19402.07 examples/s]Converting format of dataset (num_proc=2):  94%|█████████▍| 282592/300001 [00:15<00:00, 19472.56 examples/s]Converting format of dataset (num_proc=2):  95%|█████████▍| 284587/300001 [00:16<00:00, 19505.73 examples/s]Converting format of dataset (num_proc=2):  96%|█████████▌| 286592/300001 [00:16<00:00, 19508.53 examples/s]Converting format of dataset (num_proc=2):  96%|█████████▌| 288591/300001 [00:16<00:00, 19504.65 examples/s]Converting format of dataset (num_proc=2):  97%|█████████▋| 290589/300001 [00:16<00:00, 19509.42 examples/s]Converting format of dataset (num_proc=2):  98%|█████████▊| 292589/300001 [00:16<00:00, 19498.31 examples/s]Converting format of dataset (num_proc=2):  98%|█████████▊| 294588/300001 [00:16<00:00, 19481.12 examples/s]Converting format of dataset (num_proc=2):  99%|█████████▉| 296592/300001 [00:16<00:00, 19483.58 examples/s]Converting format of dataset (num_proc=2): 100%|█████████▉| 298588/300001 [00:16<00:00, 19489.12 examples/s]Converting format of dataset (num_proc=2): 100%|██████████| 300001/300001 [00:17<00:00, 17514.75 examples/s]
192.168.0.13: Converting format of dataset (num_proc=2):   0%|          | 0/300001 [00:00<?, ? examples/s]Converting format of dataset (num_proc=2):   0%|          | 521/300001 [00:00<01:28, 3370.87 examples/s]Converting format of dataset (num_proc=2):   1%|          | 2588/300001 [00:00<00:28, 10337.88 examples/s]Converting format of dataset (num_proc=2):   2%|▏         | 4582/300001 [00:00<00:21, 13700.23 examples/s]Converting format of dataset (num_proc=2):   2%|▏         | 6581/300001 [00:00<00:18, 15652.26 examples/s]Converting format of dataset (num_proc=2):   3%|▎         | 8583/300001 [00:00<00:17, 16799.83 examples/s]Converting format of dataset (num_proc=2):   4%|▎         | 10581/300001 [00:00<00:16, 17610.04 examples/s]Converting format of dataset (num_proc=2):   4%|▍         | 12579/300001 [00:00<00:15, 18130.45 examples/s]Converting format of dataset (num_proc=2):   5%|▍         | 14574/300001 [00:00<00:15, 18358.21 examples/s]Converting format of dataset (num_proc=2):   6%|▌         | 16577/300001 [00:01<00:15, 18495.25 examples/s]Converting format of dataset (num_proc=2):   6%|▌         | 18501/300001 [00:01<00:15, 18435.83 examples/s]Converting format of dataset (num_proc=2):   7%|▋         | 20576/300001 [00:01<00:15, 18447.47 examples/s]Converting format of dataset (num_proc=2):   8%|▊         | 22578/300001 [00:01<00:14, 18654.46 examples/s]Converting format of dataset (num_proc=2):   8%|▊         | 24513/300001 [00:01<00:14, 18592.57 examples/s]Converting format of dataset (num_proc=2):   9%|▉         | 26586/300001 [00:01<00:14, 18315.68 examples/s]Converting format of dataset (num_proc=2):  10%|▉         | 28592/300001 [00:01<00:14, 18545.69 examples/s]Converting format of dataset (num_proc=2):  10%|█         | 30480/300001 [00:01<00:14, 18357.66 examples/s]Converting format of dataset (num_proc=2):  11%|█         | 32590/300001 [00:01<00:15, 17826.33 examples/s]Converting format of dataset (num_proc=2):  12%|█▏        | 34580/300001 [00:02<00:14, 18063.27 examples/s]Converting format of dataset (num_proc=2):  12%|█▏        | 36561/300001 [00:02<00:14, 18171.47 examples/s]Converting format of dataset (num_proc=2):  13%|█▎        | 38581/300001 [00:02<00:14, 18175.55 examples/s]Converting format of dataset (num_proc=2):  14%|█▎        | 40585/300001 [00:02<00:14, 18302.84 examples/s]Converting format of dataset (num_proc=2):  14%|█▍        | 42590/300001 [00:02<00:13, 18498.68 examples/s]Converting format of dataset (num_proc=2):  15%|█▍        | 44492/300001 [00:02<00:13, 18378.85 examples/s]Converting format of dataset (num_proc=2):  15%|█▌        | 46341/300001 [00:02<00:14, 17767.72 examples/s]Converting format of dataset (num_proc=2):  16%|█▌        | 48303/300001 [00:02<00:13, 18287.00 examples/s]Converting format of dataset (num_proc=2):  17%|█▋        | 50258/300001 [00:02<00:13, 18641.36 examples/s]Converting format of dataset (num_proc=2):  18%|█▊        | 52811/300001 [00:02<00:13, 18721.15 examples/s]Converting format of dataset (num_proc=2):  18%|█▊        | 54767/300001 [00:03<00:13, 18803.45 examples/s]Converting format of dataset (num_proc=2):  19%|█▉        | 56739/300001 [00:03<00:12, 18895.00 examples/s]Converting format of dataset (num_proc=2):  20%|█▉        | 58704/300001 [00:03<00:12, 18631.15 examples/s]Converting format of dataset (num_proc=2):  20%|██        | 60674/300001 [00:03<00:12, 18927.59 examples/s]Converting format of dataset (num_proc=2):  21%|██        | 62642/300001 [00:03<00:12, 19139.97 examples/s]Converting format of dataset (num_proc=2):  22%|██▏       | 64617/300001 [00:03<00:12, 19315.88 examples/s]Converting format of dataset (num_proc=2):  22%|██▏       | 66582/300001 [00:03<00:12, 19225.82 examples/s]Converting format of dataset (num_proc=2):  23%|██▎       | 68593/300001 [00:03<00:12, 19189.40 examples/s]Converting format of dataset (num_proc=2):  24%|██▎       | 70595/300001 [00:03<00:11, 19291.11 examples/s]Converting format of dataset (num_proc=2):  24%|██▍       | 72943/300001 [00:04<00:11, 19052.19 examples/s]Converting format of dataset (num_proc=2):  25%|██▍       | 74895/300001 [00:04<00:11, 19051.89 examples/s]Converting format of dataset (num_proc=2):  26%|██▌       | 76862/300001 [00:04<00:11, 19060.47 examples/s]Converting format of dataset (num_proc=2):  26%|██▋       | 78792/300001 [00:04<00:11, 18853.06 examples/s]Converting format of dataset (num_proc=2):  27%|██▋       | 80794/300001 [00:04<00:11, 18877.05 examples/s]Converting format of dataset (num_proc=2):  28%|██▊       | 82753/300001 [00:04<00:11, 18969.26 examples/s]Converting format of dataset (num_proc=2):  28%|██▊       | 84721/300001 [00:04<00:11, 19023.81 examples/s]Converting format of dataset (num_proc=2):  29%|██▉       | 86634/300001 [00:04<00:11, 18925.67 examples/s]Converting format of dataset (num_proc=2):  30%|██▉       | 88640/300001 [00:04<00:11, 19024.69 examples/s]Converting format of dataset (num_proc=2):  30%|███       | 90603/300001 [00:04<00:10, 19196.99 examples/s]Converting format of dataset (num_proc=2):  31%|███       | 92589/300001 [00:05<00:10, 19315.00 examples/s]Converting format of dataset (num_proc=2):  32%|███▏      | 94549/300001 [00:05<00:10, 19213.45 examples/s]Converting format of dataset (num_proc=2):  32%|███▏      | 96590/300001 [00:05<00:10, 19213.30 examples/s]Converting format of dataset (num_proc=2):  33%|███▎      | 98592/300001 [00:05<00:10, 19285.26 examples/s]Converting format of dataset (num_proc=2):  34%|███▎      | 100593/300001 [00:05<00:10, 19365.78 examples/s]Converting format of dataset (num_proc=2):  34%|███▍      | 102902/300001 [00:05<00:09, 19788.93 examples/s]Converting format of dataset (num_proc=2):  35%|███▌      | 105468/300001 [00:05<00:10, 19324.43 examples/s]Converting format of dataset (num_proc=2):  36%|███▌      | 107427/300001 [00:05<00:09, 19395.03 examples/s]Converting format of dataset (num_proc=2):  36%|███▋      | 109385/300001 [00:05<00:09, 19439.87 examples/s]Converting format of dataset (num_proc=2):  37%|███▋      | 111375/300001 [00:06<00:09, 19566.62 examples/s]Converting format of dataset (num_proc=2):  38%|███▊      | 113347/300001 [00:06<00:09, 19607.06 examples/s]Converting format of dataset (num_proc=2):  38%|███▊      | 115316/300001 [00:06<00:09, 19628.39 examples/s]Converting format of dataset (num_proc=2):  39%|███▉      | 117281/300001 [00:06<00:09, 19630.82 examples/s]Converting format of dataset (num_proc=2):  40%|███▉      | 119259/300001 [00:06<00:09, 19672.30 examples/s]Converting format of dataset (num_proc=2):  40%|████      | 121228/300001 [00:06<00:09, 19673.15 examples/s]Converting format of dataset (num_proc=2):  41%|████      | 123590/300001 [00:06<00:09, 18077.67 examples/s]Converting format of dataset (num_proc=2):  42%|████▏     | 125590/300001 [00:06<00:09, 18329.67 examples/s]Converting format of dataset (num_proc=2):  43%|████▎     | 127595/300001 [00:06<00:09, 18664.33 examples/s]Converting format of dataset (num_proc=2):  43%|████▎     | 129594/300001 [00:06<00:09, 18917.77 examples/s]Converting format of dataset (num_proc=2):  44%|████▍     | 131588/300001 [00:07<00:08, 19086.52 examples/s]Converting format of dataset (num_proc=2):  45%|████▍     | 133592/300001 [00:07<00:08, 19246.28 examples/s]Converting format of dataset (num_proc=2):  45%|████▌     | 135928/300001 [00:07<00:08, 19558.67 examples/s]Converting format of dataset (num_proc=2):  46%|████▌     | 137892/300001 [00:07<00:08, 19430.64 examples/s]Converting format of dataset (num_proc=2):  47%|████▋     | 139863/300001 [00:07<00:08, 19354.78 examples/s]Converting format of dataset (num_proc=2):  47%|████▋     | 141833/300001 [00:07<00:08, 19352.94 examples/s]Converting format of dataset (num_proc=2):  48%|████▊     | 143806/300001 [00:07<00:08, 19344.65 examples/s]Converting format of dataset (num_proc=2):  49%|████▊     | 145766/300001 [00:07<00:07, 19340.28 examples/s]Converting format of dataset (num_proc=2):  49%|████▉     | 147741/300001 [00:07<00:07, 19166.53 examples/s]Converting format of dataset (num_proc=2):  50%|████▉     | 149709/300001 [00:08<00:07, 18898.61 examples/s]Converting format of dataset (num_proc=2):  51%|█████     | 151668/300001 [00:08<00:07, 18771.93 examples/s]Converting format of dataset (num_proc=2):  51%|█████     | 153632/300001 [00:08<00:07, 18812.84 examples/s]Converting format of dataset (num_proc=2):  52%|█████▏    | 155601/300001 [00:08<00:07, 18960.04 examples/s]Converting format of dataset (num_proc=2):  53%|█████▎    | 157593/300001 [00:08<00:07, 19124.17 examples/s]Converting format of dataset (num_proc=2):  53%|█████▎    | 159594/300001 [00:08<00:07, 19252.06 examples/s]Converting format of dataset (num_proc=2):  54%|█████▍    | 161590/300001 [00:08<00:07, 19014.01 examples/s]Converting format of dataset (num_proc=2):  55%|█████▍    | 163588/300001 [00:08<00:07, 19159.84 examples/s]Converting format of dataset (num_proc=2):  55%|█████▌    | 165590/300001 [00:08<00:06, 19294.32 examples/s]Converting format of dataset (num_proc=2):  56%|█████▌    | 168000/300001 [00:08<00:06, 19380.64 examples/s]Converting format of dataset (num_proc=2):  57%|█████▋    | 170000/300001 [00:09<00:06, 19380.05 examples/s]Converting format of dataset (num_proc=2):  57%|█████▋    | 172000/300001 [00:09<00:06, 19397.92 examples/s]Converting format of dataset (num_proc=2):  58%|█████▊    | 174000/300001 [00:09<00:06, 19402.20 examples/s]Converting format of dataset (num_proc=2):  59%|█████▊    | 176000/300001 [00:09<00:06, 19444.88 examples/s]Converting format of dataset (num_proc=2):  59%|█████▉    | 178000/300001 [00:09<00:06, 19479.20 examples/s]Converting format of dataset (num_proc=2):  60%|█████▉    | 180000/300001 [00:09<00:06, 19494.50 examples/s]Converting format of dataset (num_proc=2):  61%|██████    | 182000/300001 [00:09<00:06, 19532.04 examples/s]Converting format of dataset (num_proc=2):  61%|██████▏   | 184000/300001 [00:09<00:05, 19500.70 examples/s]Converting format of dataset (num_proc=2):  62%|██████▏   | 186000/300001 [00:09<00:05, 19507.75 examples/s]Converting format of dataset (num_proc=2):  63%|██████▎   | 188000/300001 [00:10<00:05, 19541.44 examples/s]Converting format of dataset (num_proc=2):  63%|██████▎   | 190000/300001 [00:10<00:05, 18985.83 examples/s]Converting format of dataset (num_proc=2):  64%|██████▍   | 192000/300001 [00:10<00:05, 19149.29 examples/s]Converting format of dataset (num_proc=2):  65%|██████▍   | 194000/300001 [00:10<00:05, 19234.25 examples/s]Converting format of dataset (num_proc=2):  65%|██████▌   | 196000/300001 [00:10<00:05, 19359.71 examples/s]Converting format of dataset (num_proc=2):  66%|██████▌   | 198306/300001 [00:10<00:05, 18164.45 examples/s]Converting format of dataset (num_proc=2):  67%|██████▋   | 200275/300001 [00:10<00:05, 18572.23 examples/s]Converting format of dataset (num_proc=2):  67%|██████▋   | 202251/300001 [00:10<00:05, 18899.04 examples/s]Converting format of dataset (num_proc=2):  68%|██████▊   | 204218/300001 [00:10<00:05, 19118.58 examples/s]Converting format of dataset (num_proc=2):  69%|██████▊   | 206189/300001 [00:10<00:04, 19286.25 examples/s]Converting format of dataset (num_proc=2):  69%|██████▉   | 208149/300001 [00:11<00:04, 19373.34 examples/s]Converting format of dataset (num_proc=2):  70%|███████   | 210125/300001 [00:11<00:04, 19478.79 examples/s]Converting format of dataset (num_proc=2):  71%|███████   | 212094/300001 [00:11<00:04, 19535.45 examples/s]Converting format of dataset (num_proc=2):  71%|███████▏  | 214136/300001 [00:12<00:19, 4439.96 examples/s] Converting format of dataset (num_proc=2):  72%|███████▏  | 216000/300001 [00:12<00:14, 5653.24 examples/s]Converting format of dataset (num_proc=2):  73%|███████▎  | 218000/300001 [00:12<00:11, 7205.56 examples/s]Converting format of dataset (num_proc=2):  73%|███████▎  | 220047/300001 [00:12<00:09, 8869.11 examples/s]Converting format of dataset (num_proc=2):  74%|███████▍  | 222175/300001 [00:12<00:07, 10769.59 examples/s]Converting format of dataset (num_proc=2):  75%|███████▍  | 224000/300001 [00:13<00:06, 11996.72 examples/s]Converting format of dataset (num_proc=2):  75%|███████▌  | 226000/300001 [00:13<00:05, 13563.27 examples/s]Converting format of dataset (num_proc=2):  76%|███████▌  | 228000/300001 [00:13<00:04, 14836.91 examples/s]Converting format of dataset (num_proc=2):  77%|███████▋  | 230000/300001 [00:13<00:04, 15780.73 examples/s]Converting format of dataset (num_proc=2):  77%|███████▋  | 232000/300001 [00:13<00:04, 16456.04 examples/s]Converting format of dataset (num_proc=2):  78%|███████▊  | 234000/300001 [00:13<00:03, 17007.87 examples/s]Converting format of dataset (num_proc=2):  79%|███████▊  | 236000/300001 [00:13<00:03, 17172.50 examples/s]Converting format of dataset (num_proc=2):  79%|███████▉  | 238000/300001 [00:13<00:03, 17643.93 examples/s]Converting format of dataset (num_proc=2):  80%|███████▉  | 240000/300001 [00:13<00:03, 18175.67 examples/s]Converting format of dataset (num_proc=2):  81%|████████  | 242000/300001 [00:14<00:03, 18578.90 examples/s]Converting format of dataset (num_proc=2):  81%|████████▏ | 244000/300001 [00:14<00:02, 18906.76 examples/s]Converting format of dataset (num_proc=2):  82%|████████▏ | 246000/300001 [00:14<00:02, 18773.57 examples/s]Converting format of dataset (num_proc=2):  83%|████████▎ | 248000/300001 [00:14<00:02, 18999.23 examples/s]Converting format of dataset (num_proc=2):  83%|████████▎ | 250000/300001 [00:14<00:02, 19158.76 examples/s]Converting format of dataset (num_proc=2):  84%|████████▍ | 252000/300001 [00:14<00:02, 18958.76 examples/s]Converting format of dataset (num_proc=2):  85%|████████▍ | 254000/300001 [00:14<00:02, 19076.49 examples/s]Converting format of dataset (num_proc=2):  85%|████████▌ | 256000/300001 [00:14<00:02, 19240.29 examples/s]Converting format of dataset (num_proc=2):  86%|████████▌ | 258000/300001 [00:14<00:02, 19361.73 examples/s]Converting format of dataset (num_proc=2):  87%|████████▋ | 260000/300001 [00:14<00:02, 18938.29 examples/s]Converting format of dataset (num_proc=2):  87%|████████▋ | 262000/300001 [00:15<00:01, 19109.03 examples/s]Converting format of dataset (num_proc=2):  88%|████████▊ | 264000/300001 [00:15<00:01, 19210.57 examples/s]Converting format of dataset (num_proc=2):  89%|████████▊ | 266000/300001 [00:15<00:01, 19244.97 examples/s]Converting format of dataset (num_proc=2):  89%|████████▉ | 268000/300001 [00:15<00:01, 19368.85 examples/s]Converting format of dataset (num_proc=2):  90%|████████▉ | 270000/300001 [00:15<00:01, 19086.11 examples/s]Converting format of dataset (num_proc=2):  91%|█████████ | 272000/300001 [00:15<00:01, 19075.18 examples/s]Converting format of dataset (num_proc=2):  91%|█████████▏| 274000/300001 [00:15<00:01, 18906.99 examples/s]Converting format of dataset (num_proc=2):  92%|█████████▏| 276000/300001 [00:15<00:01, 19078.14 examples/s]Converting format of dataset (num_proc=2):  93%|█████████▎| 278000/300001 [00:15<00:01, 19305.80 examples/s]Converting format of dataset (num_proc=2):  93%|█████████▎| 280000/300001 [00:16<00:01, 19358.88 examples/s]Converting format of dataset (num_proc=2):  94%|█████████▍| 282000/300001 [00:16<00:00, 19177.88 examples/s]Converting format of dataset (num_proc=2):  95%|█████████▍| 284000/300001 [00:16<00:00, 19287.59 examples/s]Converting format of dataset (num_proc=2):  95%|█████████▌| 286000/300001 [00:16<00:00, 19354.00 examples/s]Converting format of dataset (num_proc=2):  96%|█████████▌| 287993/300001 [00:16<00:00, 19368.26 examples/s]Converting format of dataset (num_proc=2):  97%|█████████▋| 289946/300001 [00:16<00:00, 18909.36 examples/s]Converting format of dataset (num_proc=2):  97%|█████████▋| 291903/300001 [00:16<00:00, 18937.01 examples/s]Converting format of dataset (num_proc=2):  98%|█████████▊| 293867/300001 [00:16<00:00, 19006.92 examples/s]Converting format of dataset (num_proc=2):  99%|█████████▊| 296079/300001 [00:16<00:00, 18805.53 examples/s]Converting format of dataset (num_proc=2):  99%|█████████▉| 298049/300001 [00:16<00:00, 19056.69 examples/s]Converting format of dataset (num_proc=2): 100%|█████████▉| 300000/300001 [00:17<00:00, 6160.29 examples/s] Converting format of dataset (num_proc=2): 100%|██████████| 300001/300001 [00:18<00:00, 16623.92 examples/s]
192.168.0.25: Converting format of dataset (num_proc=2):   0%|          | 0/300001 [00:00<?, ? examples/s]Converting format of dataset (num_proc=2):   0%|          | 552/300001 [00:00<01:11, 4194.43 examples/s]Converting format of dataset (num_proc=2):   1%|          | 2596/300001 [00:00<00:26, 11296.77 examples/s]Converting format of dataset (num_proc=2):   2%|▏         | 4594/300001 [00:00<00:20, 14526.71 examples/s]Converting format of dataset (num_proc=2):   2%|▏         | 6590/300001 [00:00<00:18, 16292.42 examples/s]Converting format of dataset (num_proc=2):   3%|▎         | 8591/300001 [00:00<00:16, 17293.07 examples/s]Converting format of dataset (num_proc=2):   4%|▎         | 10591/300001 [00:00<00:16, 17980.48 examples/s]Converting format of dataset (num_proc=2):   4%|▍         | 12587/300001 [00:00<00:15, 18399.52 examples/s]Converting format of dataset (num_proc=2):   5%|▍         | 14593/300001 [00:00<00:15, 18661.12 examples/s]Converting format of dataset (num_proc=2):   6%|▌         | 16586/300001 [00:00<00:15, 18853.27 examples/s]Converting format of dataset (num_proc=2):   6%|▌         | 18587/300001 [00:01<00:14, 18972.54 examples/s]Converting format of dataset (num_proc=2):   7%|▋         | 20587/300001 [00:01<00:14, 19067.34 examples/s]Converting format of dataset (num_proc=2):   8%|▊         | 22560/300001 [00:01<00:14, 19077.32 examples/s]Converting format of dataset (num_proc=2):   8%|▊         | 24589/300001 [00:01<00:14, 19096.07 examples/s]Converting format of dataset (num_proc=2):   9%|▉         | 26589/300001 [00:01<00:14, 19153.63 examples/s]Converting format of dataset (num_proc=2):  10%|▉         | 28590/300001 [00:01<00:14, 19225.31 examples/s]Converting format of dataset (num_proc=2):  10%|█         | 30525/300001 [00:01<00:14, 19059.87 examples/s]Converting format of dataset (num_proc=2):  11%|█         | 32589/300001 [00:01<00:14, 19038.56 examples/s]Converting format of dataset (num_proc=2):  12%|█▏        | 34590/300001 [00:01<00:13, 19130.40 examples/s]Converting format of dataset (num_proc=2):  12%|█▏        | 36592/300001 [00:02<00:13, 19207.91 examples/s]Converting format of dataset (num_proc=2):  13%|█▎        | 38589/300001 [00:02<00:13, 19277.57 examples/s]Converting format of dataset (num_proc=2):  14%|█▎        | 40554/300001 [00:02<00:13, 19204.88 examples/s]Converting format of dataset (num_proc=2):  14%|█▍        | 42590/300001 [00:02<00:13, 19187.26 examples/s]Converting format of dataset (num_proc=2):  15%|█▍        | 44585/300001 [00:02<00:13, 19231.70 examples/s]Converting format of dataset (num_proc=2):  16%|█▌        | 46588/300001 [00:02<00:13, 19244.54 examples/s]Converting format of dataset (num_proc=2):  16%|█▌        | 48523/300001 [00:02<00:13, 19093.54 examples/s]Converting format of dataset (num_proc=2):  17%|█▋        | 50587/300001 [00:02<00:13, 18690.60 examples/s]Converting format of dataset (num_proc=2):  18%|█▊        | 52595/300001 [00:02<00:13, 18844.05 examples/s]Converting format of dataset (num_proc=2):  18%|█▊        | 54589/300001 [00:02<00:12, 18960.81 examples/s]Converting format of dataset (num_proc=2):  19%|█▉        | 56592/300001 [00:03<00:12, 19008.61 examples/s]Converting format of dataset (num_proc=2):  20%|█▉        | 58589/300001 [00:03<00:12, 19097.12 examples/s]Converting format of dataset (num_proc=2):  20%|██        | 60589/300001 [00:03<00:12, 19122.00 examples/s]Converting format of dataset (num_proc=2):  21%|██        | 62546/300001 [00:03<00:12, 19000.44 examples/s]Converting format of dataset (num_proc=2):  22%|██▏       | 64593/300001 [00:03<00:12, 18466.94 examples/s]Converting format of dataset (num_proc=2):  22%|██▏       | 66552/300001 [00:03<00:12, 18481.12 examples/s]Converting format of dataset (num_proc=2):  23%|██▎       | 68584/300001 [00:03<00:12, 18641.71 examples/s]Converting format of dataset (num_proc=2):  24%|██▎       | 70582/300001 [00:03<00:12, 18780.18 examples/s]Converting format of dataset (num_proc=2):  24%|██▍       | 72587/300001 [00:03<00:12, 18939.81 examples/s]Converting format of dataset (num_proc=2):  25%|██▍       | 74513/300001 [00:04<00:11, 18855.22 examples/s]Converting format of dataset (num_proc=2):  26%|██▌       | 76590/300001 [00:04<00:11, 18853.93 examples/s]Converting format of dataset (num_proc=2):  26%|██▌       | 78594/300001 [00:04<00:11, 18996.59 examples/s]Converting format of dataset (num_proc=2):  27%|██▋       | 80593/300001 [00:04<00:11, 19180.19 examples/s]Converting format of dataset (num_proc=2):  28%|██▊       | 82592/300001 [00:04<00:11, 19248.57 examples/s]Converting format of dataset (num_proc=2):  28%|██▊       | 85094/300001 [00:04<00:10, 19564.31 examples/s]Converting format of dataset (num_proc=2):  29%|██▉       | 87173/300001 [00:04<00:10, 19736.96 examples/s]Converting format of dataset (num_proc=2):  30%|██▉       | 89173/300001 [00:04<00:10, 19637.05 examples/s]Converting format of dataset (num_proc=2):  30%|███       | 91172/300001 [00:04<00:10, 19391.76 examples/s]Converting format of dataset (num_proc=2):  31%|███       | 93169/300001 [00:04<00:10, 19389.75 examples/s]Converting format of dataset (num_proc=2):  32%|███▏      | 95504/300001 [00:05<00:11, 17596.48 examples/s]Converting format of dataset (num_proc=2):  33%|███▎      | 97548/300001 [00:05<00:11, 18331.65 examples/s]Converting format of dataset (num_proc=2):  33%|███▎      | 99456/300001 [00:05<00:10, 18528.66 examples/s]Converting format of dataset (num_proc=2):  34%|███▍      | 101761/300001 [00:05<00:10, 18989.89 examples/s]Converting format of dataset (num_proc=2):  35%|███▍      | 103717/300001 [00:05<00:10, 18979.19 examples/s]Converting format of dataset (num_proc=2):  35%|███▌      | 105675/300001 [00:05<00:10, 18878.70 examples/s]Converting format of dataset (num_proc=2):  36%|███▌      | 107631/300001 [00:05<00:10, 18936.81 examples/s]Converting format of dataset (num_proc=2):  37%|███▋      | 109591/300001 [00:05<00:10, 18987.97 examples/s]Converting format of dataset (num_proc=2):  37%|███▋      | 111588/300001 [00:05<00:09, 19129.50 examples/s]Converting format of dataset (num_proc=2):  38%|███▊      | 113563/300001 [00:06<00:09, 19148.23 examples/s]Converting format of dataset (num_proc=2):  39%|███▊      | 115587/300001 [00:06<00:09, 19273.29 examples/s]Converting format of dataset (num_proc=2):  39%|███▉      | 117585/300001 [00:06<00:09, 19304.16 examples/s]Converting format of dataset (num_proc=2):  40%|███▉      | 119584/300001 [00:06<00:09, 19382.76 examples/s]Converting format of dataset (num_proc=2):  41%|████      | 122000/300001 [00:06<00:09, 18365.82 examples/s]Converting format of dataset (num_proc=2):  41%|████▏     | 124000/300001 [00:06<00:09, 18670.62 examples/s]Converting format of dataset (num_proc=2):  42%|████▏     | 126000/300001 [00:06<00:09, 18875.83 examples/s]Converting format of dataset (num_proc=2):  43%|████▎     | 128000/300001 [00:06<00:09, 19009.43 examples/s]Converting format of dataset (num_proc=2):  43%|████▎     | 130000/300001 [00:06<00:08, 19129.76 examples/s]Converting format of dataset (num_proc=2):  44%|████▍     | 132000/300001 [00:07<00:08, 19148.83 examples/s]Converting format of dataset (num_proc=2):  45%|████▍     | 134000/300001 [00:07<00:08, 19213.36 examples/s]Converting format of dataset (num_proc=2):  45%|████▌     | 136000/300001 [00:07<00:08, 19283.92 examples/s]Converting format of dataset (num_proc=2):  46%|████▌     | 138000/300001 [00:07<00:08, 19342.92 examples/s]Converting format of dataset (num_proc=2):  47%|████▋     | 140000/300001 [00:07<00:08, 19334.77 examples/s]Converting format of dataset (num_proc=2):  47%|████▋     | 142000/300001 [00:07<00:08, 19319.04 examples/s]Converting format of dataset (num_proc=2):  48%|████▊     | 144000/300001 [00:07<00:08, 19118.53 examples/s]Converting format of dataset (num_proc=2):  49%|████▊     | 146000/300001 [00:07<00:08, 19166.79 examples/s]Converting format of dataset (num_proc=2):  49%|████▉     | 148000/300001 [00:07<00:07, 19220.60 examples/s]Converting format of dataset (num_proc=2):  50%|████▉     | 150000/300001 [00:07<00:07, 19256.72 examples/s]Converting format of dataset (num_proc=2):  51%|█████     | 152000/300001 [00:08<00:07, 19286.36 examples/s]Converting format of dataset (num_proc=2):  51%|█████▏    | 154000/300001 [00:08<00:07, 19313.54 examples/s]Converting format of dataset (num_proc=2):  52%|█████▏    | 156000/300001 [00:08<00:07, 19328.26 examples/s]Converting format of dataset (num_proc=2):  53%|█████▎    | 158000/300001 [00:08<00:07, 19347.26 examples/s]Converting format of dataset (num_proc=2):  53%|█████▎    | 160179/300001 [00:08<00:07, 19238.74 examples/s]Converting format of dataset (num_proc=2):  54%|█████▍    | 162175/300001 [00:08<00:07, 19326.05 examples/s]Converting format of dataset (num_proc=2):  55%|█████▍    | 164175/300001 [00:08<00:07, 19378.07 examples/s]Converting format of dataset (num_proc=2):  55%|█████▌    | 166172/300001 [00:08<00:06, 19398.25 examples/s]Converting format of dataset (num_proc=2):  56%|█████▌    | 168177/300001 [00:08<00:06, 19406.36 examples/s]Converting format of dataset (num_proc=2):  57%|█████▋    | 170175/300001 [00:09<00:06, 19366.46 examples/s]Converting format of dataset (num_proc=2):  58%|█████▊    | 172504/300001 [00:09<00:06, 19954.84 examples/s]Converting format of dataset (num_proc=2):  58%|█████▊    | 174584/300001 [00:09<00:06, 19692.76 examples/s]Converting format of dataset (num_proc=2):  59%|█████▉    | 176588/300001 [00:09<00:06, 19124.31 examples/s]Converting format of dataset (num_proc=2):  60%|█████▉    | 178586/300001 [00:09<00:06, 18788.37 examples/s]Converting format of dataset (num_proc=2):  60%|██████    | 180584/300001 [00:09<00:06, 18936.63 examples/s]Converting format of dataset (num_proc=2):  61%|██████    | 182586/300001 [00:09<00:06, 18953.25 examples/s]Converting format of dataset (num_proc=2):  62%|██████▏   | 184583/300001 [00:09<00:06, 19083.58 examples/s]Converting format of dataset (num_proc=2):  62%|██████▏   | 186585/300001 [00:09<00:05, 19208.35 examples/s]Converting format of dataset (num_proc=2):  63%|██████▎   | 188585/300001 [00:09<00:05, 19168.79 examples/s]Converting format of dataset (num_proc=2):  64%|██████▎   | 190586/300001 [00:10<00:05, 18422.61 examples/s]Converting format of dataset (num_proc=2):  64%|██████▍   | 192585/300001 [00:10<00:05, 18677.02 examples/s]Converting format of dataset (num_proc=2):  65%|██████▍   | 194586/300001 [00:10<00:05, 18838.44 examples/s]Converting format of dataset (num_proc=2):  66%|██████▌   | 196582/300001 [00:10<00:05, 18919.87 examples/s]Converting format of dataset (num_proc=2):  66%|██████▌   | 198583/300001 [00:10<00:05, 19089.92 examples/s]Converting format of dataset (num_proc=2):  67%|██████▋   | 200586/300001 [00:10<00:05, 18961.70 examples/s]Converting format of dataset (num_proc=2):  68%|██████▊   | 202588/300001 [00:10<00:05, 18994.86 examples/s]Converting format of dataset (num_proc=2):  68%|██████▊   | 204588/300001 [00:10<00:04, 19108.75 examples/s]Converting format of dataset (num_proc=2):  69%|██████▉   | 206586/300001 [00:10<00:04, 19075.24 examples/s]Converting format of dataset (num_proc=2):  70%|██████▉   | 208591/300001 [00:11<00:04, 19158.26 examples/s]Converting format of dataset (num_proc=2):  70%|███████   | 210586/300001 [00:11<00:04, 19221.63 examples/s]Converting format of dataset (num_proc=2):  71%|███████   | 212585/300001 [00:11<00:04, 19279.35 examples/s]Converting format of dataset (num_proc=2):  72%|███████▏  | 215000/300001 [00:11<00:04, 19699.10 examples/s]Converting format of dataset (num_proc=2):  72%|███████▏  | 217000/300001 [00:11<00:04, 19466.74 examples/s]Converting format of dataset (num_proc=2):  73%|███████▎  | 219016/300001 [00:12<00:12, 6695.60 examples/s] Converting format of dataset (num_proc=2):  74%|███████▍  | 221270/300001 [00:12<00:09, 8432.93 examples/s]Converting format of dataset (num_proc=2):  74%|███████▍  | 223169/300001 [00:12<00:07, 9967.13 examples/s]Converting format of dataset (num_proc=2):  75%|███████▌  | 225068/300001 [00:12<00:06, 11518.94 examples/s]Converting format of dataset (num_proc=2):  76%|███████▌  | 227000/300001 [00:12<00:05, 12893.01 examples/s]Converting format of dataset (num_proc=2):  76%|███████▋  | 229000/300001 [00:12<00:04, 14328.52 examples/s]Converting format of dataset (num_proc=2):  77%|███████▋  | 231000/300001 [00:12<00:04, 15535.93 examples/s]Converting format of dataset (num_proc=2):  78%|███████▊  | 233000/300001 [00:12<00:04, 16301.03 examples/s]Converting format of dataset (num_proc=2):  78%|███████▊  | 235000/300001 [00:13<00:03, 17042.84 examples/s]Converting format of dataset (num_proc=2):  79%|███████▉  | 237000/300001 [00:13<00:03, 17616.13 examples/s]Converting format of dataset (num_proc=2):  80%|███████▉  | 239000/300001 [00:13<00:03, 17700.54 examples/s]Converting format of dataset (num_proc=2):  80%|████████  | 241000/300001 [00:13<00:03, 18111.06 examples/s]Converting format of dataset (num_proc=2):  81%|████████  | 243000/300001 [00:13<00:03, 18407.27 examples/s]Converting format of dataset (num_proc=2):  82%|████████▏ | 245000/300001 [00:13<00:02, 18676.36 examples/s]Converting format of dataset (num_proc=2):  82%|████████▏ | 247000/300001 [00:13<00:02, 18819.71 examples/s]Converting format of dataset (num_proc=2):  83%|████████▎ | 249000/300001 [00:13<00:02, 18950.69 examples/s]Converting format of dataset (num_proc=2):  84%|████████▎ | 251000/300001 [00:13<00:02, 18385.57 examples/s]Converting format of dataset (num_proc=2):  84%|████████▍ | 253000/300001 [00:14<00:02, 18460.71 examples/s]Converting format of dataset (num_proc=2):  85%|████████▍ | 255000/300001 [00:14<00:02, 18567.47 examples/s]Converting format of dataset (num_proc=2):  86%|████████▌ | 257000/300001 [00:14<00:02, 17750.45 examples/s]Converting format of dataset (num_proc=2):  86%|████████▋ | 259000/300001 [00:14<00:02, 18020.09 examples/s]Converting format of dataset (num_proc=2):  87%|████████▋ | 261000/300001 [00:14<00:02, 18291.23 examples/s]Converting format of dataset (num_proc=2):  88%|████████▊ | 263000/300001 [00:14<00:01, 18551.69 examples/s]Converting format of dataset (num_proc=2):  88%|████████▊ | 265000/300001 [00:14<00:01, 18701.04 examples/s]Converting format of dataset (num_proc=2):  89%|████████▉ | 267000/300001 [00:14<00:01, 18521.99 examples/s]Converting format of dataset (num_proc=2):  90%|████████▉ | 269000/300001 [00:14<00:01, 18705.24 examples/s]Converting format of dataset (num_proc=2):  90%|█████████ | 271000/300001 [00:15<00:01, 18773.51 examples/s]Converting format of dataset (num_proc=2):  91%|█████████ | 273000/300001 [00:15<00:01, 18911.14 examples/s]Converting format of dataset (num_proc=2):  92%|█████████▏| 275000/300001 [00:15<00:01, 18296.90 examples/s]Converting format of dataset (num_proc=2):  92%|█████████▏| 277000/300001 [00:15<00:01, 18526.30 examples/s]Converting format of dataset (num_proc=2):  93%|█████████▎| 279000/300001 [00:15<00:01, 18672.44 examples/s]Converting format of dataset (num_proc=2):  94%|█████████▎| 281000/300001 [00:15<00:01, 18790.98 examples/s]Converting format of dataset (num_proc=2):  94%|█████████▍| 283000/300001 [00:15<00:00, 18648.26 examples/s]Converting format of dataset (num_proc=2):  95%|█████████▍| 285000/300001 [00:15<00:00, 18328.96 examples/s]Converting format of dataset (num_proc=2):  96%|█████████▌| 287000/300001 [00:15<00:00, 18394.85 examples/s]Converting format of dataset (num_proc=2):  96%|█████████▋| 289000/300001 [00:16<00:00, 18611.03 examples/s]Converting format of dataset (num_proc=2):  97%|█████████▋| 291000/300001 [00:16<00:00, 18126.56 examples/s]Converting format of dataset (num_proc=2):  98%|█████████▊| 293000/300001 [00:16<00:00, 18197.80 examples/s]Converting format of dataset (num_proc=2):  98%|█████████▊| 295000/300001 [00:16<00:00, 17621.78 examples/s]Converting format of dataset (num_proc=2):  99%|█████████▉| 297000/300001 [00:16<00:00, 17789.15 examples/s]Converting format of dataset (num_proc=2): 100%|█████████▉| 299000/300001 [00:16<00:00, 17940.71 examples/s]Converting format of dataset (num_proc=2): 100%|██████████| 300001/300001 [00:17<00:00, 17381.99 examples/s]
192.168.0.149: Converting format of dataset (num_proc=2):   0%|          | 0/300001 [00:00<?, ? examples/s]Converting format of dataset (num_proc=2):   0%|          | 528/300001 [00:00<01:22, 3627.14 examples/s]Converting format of dataset (num_proc=2):   1%|          | 2588/300001 [00:00<00:28, 10616.64 examples/s]Converting format of dataset (num_proc=2):   2%|▏         | 4586/300001 [00:00<00:21, 13938.48 examples/s]Converting format of dataset (num_proc=2):   2%|▏         | 6580/300001 [00:00<00:18, 15816.84 examples/s]Converting format of dataset (num_proc=2):   3%|▎         | 8582/300001 [00:00<00:17, 16905.64 examples/s]Converting format of dataset (num_proc=2):   4%|▎         | 10575/300001 [00:00<00:16, 17563.94 examples/s]Converting format of dataset (num_proc=2):   4%|▍         | 12571/300001 [00:00<00:15, 17995.11 examples/s]Converting format of dataset (num_proc=2):   5%|▍         | 14573/300001 [00:00<00:15, 18250.14 examples/s]Converting format of dataset (num_proc=2):   6%|▌         | 16575/300001 [00:01<00:15, 18442.38 examples/s]Converting format of dataset (num_proc=2):   6%|▌         | 18575/300001 [00:01<00:15, 18599.89 examples/s]Converting format of dataset (num_proc=2):   7%|▋         | 20573/300001 [00:01<00:14, 18694.84 examples/s]Converting format of dataset (num_proc=2):   8%|▊         | 22572/300001 [00:01<00:14, 18740.77 examples/s]Converting format of dataset (num_proc=2):   8%|▊         | 24603/300001 [00:01<00:14, 18884.49 examples/s]Converting format of dataset (num_proc=2):   9%|▉         | 26602/300001 [00:01<00:14, 19082.00 examples/s]Converting format of dataset (num_proc=2):  10%|▉         | 28598/300001 [00:01<00:14, 19180.02 examples/s]Converting format of dataset (num_proc=2):  10%|█         | 30600/300001 [00:01<00:14, 19198.74 examples/s]Converting format of dataset (num_proc=2):  11%|█         | 32600/300001 [00:01<00:13, 19307.64 examples/s]Converting format of dataset (num_proc=2):  12%|█▏        | 34599/300001 [00:01<00:13, 19361.33 examples/s]Converting format of dataset (num_proc=2):  12%|█▏        | 36603/300001 [00:02<00:13, 19228.26 examples/s]Converting format of dataset (num_proc=2):  13%|█▎        | 38601/300001 [00:02<00:13, 19042.61 examples/s]Converting format of dataset (num_proc=2):  14%|█▎        | 40602/300001 [00:02<00:13, 18968.95 examples/s]Converting format of dataset (num_proc=2):  14%|█▍        | 42601/300001 [00:02<00:13, 18875.84 examples/s]Converting format of dataset (num_proc=2):  15%|█▍        | 44597/300001 [00:02<00:13, 18823.99 examples/s]Converting format of dataset (num_proc=2):  16%|█▌        | 46599/300001 [00:02<00:13, 18443.02 examples/s]Converting format of dataset (num_proc=2):  16%|█▌        | 48603/300001 [00:02<00:13, 18450.92 examples/s]Converting format of dataset (num_proc=2):  17%|█▋        | 50598/300001 [00:02<00:13, 18352.56 examples/s]Converting format of dataset (num_proc=2):  18%|█▊        | 52564/300001 [00:02<00:13, 18371.24 examples/s]Converting format of dataset (num_proc=2):  18%|█▊        | 54566/300001 [00:03<00:13, 18685.74 examples/s]Converting format of dataset (num_proc=2):  19%|█▉        | 56557/300001 [00:03<00:12, 18935.12 examples/s]Converting format of dataset (num_proc=2):  20%|█▉        | 58568/300001 [00:03<00:12, 19146.57 examples/s]Converting format of dataset (num_proc=2):  20%|██        | 60565/300001 [00:03<00:12, 18974.22 examples/s]Converting format of dataset (num_proc=2):  21%|██        | 62568/300001 [00:03<00:12, 18844.94 examples/s]Converting format of dataset (num_proc=2):  22%|██▏       | 64566/300001 [00:03<00:12, 18765.95 examples/s]Converting format of dataset (num_proc=2):  22%|██▏       | 66568/300001 [00:03<00:12, 18742.41 examples/s]Converting format of dataset (num_proc=2):  23%|██▎       | 68564/300001 [00:03<00:12, 18672.19 examples/s]Converting format of dataset (num_proc=2):  24%|██▎       | 70562/300001 [00:03<00:12, 18688.05 examples/s]Converting format of dataset (num_proc=2):  24%|██▍       | 72566/300001 [00:03<00:12, 18718.87 examples/s]Converting format of dataset (num_proc=2):  25%|██▍       | 74568/300001 [00:04<00:12, 18723.92 examples/s]Converting format of dataset (num_proc=2):  26%|██▌       | 76565/300001 [00:04<00:11, 18666.84 examples/s]Converting format of dataset (num_proc=2):  26%|██▌       | 78549/300001 [00:04<00:11, 18659.75 examples/s]Converting format of dataset (num_proc=2):  27%|██▋       | 80564/300001 [00:04<00:11, 18718.68 examples/s]Converting format of dataset (num_proc=2):  28%|██▊       | 82569/300001 [00:04<00:11, 18668.35 examples/s]Converting format of dataset (num_proc=2):  28%|██▊       | 84570/300001 [00:04<00:11, 18710.01 examples/s]Converting format of dataset (num_proc=2):  29%|██▉       | 86568/300001 [00:04<00:11, 18712.59 examples/s]Converting format of dataset (num_proc=2):  30%|██▉       | 88569/300001 [00:04<00:11, 18745.47 examples/s]Converting format of dataset (num_proc=2):  30%|███       | 90569/300001 [00:04<00:11, 18744.38 examples/s]Converting format of dataset (num_proc=2):  31%|███       | 92569/300001 [00:05<00:11, 18735.42 examples/s]Converting format of dataset (num_proc=2):  32%|███▏      | 94567/300001 [00:05<00:10, 18748.30 examples/s]Converting format of dataset (num_proc=2):  32%|███▏      | 96571/300001 [00:05<00:10, 18745.18 examples/s]Converting format of dataset (num_proc=2):  33%|███▎      | 98565/300001 [00:05<00:10, 18643.28 examples/s]Converting format of dataset (num_proc=2):  34%|███▎      | 100567/300001 [00:05<00:10, 18684.66 examples/s]Converting format of dataset (num_proc=2):  34%|███▍      | 102571/300001 [00:05<00:10, 18709.94 examples/s]Converting format of dataset (num_proc=2):  35%|███▍      | 104568/300001 [00:05<00:10, 18733.18 examples/s]Converting format of dataset (num_proc=2):  36%|███▌      | 106536/300001 [00:05<00:10, 18670.02 examples/s]Converting format of dataset (num_proc=2):  36%|███▌      | 108558/300001 [00:05<00:10, 18785.23 examples/s]Converting format of dataset (num_proc=2):  37%|███▋      | 110564/300001 [00:06<00:10, 18811.22 examples/s]Converting format of dataset (num_proc=2):  38%|███▊      | 112561/300001 [00:06<00:09, 18822.71 examples/s]Converting format of dataset (num_proc=2):  38%|███▊      | 114559/300001 [00:06<00:09, 18818.51 examples/s]Converting format of dataset (num_proc=2):  39%|███▉      | 116561/300001 [00:06<00:09, 18818.69 examples/s]Converting format of dataset (num_proc=2):  40%|███▉      | 118559/300001 [00:06<00:09, 18845.09 examples/s]Converting format of dataset (num_proc=2):  40%|████      | 120561/300001 [00:06<00:09, 18848.14 examples/s]Converting format of dataset (num_proc=2):  41%|████      | 122557/300001 [00:06<00:09, 18737.77 examples/s]Converting format of dataset (num_proc=2):  42%|████▏     | 124559/300001 [00:06<00:09, 18603.87 examples/s]Converting format of dataset (num_proc=2):  42%|████▏     | 126559/300001 [00:06<00:09, 18521.98 examples/s]Converting format of dataset (num_proc=2):  43%|████▎     | 128560/300001 [00:06<00:09, 18506.27 examples/s]Converting format of dataset (num_proc=2):  44%|████▎     | 130559/300001 [00:07<00:09, 18457.10 examples/s]Converting format of dataset (num_proc=2):  44%|████▍     | 132560/300001 [00:07<00:09, 18482.32 examples/s]Converting format of dataset (num_proc=2):  45%|████▍     | 134560/300001 [00:07<00:08, 18479.35 examples/s]Converting format of dataset (num_proc=2):  46%|████▌     | 136558/300001 [00:07<00:08, 18447.23 examples/s]Converting format of dataset (num_proc=2):  46%|████▌     | 138559/300001 [00:07<00:08, 18437.27 examples/s]Converting format of dataset (num_proc=2):  47%|████▋     | 140557/300001 [00:07<00:08, 18426.23 examples/s]Converting format of dataset (num_proc=2):  47%|████▋     | 142467/300001 [00:07<00:08, 18160.35 examples/s]Converting format of dataset (num_proc=2):  48%|████▊     | 144557/300001 [00:07<00:08, 18025.35 examples/s]Converting format of dataset (num_proc=2):  49%|████▉     | 146560/300001 [00:07<00:08, 17886.57 examples/s]Converting format of dataset (num_proc=2):  49%|████▉     | 148493/300001 [00:08<00:08, 17832.08 examples/s]Converting format of dataset (num_proc=2):  50%|█████     | 150543/300001 [00:08<00:08, 17782.59 examples/s]Converting format of dataset (num_proc=2):  51%|█████     | 152574/300001 [00:08<00:08, 17880.44 examples/s]Converting format of dataset (num_proc=2):  51%|█████▏    | 154452/300001 [00:08<00:08, 17832.70 examples/s]Converting format of dataset (num_proc=2):  52%|█████▏    | 156508/300001 [00:08<00:07, 18226.71 examples/s]Converting format of dataset (num_proc=2):  53%|█████▎    | 158365/300001 [00:08<00:07, 18318.04 examples/s]Converting format of dataset (num_proc=2):  53%|█████▎    | 160228/300001 [00:08<00:07, 18401.76 examples/s]Converting format of dataset (num_proc=2):  54%|█████▍    | 162114/300001 [00:08<00:07, 18276.10 examples/s]Converting format of dataset (num_proc=2):  55%|█████▍    | 164060/300001 [00:08<00:07, 17993.74 examples/s]Converting format of dataset (num_proc=2):  55%|█████▌    | 166003/300001 [00:09<00:07, 17987.98 examples/s]Converting format of dataset (num_proc=2):  56%|█████▌    | 168000/300001 [00:09<00:07, 18147.35 examples/s]Converting format of dataset (num_proc=2):  57%|█████▋    | 170000/300001 [00:09<00:07, 18231.89 examples/s]Converting format of dataset (num_proc=2):  57%|█████▋    | 172059/300001 [00:09<00:07, 17727.72 examples/s]Converting format of dataset (num_proc=2):  58%|█████▊    | 174145/300001 [00:09<00:06, 18413.83 examples/s]Converting format of dataset (num_proc=2):  59%|█████▉    | 176377/300001 [00:09<00:06, 18257.90 examples/s]Converting format of dataset (num_proc=2):  59%|█████▉    | 178330/300001 [00:09<00:06, 18073.04 examples/s]Converting format of dataset (num_proc=2):  60%|██████    | 180295/300001 [00:09<00:06, 17795.33 examples/s]Converting format of dataset (num_proc=2):  61%|██████    | 182425/300001 [00:09<00:06, 18480.43 examples/s]Converting format of dataset (num_proc=2):  61%|██████▏   | 184391/300001 [00:10<00:06, 18802.54 examples/s]Converting format of dataset (num_proc=2):  62%|██████▏   | 186365/300001 [00:10<00:05, 19064.46 examples/s]Converting format of dataset (num_proc=2):  63%|██████▎   | 188335/300001 [00:10<00:05, 19241.83 examples/s]Converting format of dataset (num_proc=2):  63%|██████▎   | 190297/300001 [00:10<00:05, 19345.85 examples/s]Converting format of dataset (num_proc=2):  64%|██████▍   | 192266/300001 [00:10<00:05, 19438.86 examples/s]Converting format of dataset (num_proc=2):  65%|██████▍   | 194218/300001 [00:10<00:05, 19460.13 examples/s]Converting format of dataset (num_proc=2):  65%|██████▌   | 196188/300001 [00:10<00:05, 19528.40 examples/s]Converting format of dataset (num_proc=2):  66%|██████▌   | 198162/300001 [00:10<00:05, 19264.57 examples/s]Converting format of dataset (num_proc=2):  67%|██████▋   | 200158/300001 [00:10<00:05, 19033.38 examples/s]Converting format of dataset (num_proc=2):  67%|██████▋   | 202154/300001 [00:10<00:05, 18458.06 examples/s]Converting format of dataset (num_proc=2):  68%|██████▊   | 204160/300001 [00:11<00:05, 18560.15 examples/s]Converting format of dataset (num_proc=2):  69%|██████▊   | 206158/300001 [00:11<00:05, 18633.26 examples/s]Converting format of dataset (num_proc=2):  69%|██████▉   | 208160/300001 [00:11<00:04, 18650.35 examples/s]Converting format of dataset (num_proc=2):  70%|███████   | 210155/300001 [00:11<00:04, 18654.72 examples/s]Converting format of dataset (num_proc=2):  71%|███████   | 212163/300001 [00:11<00:04, 18634.48 examples/s]Converting format of dataset (num_proc=2):  71%|███████▏  | 214154/300001 [00:11<00:04, 18648.75 examples/s]Converting format of dataset (num_proc=2):  72%|███████▏  | 216562/300001 [00:11<00:05, 15254.27 examples/s]Converting format of dataset (num_proc=2):  73%|███████▎  | 218211/300001 [00:12<00:10, 8008.07 examples/s] Converting format of dataset (num_proc=2):  73%|███████▎  | 219789/300001 [00:12<00:09, 8526.60 examples/s]Converting format of dataset (num_proc=2):  74%|███████▎  | 221116/300001 [00:12<00:09, 8597.32 examples/s]Converting format of dataset (num_proc=2):  74%|███████▍  | 222297/300001 [00:12<00:08, 8979.14 examples/s]Converting format of dataset (num_proc=2):  75%|███████▍  | 224690/300001 [00:12<00:06, 11114.06 examples/s]Converting format of dataset (num_proc=2):  76%|███████▌  | 226632/300001 [00:12<00:05, 12850.58 examples/s]Converting format of dataset (num_proc=2):  76%|███████▌  | 228606/300001 [00:13<00:04, 14435.27 examples/s]Converting format of dataset (num_proc=2):  77%|███████▋  | 230595/300001 [00:13<00:04, 15752.69 examples/s]Converting format of dataset (num_proc=2):  78%|███████▊  | 232588/300001 [00:13<00:04, 16783.80 examples/s]Converting format of dataset (num_proc=2):  78%|███████▊  | 234596/300001 [00:13<00:03, 17563.64 examples/s]Converting format of dataset (num_proc=2):  79%|███████▉  | 236591/300001 [00:13<00:03, 18129.76 examples/s]Converting format of dataset (num_proc=2):  80%|███████▉  | 238592/300001 [00:13<00:03, 18283.70 examples/s]Converting format of dataset (num_proc=2):  80%|████████  | 240588/300001 [00:13<00:03, 18362.89 examples/s]Converting format of dataset (num_proc=2):  81%|████████  | 242592/300001 [00:13<00:03, 17994.92 examples/s]Converting format of dataset (num_proc=2):  82%|████████▏ | 244589/300001 [00:13<00:03, 18154.35 examples/s]Converting format of dataset (num_proc=2):  82%|████████▏ | 246594/300001 [00:14<00:02, 18237.44 examples/s]Converting format of dataset (num_proc=2):  83%|████████▎ | 248587/300001 [00:14<00:02, 18297.41 examples/s]Converting format of dataset (num_proc=2):  84%|████████▎ | 250593/300001 [00:14<00:02, 18350.29 examples/s]Converting format of dataset (num_proc=2):  84%|████████▍ | 252585/300001 [00:14<00:02, 18371.20 examples/s]Converting format of dataset (num_proc=2):  85%|████████▍ | 254582/300001 [00:14<00:02, 18393.28 examples/s]Converting format of dataset (num_proc=2):  86%|████████▌ | 256579/300001 [00:14<00:02, 18375.81 examples/s]Converting format of dataset (num_proc=2):  86%|████████▌ | 258553/300001 [00:14<00:02, 18141.04 examples/s]Converting format of dataset (num_proc=2):  87%|████████▋ | 260554/300001 [00:14<00:02, 18385.91 examples/s]Converting format of dataset (num_proc=2):  88%|████████▊ | 262555/300001 [00:14<00:02, 18615.90 examples/s]Converting format of dataset (num_proc=2):  88%|████████▊ | 264557/300001 [00:14<00:01, 18755.94 examples/s]Converting format of dataset (num_proc=2):  89%|████████▉ | 266555/300001 [00:15<00:01, 18811.63 examples/s]Converting format of dataset (num_proc=2):  90%|████████▉ | 268558/300001 [00:15<00:01, 18581.39 examples/s]Converting format of dataset (num_proc=2):  90%|█████████ | 270554/300001 [00:15<00:01, 18807.77 examples/s]Converting format of dataset (num_proc=2):  91%|█████████ | 272555/300001 [00:15<00:01, 19050.69 examples/s]Converting format of dataset (num_proc=2):  92%|█████████▏| 274553/300001 [00:15<00:01, 18784.77 examples/s]Converting format of dataset (num_proc=2):  92%|█████████▏| 276554/300001 [00:15<00:01, 19002.61 examples/s]Converting format of dataset (num_proc=2):  93%|█████████▎| 278556/300001 [00:15<00:01, 19195.69 examples/s]Converting format of dataset (num_proc=2):  94%|█████████▎| 280548/300001 [00:15<00:01, 19236.49 examples/s]Converting format of dataset (num_proc=2):  94%|█████████▍| 282551/300001 [00:15<00:00, 19316.03 examples/s]Converting format of dataset (num_proc=2):  95%|█████████▍| 284554/300001 [00:16<00:00, 19367.44 examples/s]Converting format of dataset (num_proc=2):  96%|█████████▌| 286546/300001 [00:16<00:00, 19148.69 examples/s]Converting format of dataset (num_proc=2):  96%|█████████▌| 288553/300001 [00:16<00:00, 18881.91 examples/s]Converting format of dataset (num_proc=2):  97%|█████████▋| 290552/300001 [00:16<00:00, 18757.51 examples/s]Converting format of dataset (num_proc=2):  98%|█████████▊| 292548/300001 [00:16<00:00, 18608.04 examples/s]Converting format of dataset (num_proc=2):  98%|█████████▊| 294490/300001 [00:16<00:00, 18176.93 examples/s]Converting format of dataset (num_proc=2):  99%|█████████▉| 296560/300001 [00:16<00:00, 14004.49 examples/s]Converting format of dataset (num_proc=2): 100%|█████████▉| 298565/300001 [00:17<00:00, 11996.57 examples/s]Converting format of dataset (num_proc=2): 100%|██████████| 300001/300001 [00:17<00:00, 10967.89 examples/s]Converting format of dataset (num_proc=2): 100%|██████████| 300001/300001 [00:17<00:00, 16925.83 examples/s]
192.168.0.89: 09/29/2024 11:26:57 - INFO - llamafactory.data.loader - Loading dataset pt_512_c4_format.json...
192.168.0.89: 09/29/2024 11:26:57 - INFO - llamafactory.data.loader - Loading dataset pt_512_c4_format.json...
192.168.0.89: 09/29/2024 11:26:57 - INFO - llamafactory.data.loader - Loading dataset pt_512_c4_format.json...
192.168.0.13: 09/29/2024 11:27:13 - INFO - llamafactory.data.loader - Loading dataset pt_512_c4_format.json...
192.168.0.13: 09/29/2024 11:27:13 - INFO - llamafactory.data.loader - Loading dataset pt_512_c4_format.json...
192.168.0.89: 09/29/2024 11:26:57 - INFO - llamafactory.data.loader - Loading dataset pt_512_c4_format.json...
192.168.0.149: 09/29/2024 11:27:16 - INFO - llamafactory.data.loader - Loading dataset pt_512_c4_format.json...
192.168.0.149: 09/29/2024 11:27:16 - INFO - llamafactory.data.loader - Loading dataset pt_512_c4_format.json...
192.168.0.149: 09/29/2024 11:27:16 - INFO - llamafactory.data.loader - Loading dataset pt_512_c4_format.json...
192.168.0.13: 09/29/2024 11:27:13 - INFO - llamafactory.data.loader - Loading dataset pt_512_c4_format.json...
192.168.0.89: 09/29/2024 11:26:57 - INFO - llamafactory.data.loader - Loading dataset pt_512_c4_format.json...
192.168.0.89: 09/29/2024 11:26:57 - INFO - llamafactory.data.loader - Loading dataset pt_512_c4_format.json...
192.168.0.25: 09/29/2024 11:27:13 - INFO - llamafactory.data.loader - Loading dataset pt_512_c4_format.json...
192.168.0.89: 09/29/2024 11:26:57 - INFO - llamafactory.data.loader - Loading dataset pt_512_c4_format.json...
192.168.0.149: 09/29/2024 11:27:16 - INFO - llamafactory.data.loader - Loading dataset pt_512_c4_format.json...
192.168.0.25: 09/29/2024 11:27:13 - INFO - llamafactory.data.loader - Loading dataset pt_512_c4_format.json...
192.168.0.149: 09/29/2024 11:27:16 - INFO - llamafactory.data.loader - Loading dataset pt_512_c4_format.json...
192.168.0.13: 09/29/2024 11:27:13 - INFO - llamafactory.data.loader - Loading dataset pt_512_c4_format.json...
192.168.0.25: 09/29/2024 11:27:13 - INFO - llamafactory.data.loader - Loading dataset pt_512_c4_format.json...
192.168.0.149: 09/29/2024 11:27:16 - INFO - llamafactory.data.loader - Loading dataset pt_512_c4_format.json...
192.168.0.13: 09/29/2024 11:27:13 - INFO - llamafactory.data.loader - Loading dataset pt_512_c4_format.json...
192.168.0.25: 09/29/2024 11:27:13 - INFO - llamafactory.data.loader - Loading dataset pt_512_c4_format.json...
192.168.0.25: 09/29/2024 11:27:13 - INFO - llamafactory.data.loader - Loading dataset pt_512_c4_format.json...
192.168.0.149: 09/29/2024 11:27:16 - INFO - llamafactory.data.loader - Loading dataset pt_512_c4_format.json...
192.168.0.25: 09/29/2024 11:27:13 - INFO - llamafactory.data.loader - Loading dataset pt_512_c4_format.json...
192.168.0.25: 09/29/2024 11:27:13 - INFO - llamafactory.data.loader - Loading dataset pt_512_c4_format.json...
192.168.0.13: 09/29/2024 11:27:13 - INFO - llamafactory.data.loader - Loading dataset pt_512_c4_format.json...
192.168.0.13: 09/29/2024 11:27:13 - INFO - llamafactory.data.loader - Loading dataset pt_512_c4_format.json...
192.168.0.89: Running tokenizer on dataset (num_proc=2):   0%|          | 0/300001 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=2):   0%|          | 1000/300001 [00:02<11:56, 417.54 examples/s]Running tokenizer on dataset (num_proc=2):   1%|          | 2000/300001 [00:02<05:21, 925.98 examples/s]Running tokenizer on dataset (num_proc=2):   1%|          | 3000/300001 [00:03<05:40, 872.18 examples/s]Running tokenizer on dataset (num_proc=2):   2%|▏         | 5000/300001 [00:05<04:13, 1163.76 examples/s]Running tokenizer on dataset (num_proc=2):   2%|▏         | 6000/300001 [00:05<03:35, 1362.54 examples/s]Running tokenizer on dataset (num_proc=2):   2%|▏         | 7000/300001 [00:06<03:43, 1309.90 examples/s]Running tokenizer on dataset (num_proc=2):   3%|▎         | 8000/300001 [00:06<03:20, 1454.48 examples/s]Running tokenizer on dataset (num_proc=2):   3%|▎         | 9000/300001 [00:07<03:26, 1412.51 examples/s]Running tokenizer on dataset (num_proc=2):   3%|▎         | 10000/300001 [00:08<03:13, 1500.10 examples/s]Running tokenizer on dataset (num_proc=2):   4%|▎         | 11000/300001 [00:08<03:11, 1506.25 examples/s]Running tokenizer on dataset (num_proc=2):   4%|▍         | 12000/300001 [00:09<03:29, 1374.92 examples/s]Running tokenizer on dataset (num_proc=2):   4%|▍         | 13000/300001 [00:10<02:57, 1612.40 examples/s]Running tokenizer on dataset (num_proc=2):   5%|▍         | 14000/300001 [00:11<03:40, 1299.67 examples/s]Running tokenizer on dataset (num_proc=2):   5%|▌         | 16000/300001 [00:12<03:24, 1387.27 examples/s]Running tokenizer on dataset (num_proc=2):   6%|▌         | 17000/300001 [00:12<02:40, 1763.05 examples/s]Running tokenizer on dataset (num_proc=2):   6%|▌         | 18000/300001 [00:13<03:26, 1364.02 examples/s]Running tokenizer on dataset (num_proc=2):   6%|▋         | 19000/300001 [00:14<02:49, 1654.90 examples/s]Running tokenizer on dataset (num_proc=2):   7%|▋         | 20000/300001 [00:15<03:21, 1389.76 examples/s]Running tokenizer on dataset (num_proc=2):   7%|▋         | 21000/300001 [00:15<02:58, 1565.62 examples/s]Running tokenizer on dataset (num_proc=2):   7%|▋         | 22000/300001 [00:16<03:07, 1484.24 examples/s]Running tokenizer on dataset (num_proc=2):   8%|▊         | 23000/300001 [00:16<02:53, 1592.12 examples/s]Running tokenizer on dataset (num_proc=2):   8%|▊         | 24000/300001 [00:17<03:07, 1468.15 examples/s]Running tokenizer on dataset (num_proc=2):   8%|▊         | 25000/300001 [00:18<02:49, 1624.87 examples/s]Running tokenizer on dataset (num_proc=2):   9%|▊         | 26000/300001 [00:18<03:10, 1435.49 examples/s]Running tokenizer on dataset (num_proc=2):   9%|▉         | 27000/300001 [00:19<02:42, 1678.98 examples/s]Running tokenizer on dataset (num_proc=2):   9%|▉         | 28000/300001 [00:20<03:34, 1270.05 examples/s]Running tokenizer on dataset (num_proc=2):  10%|▉         | 29000/300001 [00:20<03:06, 1454.29 examples/s]Running tokenizer on dataset (num_proc=2):  10%|▉         | 30000/300001 [00:22<03:50, 1170.25 examples/s]Running tokenizer on dataset (num_proc=2):  10%|█         | 31000/300001 [00:22<03:04, 1455.81 examples/s]Running tokenizer on dataset (num_proc=2):  11%|█         | 32000/300001 [00:23<03:23, 1318.26 examples/s]Running tokenizer on dataset (num_proc=2):  11%|█         | 33000/300001 [00:23<02:48, 1582.16 examples/s]Running tokenizer on dataset (num_proc=2):  11%|█▏        | 34000/300001 [00:24<03:07, 1420.87 examples/s]Running tokenizer on dataset (num_proc=2):  12%|█▏        | 35000/300001 [00:25<02:43, 1625.18 examples/s]Running tokenizer on dataset (num_proc=2):  12%|█▏        | 36000/300001 [00:25<03:00, 1465.68 examples/s]Running tokenizer on dataset (num_proc=2):  12%|█▏        | 37000/300001 [00:26<02:33, 1709.68 examples/s]Running tokenizer on dataset (num_proc=2):  13%|█▎        | 38000/300001 [00:27<02:51, 1526.68 examples/s]Running tokenizer on dataset (num_proc=2):  13%|█▎        | 39000/300001 [00:27<02:27, 1763.64 examples/s]Running tokenizer on dataset (num_proc=2):  13%|█▎        | 40000/300001 [00:28<02:49, 1532.26 examples/s]Running tokenizer on dataset (num_proc=2):  14%|█▎        | 41000/300001 [00:28<02:28, 1746.29 examples/s]Running tokenizer on dataset (num_proc=2):  14%|█▍        | 42000/300001 [00:29<02:47, 1544.30 examples/s]Running tokenizer on dataset (num_proc=2):  14%|█▍        | 43000/300001 [00:29<02:26, 1749.18 examples/s]Running tokenizer on dataset (num_proc=2):  15%|█▍        | 44000/300001 [00:30<02:45, 1549.15 examples/s]Running tokenizer on dataset (num_proc=2):  15%|█▍        | 45000/300001 [00:31<02:25, 1753.01 examples/s]Running tokenizer on dataset (num_proc=2):  15%|█▌        | 46000/300001 [00:31<02:47, 1515.29 examples/s]Running tokenizer on dataset (num_proc=2):  16%|█▌        | 47000/300001 [00:32<02:23, 1761.38 examples/s]Running tokenizer on dataset (num_proc=2):  16%|█▌        | 48000/300001 [00:33<02:52, 1460.73 examples/s]Running tokenizer on dataset (num_proc=2):  16%|█▋        | 49000/300001 [00:33<02:18, 1817.43 examples/s]Running tokenizer on dataset (num_proc=2):  17%|█▋        | 50000/300001 [00:34<02:50, 1463.74 examples/s]Running tokenizer on dataset (num_proc=2):  17%|█▋        | 51000/300001 [00:34<02:13, 1859.20 examples/s]Running tokenizer on dataset (num_proc=2):  17%|█▋        | 52000/300001 [00:35<02:54, 1422.85 examples/s]Running tokenizer on dataset (num_proc=2):  18%|█▊        | 53000/300001 [00:35<02:12, 1865.02 examples/s]Running tokenizer on dataset (num_proc=2):  18%|█▊        | 54000/300001 [00:36<02:47, 1471.13 examples/s]Running tokenizer on dataset (num_proc=2):  18%|█▊        | 55000/300001 [00:37<02:14, 1819.08 examples/s]Running tokenizer on dataset (num_proc=2):  19%|█▊        | 56000/300001 [00:38<02:43, 1494.34 examples/s]Running tokenizer on dataset (num_proc=2):  19%|█▉        | 57000/300001 [00:38<02:12, 1830.30 examples/s]Running tokenizer on dataset (num_proc=2):  19%|█▉        | 58000/300001 [00:39<02:43, 1480.37 examples/s]Running tokenizer on dataset (num_proc=2):  20%|█▉        | 59000/300001 [00:39<02:09, 1854.81 examples/s]Running tokenizer on dataset (num_proc=2):  20%|█▉        | 60000/300001 [00:40<02:47, 1432.30 examples/s]Running tokenizer on dataset (num_proc=2):  20%|██        | 61000/300001 [00:40<02:07, 1868.88 examples/s]Running tokenizer on dataset (num_proc=2):  21%|██        | 62000/300001 [00:41<02:45, 1434.34 examples/s]Running tokenizer on dataset (num_proc=2):  21%|██        | 63000/300001 [00:42<02:05, 1885.51 examples/s]Running tokenizer on dataset (num_proc=2):  21%|██▏       | 64000/300001 [00:43<02:46, 1419.43 examples/s]Running tokenizer on dataset (num_proc=2):  22%|██▏       | 65000/300001 [00:43<02:03, 1900.38 examples/s]Running tokenizer on dataset (num_proc=2):  22%|██▏       | 66000/300001 [00:44<02:43, 1432.30 examples/s]Running tokenizer on dataset (num_proc=2):  23%|██▎       | 68000/300001 [00:45<02:30, 1542.43 examples/s]Running tokenizer on dataset (num_proc=2):  23%|██▎       | 69000/300001 [00:45<01:59, 1929.74 examples/s]Running tokenizer on dataset (num_proc=2):  23%|██▎       | 70000/300001 [00:46<02:37, 1460.99 examples/s]Running tokenizer on dataset (num_proc=2):  24%|██▍       | 72000/300001 [00:48<02:28, 1532.23 examples/s]Running tokenizer on dataset (num_proc=2):  25%|██▍       | 74000/300001 [00:49<02:23, 1574.34 examples/s]Running tokenizer on dataset (num_proc=2):  25%|██▍       | 75000/300001 [00:49<02:15, 1659.58 examples/s]Running tokenizer on dataset (num_proc=2):  25%|██▌       | 76000/300001 [00:51<02:49, 1324.13 examples/s]Running tokenizer on dataset (num_proc=2):  26%|██▌       | 78000/300001 [00:52<02:34, 1432.99 examples/s]Running tokenizer on dataset (num_proc=2):  27%|██▋       | 80000/300001 [00:53<02:25, 1515.38 examples/s]Running tokenizer on dataset (num_proc=2):  27%|██▋       | 81000/300001 [00:53<01:59, 1828.82 examples/s]Running tokenizer on dataset (num_proc=2):  27%|██▋       | 82000/300001 [00:54<02:26, 1485.32 examples/s]Running tokenizer on dataset (num_proc=2):  28%|██▊       | 83000/300001 [00:54<01:59, 1810.19 examples/s]Running tokenizer on dataset (num_proc=2):  28%|██▊       | 84000/300001 [00:55<02:29, 1446.11 examples/s]Running tokenizer on dataset (num_proc=2):  28%|██▊       | 85000/300001 [00:56<01:57, 1825.52 examples/s]Running tokenizer on dataset (num_proc=2):  29%|██▊       | 86000/300001 [00:57<02:28, 1440.19 examples/s]Running tokenizer on dataset (num_proc=2):  29%|██▉       | 87000/300001 [00:57<01:54, 1854.36 examples/s]Running tokenizer on dataset (num_proc=2):  29%|██▉       | 88000/300001 [00:58<02:28, 1429.20 examples/s]Running tokenizer on dataset (num_proc=2):  30%|██▉       | 89000/300001 [00:58<01:53, 1854.28 examples/s]Running tokenizer on dataset (num_proc=2):  30%|██▉       | 90000/300001 [00:59<02:27, 1426.98 examples/s]Running tokenizer on dataset (num_proc=2):  30%|███       | 91000/300001 [00:59<01:58, 1768.85 examples/s]Running tokenizer on dataset (num_proc=2):  31%|███       | 92000/300001 [01:00<02:22, 1459.30 examples/s]Running tokenizer on dataset (num_proc=2):  31%|███       | 93000/300001 [01:01<02:03, 1673.54 examples/s]Running tokenizer on dataset (num_proc=2):  31%|███▏      | 94000/300001 [01:02<02:17, 1493.15 examples/s]Running tokenizer on dataset (num_proc=2):  32%|███▏      | 95000/300001 [01:02<02:00, 1702.18 examples/s]Running tokenizer on dataset (num_proc=2):  32%|███▏      | 96000/300001 [01:03<02:13, 1529.73 examples/s]Running tokenizer on dataset (num_proc=2):  32%|███▏      | 97000/300001 [01:03<02:05, 1618.55 examples/s]Running tokenizer on dataset (num_proc=2):  33%|███▎      | 98000/300001 [01:04<02:07, 1578.87 examples/s]Running tokenizer on dataset (num_proc=2):  33%|███▎      | 99000/300001 [01:05<02:03, 1633.27 examples/s]Running tokenizer on dataset (num_proc=2):  33%|███▎      | 100000/300001 [01:05<02:05, 1598.89 examples/s]Running tokenizer on dataset (num_proc=2):  34%|███▎      | 101000/300001 [01:06<02:01, 1643.08 examples/s]Running tokenizer on dataset (num_proc=2):  34%|███▍      | 102000/300001 [01:06<02:02, 1615.16 examples/s]Running tokenizer on dataset (num_proc=2):  34%|███▍      | 103000/300001 [01:07<01:57, 1683.67 examples/s]Running tokenizer on dataset (num_proc=2):  35%|███▍      | 104000/300001 [01:08<01:59, 1633.88 examples/s]Running tokenizer on dataset (num_proc=2):  35%|███▍      | 105000/300001 [01:08<02:01, 1602.36 examples/s]Running tokenizer on dataset (num_proc=2):  35%|███▌      | 106000/300001 [01:09<02:08, 1509.02 examples/s]Running tokenizer on dataset (num_proc=2):  36%|███▌      | 107000/300001 [01:10<01:58, 1631.24 examples/s]Running tokenizer on dataset (num_proc=2):  36%|███▌      | 108000/300001 [01:10<02:00, 1591.07 examples/s]Running tokenizer on dataset (num_proc=2):  36%|███▋      | 109000/300001 [01:11<01:55, 1660.11 examples/s]Running tokenizer on dataset (num_proc=2):  37%|███▋      | 110000/300001 [01:11<02:00, 1578.35 examples/s]Running tokenizer on dataset (num_proc=2):  37%|███▋      | 111000/300001 [01:12<01:52, 1682.78 examples/s]Running tokenizer on dataset (num_proc=2):  37%|███▋      | 112000/300001 [01:13<01:56, 1611.03 examples/s]Running tokenizer on dataset (num_proc=2):  38%|███▊      | 113000/300001 [01:13<01:53, 1652.59 examples/s]Running tokenizer on dataset (num_proc=2):  38%|███▊      | 114000/300001 [01:14<01:54, 1625.58 examples/s]Running tokenizer on dataset (num_proc=2):  38%|███▊      | 115000/300001 [01:14<01:55, 1601.31 examples/s]Running tokenizer on dataset (num_proc=2):  39%|███▊      | 116000/300001 [01:15<01:51, 1643.90 examples/s]Running tokenizer on dataset (num_proc=2):  39%|███▉      | 117000/300001 [01:16<01:55, 1590.81 examples/s]Running tokenizer on dataset (num_proc=2):  39%|███▉      | 118000/300001 [01:16<01:50, 1651.52 examples/s]Running tokenizer on dataset (num_proc=2):  40%|███▉      | 119000/300001 [01:17<01:50, 1639.89 examples/s]Running tokenizer on dataset (num_proc=2):  40%|███▉      | 120000/300001 [01:17<01:47, 1668.60 examples/s]Running tokenizer on dataset (num_proc=2):  40%|████      | 121000/300001 [01:19<02:15, 1324.13 examples/s]Running tokenizer on dataset (num_proc=2):  41%|████      | 122000/300001 [01:19<02:04, 1425.84 examples/s]Running tokenizer on dataset (num_proc=2):  41%|████      | 123000/300001 [01:20<02:00, 1463.57 examples/s]Running tokenizer on dataset (num_proc=2):  41%|████▏     | 124000/300001 [01:20<01:54, 1531.32 examples/s]Running tokenizer on dataset (num_proc=2):  42%|████▏     | 125000/300001 [01:21<01:54, 1524.22 examples/s]Running tokenizer on dataset (num_proc=2):  42%|████▏     | 126000/300001 [01:22<01:49, 1588.46 examples/s]Running tokenizer on dataset (num_proc=2):  42%|████▏     | 127000/300001 [01:22<01:51, 1553.72 examples/s]Running tokenizer on dataset (num_proc=2):  43%|████▎     | 128000/300001 [01:23<01:45, 1626.94 examples/s]Running tokenizer on dataset (num_proc=2):  43%|████▎     | 129000/300001 [01:24<01:48, 1570.92 examples/s]Running tokenizer on dataset (num_proc=2):  43%|████▎     | 130000/300001 [01:24<01:42, 1665.12 examples/s]Running tokenizer on dataset (num_proc=2):  44%|████▎     | 131000/300001 [01:25<01:45, 1607.34 examples/s]Running tokenizer on dataset (num_proc=2):  44%|████▍     | 132000/300001 [01:25<01:40, 1670.18 examples/s]Running tokenizer on dataset (num_proc=2):  44%|████▍     | 133000/300001 [01:26<01:45, 1579.42 examples/s]Running tokenizer on dataset (num_proc=2):  45%|████▍     | 134000/300001 [01:26<01:38, 1690.13 examples/s]Running tokenizer on dataset (num_proc=2):  45%|████▍     | 135000/300001 [01:27<01:42, 1614.16 examples/s]Running tokenizer on dataset (num_proc=2):  45%|████▌     | 136000/300001 [01:28<01:35, 1710.59 examples/s]Running tokenizer on dataset (num_proc=2):  46%|████▌     | 137000/300001 [01:28<01:40, 1615.21 examples/s]Running tokenizer on dataset (num_proc=2):  46%|████▌     | 138000/300001 [01:29<01:37, 1660.72 examples/s]Running tokenizer on dataset (num_proc=2):  46%|████▋     | 139000/300001 [01:30<01:42, 1567.71 examples/s]Running tokenizer on dataset (num_proc=2):  47%|████▋     | 140000/300001 [01:30<01:37, 1647.50 examples/s]Running tokenizer on dataset (num_proc=2):  47%|████▋     | 141000/300001 [01:31<01:42, 1558.22 examples/s]Running tokenizer on dataset (num_proc=2):  47%|████▋     | 142000/300001 [01:31<01:34, 1669.27 examples/s]Running tokenizer on dataset (num_proc=2):  48%|████▊     | 143000/300001 [01:32<01:41, 1541.54 examples/s]Running tokenizer on dataset (num_proc=2):  48%|████▊     | 144000/300001 [01:33<01:33, 1668.87 examples/s]Running tokenizer on dataset (num_proc=2):  48%|████▊     | 145000/300001 [01:33<01:42, 1510.00 examples/s]Running tokenizer on dataset (num_proc=2):  49%|████▊     | 146000/300001 [01:34<01:29, 1723.71 examples/s]Running tokenizer on dataset (num_proc=2):  49%|████▉     | 147000/300001 [01:35<01:42, 1494.44 examples/s]Running tokenizer on dataset (num_proc=2):  49%|████▉     | 148000/300001 [01:35<01:25, 1776.52 examples/s]Running tokenizer on dataset (num_proc=2):  50%|████▉     | 149000/300001 [01:36<01:39, 1516.30 examples/s]Running tokenizer on dataset (num_proc=2):  50%|████▉     | 150000/300001 [01:36<01:22, 1819.88 examples/s]Running tokenizer on dataset (num_proc=2):  50%|█████     | 151000/300001 [01:37<01:43, 1435.54 examples/s]Running tokenizer on dataset (num_proc=2):  51%|█████     | 152000/300001 [01:37<01:20, 1830.29 examples/s]Running tokenizer on dataset (num_proc=2):  51%|█████     | 153000/300001 [01:38<01:40, 1456.04 examples/s]Running tokenizer on dataset (num_proc=2):  51%|█████▏    | 154000/300001 [01:39<01:19, 1834.29 examples/s]Running tokenizer on dataset (num_proc=2):  52%|█████▏    | 155000/300001 [01:40<01:41, 1422.24 examples/s]Running tokenizer on dataset (num_proc=2):  52%|█████▏    | 156000/300001 [01:40<01:16, 1885.54 examples/s]Running tokenizer on dataset (num_proc=2):  52%|█████▏    | 157000/300001 [01:41<01:39, 1442.81 examples/s]Running tokenizer on dataset (num_proc=2):  53%|█████▎    | 158000/300001 [01:41<01:16, 1863.75 examples/s]Running tokenizer on dataset (num_proc=2):  53%|█████▎    | 159000/300001 [01:42<01:40, 1399.23 examples/s]Running tokenizer on dataset (num_proc=2):  54%|█████▎    | 161000/300001 [01:44<01:34, 1468.18 examples/s]Running tokenizer on dataset (num_proc=2):  54%|█████▍    | 162000/300001 [01:44<01:14, 1849.51 examples/s]Running tokenizer on dataset (num_proc=2):  54%|█████▍    | 163000/300001 [01:45<01:34, 1455.94 examples/s]Running tokenizer on dataset (num_proc=2):  55%|█████▍    | 164000/300001 [01:45<01:12, 1886.08 examples/s]Running tokenizer on dataset (num_proc=2):  55%|█████▍    | 165000/300001 [01:46<01:34, 1433.56 examples/s]Running tokenizer on dataset (num_proc=2):  55%|█████▌    | 166000/300001 [01:46<01:11, 1864.90 examples/s]Running tokenizer on dataset (num_proc=2):  56%|█████▌    | 167000/300001 [01:48<01:52, 1177.13 examples/s]Running tokenizer on dataset (num_proc=2):  56%|█████▋    | 169000/300001 [01:49<01:39, 1319.53 examples/s]Running tokenizer on dataset (num_proc=2):  57%|█████▋    | 171000/300001 [01:50<01:30, 1428.13 examples/s]Running tokenizer on dataset (num_proc=2):  58%|█████▊    | 173000/300001 [01:51<01:24, 1497.88 examples/s]Running tokenizer on dataset (num_proc=2):  58%|█████▊    | 175000/300001 [01:53<01:21, 1539.27 examples/s]Running tokenizer on dataset (num_proc=2):  59%|█████▉    | 177000/300001 [01:54<01:18, 1559.57 examples/s]Running tokenizer on dataset (num_proc=2):  60%|█████▉    | 179000/300001 [01:55<01:16, 1583.63 examples/s]Running tokenizer on dataset (num_proc=2):  60%|██████    | 181000/300001 [01:56<01:15, 1576.94 examples/s]Running tokenizer on dataset (num_proc=2):  61%|██████    | 183000/300001 [01:58<01:13, 1596.01 examples/s]Running tokenizer on dataset (num_proc=2):  61%|██████▏   | 184000/300001 [01:58<01:04, 1793.07 examples/s]Running tokenizer on dataset (num_proc=2):  62%|██████▏   | 185000/300001 [01:59<01:13, 1562.67 examples/s]Running tokenizer on dataset (num_proc=2):  62%|██████▏   | 186000/300001 [01:59<01:08, 1661.61 examples/s]Running tokenizer on dataset (num_proc=2):  62%|██████▏   | 187000/300001 [02:00<01:11, 1576.16 examples/s]Running tokenizer on dataset (num_proc=2):  63%|██████▎   | 188000/300001 [02:01<01:13, 1525.58 examples/s]Running tokenizer on dataset (num_proc=2):  63%|██████▎   | 189000/300001 [02:01<01:06, 1675.71 examples/s]Running tokenizer on dataset (num_proc=2):  63%|██████▎   | 190000/300001 [02:02<01:18, 1404.80 examples/s]Running tokenizer on dataset (num_proc=2):  64%|██████▎   | 191000/300001 [02:02<01:02, 1757.32 examples/s]Running tokenizer on dataset (num_proc=2):  64%|██████▍   | 192000/300001 [02:04<01:21, 1321.59 examples/s]Running tokenizer on dataset (num_proc=2):  65%|██████▍   | 194000/300001 [02:05<01:15, 1397.95 examples/s]Running tokenizer on dataset (num_proc=2):  65%|██████▍   | 195000/300001 [02:05<01:02, 1670.75 examples/s]Running tokenizer on dataset (num_proc=2):  65%|██████▌   | 196000/300001 [02:06<01:12, 1439.70 examples/s]Running tokenizer on dataset (num_proc=2):  66%|██████▌   | 197000/300001 [02:07<01:08, 1511.33 examples/s]Running tokenizer on dataset (num_proc=2):  66%|██████▌   | 198000/300001 [02:07<01:07, 1501.37 examples/s]Running tokenizer on dataset (num_proc=2):  66%|██████▋   | 199000/300001 [02:08<01:11, 1416.66 examples/s]Running tokenizer on dataset (num_proc=2):  67%|██████▋   | 200000/300001 [02:09<01:02, 1599.55 examples/s]Running tokenizer on dataset (num_proc=2):  67%|██████▋   | 201000/300001 [02:10<01:13, 1340.06 examples/s]Running tokenizer on dataset (num_proc=2):  67%|██████▋   | 202000/300001 [02:10<00:58, 1676.33 examples/s]Running tokenizer on dataset (num_proc=2):  68%|██████▊   | 203000/300001 [02:11<01:16, 1268.62 examples/s]Running tokenizer on dataset (num_proc=2):  68%|██████▊   | 205000/300001 [02:12<01:08, 1385.49 examples/s]Running tokenizer on dataset (num_proc=2):  69%|██████▊   | 206000/300001 [02:13<00:55, 1690.72 examples/s]Running tokenizer on dataset (num_proc=2):  69%|██████▉   | 207000/300001 [02:14<01:05, 1421.97 examples/s]Running tokenizer on dataset (num_proc=2):  69%|██████▉   | 208000/300001 [02:14<01:00, 1529.99 examples/s]Running tokenizer on dataset (num_proc=2):  70%|██████▉   | 209000/300001 [02:15<01:06, 1370.90 examples/s]Running tokenizer on dataset (num_proc=2):  70%|██████▉   | 210000/300001 [02:16<01:01, 1459.88 examples/s]Running tokenizer on dataset (num_proc=2):  70%|███████   | 211000/300001 [02:17<01:12, 1222.24 examples/s]Running tokenizer on dataset (num_proc=2):  71%|███████   | 212000/300001 [02:17<01:00, 1464.05 examples/s]Running tokenizer on dataset (num_proc=2):  71%|███████   | 213000/300001 [02:18<01:03, 1365.40 examples/s]Running tokenizer on dataset (num_proc=2):  71%|███████▏  | 214000/300001 [02:19<00:59, 1436.23 examples/s]Running tokenizer on dataset (num_proc=2):  72%|███████▏  | 215000/300001 [02:19<00:57, 1487.22 examples/s]Running tokenizer on dataset (num_proc=2):  72%|███████▏  | 216000/300001 [02:20<01:09, 1204.95 examples/s]Running tokenizer on dataset (num_proc=2):  72%|███████▏  | 217000/300001 [02:21<01:03, 1298.95 examples/s]Running tokenizer on dataset (num_proc=2):  73%|███████▎  | 218000/300001 [02:22<00:59, 1383.55 examples/s]Running tokenizer on dataset (num_proc=2):  73%|███████▎  | 219000/300001 [02:23<01:02, 1302.93 examples/s]Running tokenizer on dataset (num_proc=2):  73%|███████▎  | 220000/300001 [02:23<00:56, 1422.88 examples/s]Running tokenizer on dataset (num_proc=2):  74%|███████▎  | 221000/300001 [02:24<01:00, 1298.53 examples/s]Running tokenizer on dataset (num_proc=2):  74%|███████▍  | 222000/300001 [02:24<00:49, 1565.18 examples/s]Running tokenizer on dataset (num_proc=2):  74%|███████▍  | 223000/300001 [02:25<00:58, 1308.27 examples/s]Running tokenizer on dataset (num_proc=2):  75%|███████▍  | 224000/300001 [02:26<00:44, 1710.74 examples/s]Running tokenizer on dataset (num_proc=2):  75%|███████▍  | 225000/300001 [02:27<00:59, 1264.69 examples/s]Running tokenizer on dataset (num_proc=2):  76%|███████▌  | 227000/300001 [02:28<00:51, 1421.81 examples/s]Running tokenizer on dataset (num_proc=2):  76%|███████▌  | 228000/300001 [02:28<00:43, 1656.41 examples/s]Running tokenizer on dataset (num_proc=2):  76%|███████▋  | 229000/300001 [02:29<00:48, 1461.69 examples/s]Running tokenizer on dataset (num_proc=2):  77%|███████▋  | 230000/300001 [02:30<00:45, 1524.32 examples/s]Running tokenizer on dataset (num_proc=2):  77%|███████▋  | 231000/300001 [02:30<00:43, 1589.23 examples/s]Running tokenizer on dataset (num_proc=2):  77%|███████▋  | 232000/300001 [02:31<00:48, 1396.24 examples/s]Running tokenizer on dataset (num_proc=2):  78%|███████▊  | 233000/300001 [02:32<00:40, 1672.54 examples/s]Running tokenizer on dataset (num_proc=2):  78%|███████▊  | 234000/300001 [02:33<00:50, 1296.53 examples/s]Running tokenizer on dataset (num_proc=2):  79%|███████▊  | 236000/300001 [02:34<00:45, 1409.60 examples/s]Running tokenizer on dataset (num_proc=2):  79%|███████▉  | 237000/300001 [02:34<00:35, 1757.96 examples/s]Running tokenizer on dataset (num_proc=2):  79%|███████▉  | 238000/300001 [02:35<00:44, 1387.76 examples/s]Running tokenizer on dataset (num_proc=2):  80%|███████▉  | 239000/300001 [02:36<00:37, 1644.96 examples/s]Running tokenizer on dataset (num_proc=2):  80%|███████▉  | 240000/300001 [02:37<00:42, 1416.58 examples/s]Running tokenizer on dataset (num_proc=2):  80%|████████  | 241000/300001 [02:37<00:37, 1560.39 examples/s]Running tokenizer on dataset (num_proc=2):  81%|████████  | 242000/300001 [02:38<00:38, 1498.59 examples/s]Running tokenizer on dataset (num_proc=2):  81%|████████  | 243000/300001 [02:39<00:39, 1431.56 examples/s]Running tokenizer on dataset (num_proc=2):  81%|████████▏ | 244000/300001 [02:39<00:36, 1532.42 examples/s]Running tokenizer on dataset (num_proc=2):  82%|████████▏ | 245000/300001 [02:40<00:40, 1350.91 examples/s]Running tokenizer on dataset (num_proc=2):  82%|████████▏ | 246000/300001 [02:41<00:35, 1531.56 examples/s]Running tokenizer on dataset (num_proc=2):  82%|████████▏ | 247000/300001 [02:42<00:39, 1353.39 examples/s]Running tokenizer on dataset (num_proc=2):  83%|████████▎ | 248000/300001 [02:42<00:31, 1669.14 examples/s]Running tokenizer on dataset (num_proc=2):  83%|████████▎ | 249000/300001 [02:43<00:40, 1272.66 examples/s]Running tokenizer on dataset (num_proc=2):  84%|████████▎ | 251000/300001 [02:44<00:34, 1423.92 examples/s]Running tokenizer on dataset (num_proc=2):  84%|████████▍ | 252000/300001 [02:45<00:27, 1717.48 examples/s]Running tokenizer on dataset (num_proc=2):  84%|████████▍ | 253000/300001 [02:46<00:38, 1218.74 examples/s]Running tokenizer on dataset (num_proc=2):  85%|████████▍ | 255000/300001 [02:47<00:33, 1358.99 examples/s]Running tokenizer on dataset (num_proc=2):  85%|████████▌ | 256000/300001 [02:48<00:27, 1573.60 examples/s]Running tokenizer on dataset (num_proc=2):  86%|████████▌ | 257000/300001 [02:48<00:30, 1424.05 examples/s]Running tokenizer on dataset (num_proc=2):  86%|████████▌ | 258000/300001 [02:49<00:28, 1492.48 examples/s]Running tokenizer on dataset (num_proc=2):  86%|████████▋ | 259000/300001 [02:50<00:27, 1502.73 examples/s]Running tokenizer on dataset (num_proc=2):  87%|████████▋ | 260000/300001 [02:51<00:30, 1324.81 examples/s]Running tokenizer on dataset (num_proc=2):  87%|████████▋ | 261000/300001 [02:51<00:24, 1598.01 examples/s]Running tokenizer on dataset (num_proc=2):  87%|████████▋ | 262000/300001 [02:52<00:29, 1280.93 examples/s]Running tokenizer on dataset (num_proc=2):  88%|████████▊ | 263000/300001 [02:52<00:21, 1709.70 examples/s]Running tokenizer on dataset (num_proc=2):  88%|████████▊ | 264000/300001 [02:54<00:28, 1280.49 examples/s]Running tokenizer on dataset (num_proc=2):  89%|████████▊ | 266000/300001 [02:55<00:23, 1452.18 examples/s]Running tokenizer on dataset (num_proc=2):  89%|████████▉ | 267000/300001 [02:56<00:27, 1214.37 examples/s]Running tokenizer on dataset (num_proc=2):  90%|████████▉ | 269000/300001 [02:57<00:22, 1356.91 examples/s]Running tokenizer on dataset (num_proc=2):  90%|████████▉ | 270000/300001 [02:57<00:19, 1564.57 examples/s]Running tokenizer on dataset (num_proc=2):  90%|█████████ | 271000/300001 [02:59<00:21, 1326.90 examples/s]Running tokenizer on dataset (num_proc=2):  91%|█████████ | 272000/300001 [02:59<00:17, 1555.65 examples/s]Running tokenizer on dataset (num_proc=2):  91%|█████████ | 273000/300001 [03:00<00:19, 1373.68 examples/s]Running tokenizer on dataset (num_proc=2):  91%|█████████▏| 274000/300001 [03:00<00:17, 1468.12 examples/s]Running tokenizer on dataset (num_proc=2):  92%|█████████▏| 275000/300001 [03:01<00:17, 1461.43 examples/s]Running tokenizer on dataset (num_proc=2):  92%|█████████▏| 276000/300001 [03:02<00:17, 1399.93 examples/s]Running tokenizer on dataset (num_proc=2):  92%|█████████▏| 277000/300001 [03:02<00:15, 1507.25 examples/s]Running tokenizer on dataset (num_proc=2):  93%|█████████▎| 278000/300001 [03:03<00:16, 1365.73 examples/s]Running tokenizer on dataset (num_proc=2):  93%|█████████▎| 279000/300001 [03:04<00:12, 1641.97 examples/s]Running tokenizer on dataset (num_proc=2):  93%|█████████▎| 280000/300001 [03:05<00:15, 1300.80 examples/s]Running tokenizer on dataset (num_proc=2):  94%|█████████▍| 282000/300001 [03:06<00:12, 1395.11 examples/s]Running tokenizer on dataset (num_proc=2):  94%|█████████▍| 283000/300001 [03:06<00:09, 1732.03 examples/s]Running tokenizer on dataset (num_proc=2):  95%|█████████▍| 284000/300001 [03:07<00:11, 1439.97 examples/s]Running tokenizer on dataset (num_proc=2):  95%|█████████▍| 285000/300001 [03:08<00:09, 1607.49 examples/s]Running tokenizer on dataset (num_proc=2):  95%|█████████▌| 286000/300001 [03:09<00:09, 1437.76 examples/s]Running tokenizer on dataset (num_proc=2):  96%|█████████▌| 287000/300001 [03:09<00:08, 1578.43 examples/s]Running tokenizer on dataset (num_proc=2):  96%|█████████▌| 288000/300001 [03:10<00:08, 1491.93 examples/s]Running tokenizer on dataset (num_proc=2):  96%|█████████▋| 289000/300001 [03:11<00:07, 1453.56 examples/s]Running tokenizer on dataset (num_proc=2):  97%|█████████▋| 290000/300001 [03:11<00:06, 1638.54 examples/s]Running tokenizer on dataset (num_proc=2):  97%|█████████▋| 291000/300001 [03:12<00:06, 1342.09 examples/s]Running tokenizer on dataset (num_proc=2):  97%|█████████▋| 292000/300001 [03:13<00:07, 1044.39 examples/s]Running tokenizer on dataset (num_proc=2):  98%|█████████▊| 293000/300001 [03:15<00:07, 891.77 examples/s] Running tokenizer on dataset (num_proc=2):  98%|█████████▊| 294000/300001 [03:16<00:07, 811.67 examples/s]Running tokenizer on dataset (num_proc=2):  98%|█████████▊| 295000/300001 [03:18<00:06, 756.82 examples/s]Running tokenizer on dataset (num_proc=2):  99%|█████████▊| 296000/300001 [03:20<00:05, 726.42 examples/s]Running tokenizer on dataset (num_proc=2):  99%|█████████▉| 297000/300001 [03:21<00:04, 715.87 examples/s]Running tokenizer on dataset (num_proc=2):  99%|█████████▉| 298000/300001 [03:23<00:02, 691.49 examples/s]Running tokenizer on dataset (num_proc=2): 100%|█████████▉| 299000/300001 [03:24<00:01, 692.63 examples/s]Running tokenizer on dataset (num_proc=2): 100%|█████████▉| 300000/300001 [03:25<00:00, 684.18 examples/s]Running tokenizer on dataset (num_proc=2): 100%|██████████| 300001/300001 [03:26<00:00, 1454.56 examples/s]
192.168.0.89: training example:
192.168.0.89: input_ids:
192.168.0.89: [791, 42500, 13935, 8668, 315, 264, 13128, 374, 25, 386, 650, 735, 473, 735, 350, 358, 432, 452, 435, 362, 362, 328, 432, 445, 358, 362, 362, 350, 350, 435, 445, 328, 362, 650, 816, 445, 393, 362, 362, 435, 362, 1229, 350, 358, 650, 452, 452, 358, 816, 328, 469, 735, 480, 445, 362, 480, 350, 386, 328, 1229, 350, 1229, 445, 423, 350, 452, 350, 816, 350, 328, 1229, 362, 432, 435, 480, 468, 452, 452, 432, 432, 358, 650, 358, 423, 469, 735, 445, 735, 445, 423, 423, 735, 480, 432, 358, 445, 480, 435, 469, 362, 1229, 480, 1229, 328, 362, 435, 480, 362, 423, 358, 328, 469, 328, 816, 386, 468, 469, 452, 480, 1229, 362, 735, 468, 1229, 452, 423, 423, 469, 473, 328, 328, 650, 452, 350, 452, 1229, 393, 350, 435, 816, 650, 393, 735, 452, 328, 350, 445, 362, 358, 423, 452, 362, 445, 358, 432, 816, 445, 358, 735, 452, 423, 445, 469, 1229, 358, 735, 650, 445, 393, 473, 480, 735, 650, 350, 435, 350, 735, 445, 423, 350, 650, 735, 445, 480, 423, 469, 1229, 650, 816, 445, 816, 362, 735, 328, 480, 445, 350, 386, 350, 393, 423, 435, 362, 468, 816, 423, 735, 469, 480, 452, 816, 435, 362, 1229, 328, 468, 480, 480, 386, 816, 650, 358, 432, 423, 480, 435, 328, 735, 469, 1229, 435, 423, 1229, 445, 735, 650, 432, 1229, 445, 735, 362, 469, 1229, 362, 816, 445, 469, 452, 445, 362, 469, 1229, 445, 350, 469, 816, 473, 328, 362, 445, 445, 445, 423, 1229, 650, 452, 650, 435, 423, 650, 469, 735, 480, 350, 350, 445, 350, 480, 1229, 1229, 650, 445, 358, 469, 1229, 480, 735, 358, 650, 1229, 650, 362, 393, 735, 358, 469, 445, 480, 735, 735, 650, 362, 350, 650, 452, 480, 1229, 480, 735, 350, 445, 358, 393, 480, 445, 468, 423, 386, 473, 480, 473, 445, 328, 735, 423, 350, 480, 358, 445, 452, 358, 362, 350, 480, 650, 350, 328, 650, 432, 423, 386, 480, 452, 469, 473, 1229, 452, 445, 386, 469, 358, 1229, 362, 445, 435, 423, 350, 480, 735, 650, 445, 480, 350, 432, 650, 816, 432, 362, 480, 435, 386, 423, 735, 816, 328, 469, 452, 328, 362, 480, 445, 328, 650, 735, 350, 445, 469, 469, 362, 445, 469, 386, 650, 423, 435, 435, 362, 423, 452, 480, 816, 650, 1229, 358, 735, 445, 816, 328, 328, 358, 423, 393, 735, 468, 650, 469, 393, 358, 362, 1229, 432, 362, 473, 328, 432, 480, 445, 432, 445, 328, 480, 473, 358, 393, 362, 435, 386, 350, 362, 469, 1229, 362, 650, 473, 362, 480, 816, 452, 469, 358, 1229, 473, 358, 452, 386, 445, 435, 445, 452, 435, 445, 362, 480, 469, 469, 650, 423, 350, 432, 350, 735, 1229, 432, 435, 328, 445, 358, 480, 469, 735, 362, 362, 469, 386, 393, 386, 350, 480, 735, 469, 386, 423, 362, 435, 358, 735, 445, 445, 362, 452, 452, 452, 650, 650, 358, 423, 393, 350, 650, 328, 350, 435, 432, 328, 445, 445, 386, 328, 1229, 452, 735, 1229, 650, 423, 1229, 469, 435, 362, 469, 358, 362, 328, 473, 445, 393, 393, 452, 435, 650, 432, 452, 445, 735, 480, 362, 1229, 386, 1229, 650, 469, 469, 469, 473, 1229, 328, 362, 816, 1229, 452, 328, 480, 423, 362, 445, 445, 735, 386, 650, 735, 358, 445, 816, 423, 362, 480, 650, 393, 386, 650, 362, 480, 350, 423, 328, 650, 393, 480, 435, 350, 445, 445, 432, 469, 445, 469, 445, 816, 350, 386, 362, 480, 358, 393, 350, 350, 469, 650, 445, 735, 386, 362, 350, 358, 423, 328, 362, 432, 445, 386, 480, 650, 362, 473, 1229, 350, 480, 328, 358, 328, 469, 480, 735, 650, 362, 423, 445, 358, 445, 650, 423, 480, 423, 393, 328, 735, 423, 358, 735, 362, 445, 432, 735, 445, 328, 445, 650, 358, 735, 480, 350, 1229, 650, 435, 735, 393, 469, 362, 358, 435, 469, 1229, 358, 480, 358, 362, 393, 435, 350, 735, 362, 328, 432, 358, 650, 445, 1246, 2116, 420, 8668, 11, 279, 6070, 315, 279, 13128, 374, 25, 128750, 128357, 128268, 128672, 128657, 128516, 128335, 128388, 128296, 128335, 128407, 128516, 128497, 128334, 128350, 128334, 128407, 128649, 128661, 128731, 128302, 128335, 128535, 128585, 128302, 128727, 128648, 128756, 128651, 128725, 128270, 128732, 128547, 128343, 128378, 128585, 128513, 128476, 128315, 128487, 128330, 128566, 128510, 128417, 128417, 128520, 128732, 128321, 128433, 128435, 128419, 128386, 128603, 128516, 128366, 128417, 128346, 128441, 128549, 128493, 128344, 128493, 128315, 128581, 128288, 128651, 128343, 128432, 128487, 128746, 128585, 128369, 128651, 128504, 128651, 128592, 128272, 128625, 128531, 128306, 128730, 128366, 128420, 128526, 128446, 128593, 128517, 128420, 128520, 128443, 128334, 128518, 128581, 128347, 128514, 128739, 128593, 128726, 128663, 128586, 128625, 128328, 128695, 128571, 128263, 128739, 128572, 128301, 128366, 128371, 128483, 128432, 128429, 128268, 128671, 128502, 128562, 128330, 128602, 128431, 128430, 128326, 128622, 128713, 128678, 128352, 128742, 128478, 128340, 128584, 128675, 128685, 128476, 128297, 128368, 128588, 128723, 128336, 128272, 128727, 128592, 128258, 128408, 128396, 128369, 128268, 128321, 128705, 128424, 128268, 128321, 128423, 128547, 128624, 128308, 128417, 128261, 128520, 128343, 128543, 128580, 128656, 128411, 128368, 128486, 128334, 128257, 128343, 128569, 128576, 128649, 128303, 128514, 128305, 128413, 128476, 128343, 128354, 128475, 128547, 128512, 128547, 128648, 128493, 128462, 128483, 128256, 128585, 128294, 128398, 128575, 128365, 128331, 128623, 128649, 128593, 128283, 128349, 128281, 128575, 128289, 128582, 128487, 128493, 128524, 128298, 128756, 128706, 128349, 128732, 128649, 128330, 128458, 128305, 128516, 128288, 128514, 128727, 128403, 128702, 128487, 128268, 128353, 128487, 128265, 128286, 128333, 128298, 128366, 128330, 128587, 128298, 128268, 128353, 128518, 128578, 128310, 128620, 128547, 128435, 128592, 128514, 128614, 128432, 128696, 128648, 128365, 128462, 128448, 128360, 128315, 128346, 128306, 128629, 128577, 128305, 128340, 128404, 128462, 128690, 128593, 128593, 128517, 128497, 128285, 128438, 128384, 128310, 128331, 128677, 128492, 128393, 128759, 128305, 128346, 128270, 128667, 128539, 128592, 128364, 128441, 128392, 128446, 128267, 128610, 128603, 128274, 128472, 128694, 128524, 128590, 128469, 128381, 128718, 128525, 128401, 128678, 128635, 128358, 128323, 128483, 128297, 128575, 128444, 128471, 128387, 128483, 128746, 128317, 128487, 128290, 128366, 128629, 128381, 128443, 128547, 128435, 128720, 128547, 128466, 128484, 128693, 128524, 128765, 128271, 128446, 128486]
192.168.0.89: inputs:
192.168.0.89: The amino acid sequence of a protein is: M V K H K T I R N F A A S R L I A A T T F L S A V Y L P A A F A Q T I V N N I Y S E K G L A G T M S Q T Q L D T N T Y T S Q A R F G W N N R R I V I D E K L K L D D K G R I L G F E A Q G Q S A F G A D I S E S Y M W E N G Q A K W Q N D D E H S S V N T N Q P T F Y V P K N S T L A I D N A L I R Y L I K N D L E Q I K V L P H G K V T F T K L D T V K L G D E Q V Y L Y A K S G L T M T P D F A W Y D K E G N Y F A Q S W G G M Y V I R D G F S K E Q F D Q L K V R Q L K A E Q A Y L E N L A E Q L T E Y H S A L L L D Q V N V F D V E K G T T L T G Q Q V L I E Q G K I V Q V A P K I E L G K K V A T V N G Q G K T L I P G L W D M H G H L S K D T G I L N I A T G V T S V R D M G N E H Q N L M E I Q A L F D T G K V L G T R V Y R A G F M D K Y S E N S A G L S V K T L E E A L E M V D F F A D N G Y V Q I K L Y S S I D P K W V E P I A Q R A H S R G L R L S G H I P A F M T A E Q A V H A G Y N E I Q H I N M L F L N F L A G E E V D T R T K Q R F S L I G E K A A E M P M T G K E M D A F I K L L A N N N V V I D P T V S T F R S L L M S Q N K Q V D Q E F A E I A S H L P P N F V R N L K G A Q M Q V E E E H Q S A Y Q N S G D A L L K M V K I L Y D A G V P M V A G T D S V P G F T L L R E L E L Y T M A G I P T T E V L K M A T I D S A R L M G V A H Q T G S I S E G K V A D L I L V D G D P S K D I K A L R K L S L V I K G T Q V F K P E A I F E Q I G I A P F T K A S R I V L.Given this sequence, the structure of the protein is:珖屼袱哻豊筇臧悕傱臧穉筇爲陼玭陼穉栲蒀篨岜臧潋愲岜薠玝棷伌芎瑷阌篣茖諒愲潼矻阇槠恮瘈埘粺粺蕲阌柎稽斾苁魼訞筇咍粺嫺獐昵咥嗋咥阇睔罯伌茖醜槠蔴愲軵伌栶伌鞻稰呪墌謜厓咍鲕滓碟澶鳊鲕蕲鹣陼鵙睔娓濩鹽澶詐匜簰呪鳗韆扞鸓鹽娙濊咍阎馑醜墡袱韪儶焄恮檈鲵犫羝瑰骬饹曋厖躈睭蟺譺鵸矻臡躄麃醿圉稰薠鞻僭慞仂軵袱柎泿殛袱柎璹篣褴岫粺菙蕲茖繖綵愀擥躄穠陼伣茖酎革栲庈濩韣翕矻茖戗虩篣炤篣玝咥辋馑緭愲铏狾裲難寤雊栲澶惔撙瀷裲蟪疿槠咥闚牦棷絡撙阌栲恮漰韣筇罯濩薠篥秺槠袱鎉槠褓銲鬋牦咍恮饠牦袱鎉鵙冔魳鉾篣斾鞻濩姏醜樐玝難辋酬筌阇嫺謜雿磹韣睭遒辋箧澶澶鳊爲趯橅萹魳寤傿劙璲驞韣嫺瑷镏耒鞻镕獐磼碟蝯羆訞枌褕刼闚簪肜岭荓雟敍饹韎聱匫馑臡裲蛖茤稒馑蔴貣槠缌咍雿岭鹣篣斾龃篣籝檗扢闚錜忲碟穠
192.168.0.25: Running tokenizer on dataset (num_proc=2):   0%|          | 0/300001 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=2):   0%|          | 1000/300001 [00:02<11:47, 422.57 examples/s]Running tokenizer on dataset (num_proc=2):   1%|          | 2000/300001 [00:02<05:53, 843.87 examples/s]Running tokenizer on dataset (num_proc=2):   1%|          | 3000/300001 [00:03<05:50, 848.11 examples/s]Running tokenizer on dataset (num_proc=2):   2%|▏         | 5000/300001 [00:05<04:22, 1125.25 examples/s]Running tokenizer on dataset (num_proc=2):   2%|▏         | 6000/300001 [00:05<03:16, 1496.45 examples/s]Running tokenizer on dataset (num_proc=2):   2%|▏         | 7000/300001 [00:06<04:02, 1209.78 examples/s]Running tokenizer on dataset (num_proc=2):   3%|▎         | 9000/300001 [00:07<03:36, 1341.14 examples/s]Running tokenizer on dataset (num_proc=2):   4%|▎         | 11000/300001 [00:09<03:22, 1426.97 examples/s]Running tokenizer on dataset (num_proc=2):   4%|▍         | 12000/300001 [00:09<02:44, 1755.92 examples/s]Running tokenizer on dataset (num_proc=2):   4%|▍         | 13000/300001 [00:10<03:22, 1418.23 examples/s]Running tokenizer on dataset (num_proc=2):   5%|▍         | 14000/300001 [00:10<02:45, 1730.30 examples/s]Running tokenizer on dataset (num_proc=2):   5%|▍         | 15000/300001 [00:11<03:18, 1438.86 examples/s]Running tokenizer on dataset (num_proc=2):   5%|▌         | 16000/300001 [00:11<02:40, 1771.94 examples/s]Running tokenizer on dataset (num_proc=2):   6%|▌         | 17000/300001 [00:12<03:18, 1425.73 examples/s]Running tokenizer on dataset (num_proc=2):   6%|▌         | 18000/300001 [00:13<02:35, 1811.04 examples/s]Running tokenizer on dataset (num_proc=2):   6%|▋         | 19000/300001 [00:14<03:16, 1430.75 examples/s]Running tokenizer on dataset (num_proc=2):   7%|▋         | 20000/300001 [00:14<02:45, 1692.96 examples/s]Running tokenizer on dataset (num_proc=2):   7%|▋         | 21000/300001 [00:15<03:14, 1432.29 examples/s]Running tokenizer on dataset (num_proc=2):   7%|▋         | 22000/300001 [00:15<02:46, 1666.42 examples/s]Running tokenizer on dataset (num_proc=2):   8%|▊         | 23000/300001 [00:16<03:11, 1444.10 examples/s]Running tokenizer on dataset (num_proc=2):   8%|▊         | 24000/300001 [00:16<02:40, 1715.09 examples/s]Running tokenizer on dataset (num_proc=2):   8%|▊         | 25000/300001 [00:17<03:07, 1464.03 examples/s]Running tokenizer on dataset (num_proc=2):   9%|▊         | 26000/300001 [00:18<02:39, 1712.95 examples/s]Running tokenizer on dataset (num_proc=2):   9%|▉         | 27000/300001 [00:19<03:12, 1419.69 examples/s]Running tokenizer on dataset (num_proc=2):   9%|▉         | 28000/300001 [00:19<02:39, 1707.56 examples/s]Running tokenizer on dataset (num_proc=2):  10%|▉         | 29000/300001 [00:20<03:09, 1433.26 examples/s]Running tokenizer on dataset (num_proc=2):  10%|▉         | 30000/300001 [00:20<02:39, 1694.77 examples/s]Running tokenizer on dataset (num_proc=2):  10%|█         | 31000/300001 [00:21<03:06, 1439.01 examples/s]Running tokenizer on dataset (num_proc=2):  11%|█         | 32000/300001 [00:22<02:47, 1601.95 examples/s]Running tokenizer on dataset (num_proc=2):  11%|█         | 33000/300001 [00:23<03:04, 1445.99 examples/s]Running tokenizer on dataset (num_proc=2):  11%|█▏        | 34000/300001 [00:23<02:47, 1590.74 examples/s]Running tokenizer on dataset (num_proc=2):  12%|█▏        | 35000/300001 [00:24<02:59, 1473.12 examples/s]Running tokenizer on dataset (num_proc=2):  12%|█▏        | 36000/300001 [00:24<02:39, 1653.35 examples/s]Running tokenizer on dataset (num_proc=2):  12%|█▏        | 37000/300001 [00:25<02:52, 1528.72 examples/s]Running tokenizer on dataset (num_proc=2):  13%|█▎        | 38000/300001 [00:26<02:43, 1599.95 examples/s]Running tokenizer on dataset (num_proc=2):  13%|█▎        | 39000/300001 [00:26<02:49, 1538.37 examples/s]Running tokenizer on dataset (num_proc=2):  13%|█▎        | 40000/300001 [00:27<02:35, 1667.51 examples/s]Running tokenizer on dataset (num_proc=2):  14%|█▎        | 41000/300001 [00:28<02:52, 1503.98 examples/s]Running tokenizer on dataset (num_proc=2):  14%|█▍        | 42000/300001 [00:28<02:40, 1607.02 examples/s]Running tokenizer on dataset (num_proc=2):  14%|█▍        | 43000/300001 [00:29<02:46, 1541.35 examples/s]Running tokenizer on dataset (num_proc=2):  15%|█▍        | 44000/300001 [00:29<02:34, 1653.20 examples/s]Running tokenizer on dataset (num_proc=2):  15%|█▍        | 45000/300001 [00:30<02:44, 1547.67 examples/s]Running tokenizer on dataset (num_proc=2):  15%|█▌        | 46000/300001 [00:31<02:30, 1690.96 examples/s]Running tokenizer on dataset (num_proc=2):  16%|█▌        | 47000/300001 [00:31<02:44, 1542.67 examples/s]Running tokenizer on dataset (num_proc=2):  16%|█▌        | 48000/300001 [00:32<02:31, 1666.41 examples/s]Running tokenizer on dataset (num_proc=2):  16%|█▋        | 49000/300001 [00:33<02:40, 1561.42 examples/s]Running tokenizer on dataset (num_proc=2):  17%|█▋        | 50000/300001 [00:33<02:29, 1677.43 examples/s]Running tokenizer on dataset (num_proc=2):  17%|█▋        | 51000/300001 [00:34<02:47, 1490.26 examples/s]Running tokenizer on dataset (num_proc=2):  17%|█▋        | 52000/300001 [00:34<02:25, 1703.12 examples/s]Running tokenizer on dataset (num_proc=2):  18%|█▊        | 53000/300001 [00:35<02:48, 1467.81 examples/s]Running tokenizer on dataset (num_proc=2):  18%|█▊        | 54000/300001 [00:36<02:22, 1720.91 examples/s]Running tokenizer on dataset (num_proc=2):  18%|█▊        | 55000/300001 [00:37<03:38, 1122.50 examples/s]Running tokenizer on dataset (num_proc=2):  19%|█▊        | 56000/300001 [00:38<03:01, 1342.49 examples/s]Running tokenizer on dataset (num_proc=2):  19%|█▉        | 57000/300001 [00:38<03:07, 1299.41 examples/s]Running tokenizer on dataset (num_proc=2):  19%|█▉        | 58000/300001 [00:39<02:40, 1505.75 examples/s]Running tokenizer on dataset (num_proc=2):  20%|█▉        | 59000/300001 [00:40<03:11, 1255.82 examples/s]Running tokenizer on dataset (num_proc=2):  20%|█▉        | 60000/300001 [00:40<02:47, 1436.45 examples/s]Running tokenizer on dataset (num_proc=2):  20%|██        | 61000/300001 [00:41<03:15, 1221.87 examples/s]Running tokenizer on dataset (num_proc=2):  21%|██        | 62000/300001 [00:42<02:54, 1365.84 examples/s]Running tokenizer on dataset (num_proc=2):  21%|██        | 63000/300001 [00:43<03:14, 1219.97 examples/s]Running tokenizer on dataset (num_proc=2):  21%|██▏       | 64000/300001 [00:44<03:06, 1262.92 examples/s]Running tokenizer on dataset (num_proc=2):  22%|██▏       | 65000/300001 [00:44<02:59, 1306.68 examples/s]Running tokenizer on dataset (num_proc=2):  22%|██▏       | 66000/300001 [00:45<03:12, 1213.07 examples/s]Running tokenizer on dataset (num_proc=2):  22%|██▏       | 67000/300001 [00:46<02:34, 1509.14 examples/s]Running tokenizer on dataset (num_proc=2):  23%|██▎       | 68000/300001 [00:47<03:14, 1195.69 examples/s]Running tokenizer on dataset (num_proc=2):  23%|██▎       | 70000/300001 [00:48<02:59, 1280.07 examples/s]Running tokenizer on dataset (num_proc=2):  24%|██▎       | 71000/300001 [00:49<02:21, 1614.66 examples/s]Running tokenizer on dataset (num_proc=2):  24%|██▍       | 72000/300001 [00:50<03:07, 1218.48 examples/s]Running tokenizer on dataset (num_proc=2):  25%|██▍       | 74000/300001 [00:51<02:53, 1299.23 examples/s]Running tokenizer on dataset (num_proc=2):  25%|██▍       | 75000/300001 [00:52<02:27, 1525.27 examples/s]Running tokenizer on dataset (num_proc=2):  25%|██▌       | 76000/300001 [00:53<03:00, 1239.02 examples/s]Running tokenizer on dataset (num_proc=2):  26%|██▌       | 77000/300001 [00:53<02:28, 1500.53 examples/s]Running tokenizer on dataset (num_proc=2):  26%|██▌       | 78000/300001 [00:54<02:58, 1241.04 examples/s]Running tokenizer on dataset (num_proc=2):  26%|██▋       | 79000/300001 [00:55<02:29, 1478.65 examples/s]Running tokenizer on dataset (num_proc=2):  27%|██▋       | 80000/300001 [00:56<03:06, 1182.51 examples/s]Running tokenizer on dataset (num_proc=2):  27%|██▋       | 82000/300001 [00:57<02:56, 1234.87 examples/s]Running tokenizer on dataset (num_proc=2):  28%|██▊       | 84000/300001 [00:59<02:49, 1275.23 examples/s]Running tokenizer on dataset (num_proc=2):  28%|██▊       | 85000/300001 [00:59<02:17, 1567.15 examples/s]Running tokenizer on dataset (num_proc=2):  29%|██▊       | 86000/300001 [01:00<02:54, 1225.90 examples/s]Running tokenizer on dataset (num_proc=2):  29%|██▉       | 88000/300001 [01:02<02:46, 1276.85 examples/s]Running tokenizer on dataset (num_proc=2):  30%|██▉       | 90000/300001 [01:03<02:37, 1335.36 examples/s]Running tokenizer on dataset (num_proc=2):  30%|███       | 91000/300001 [01:04<02:15, 1538.84 examples/s]Running tokenizer on dataset (num_proc=2):  31%|███       | 92000/300001 [01:05<02:30, 1378.06 examples/s]Running tokenizer on dataset (num_proc=2):  31%|███       | 93000/300001 [01:05<02:16, 1514.82 examples/s]Running tokenizer on dataset (num_proc=2):  31%|███▏      | 94000/300001 [01:06<02:26, 1409.19 examples/s]Running tokenizer on dataset (num_proc=2):  32%|███▏      | 95000/300001 [01:06<02:17, 1493.30 examples/s]Running tokenizer on dataset (num_proc=2):  32%|███▏      | 96000/300001 [01:07<02:16, 1489.91 examples/s]Running tokenizer on dataset (num_proc=2):  32%|███▏      | 97000/300001 [01:08<02:11, 1538.06 examples/s]Running tokenizer on dataset (num_proc=2):  33%|███▎      | 98000/300001 [01:08<02:12, 1520.97 examples/s]Running tokenizer on dataset (num_proc=2):  33%|███▎      | 99000/300001 [01:09<02:13, 1500.06 examples/s]Running tokenizer on dataset (num_proc=2):  33%|███▎      | 100000/300001 [01:10<02:07, 1572.92 examples/s]Running tokenizer on dataset (num_proc=2):  34%|███▎      | 101000/300001 [01:10<02:09, 1537.12 examples/s]Running tokenizer on dataset (num_proc=2):  34%|███▍      | 102000/300001 [01:11<02:02, 1612.93 examples/s]Running tokenizer on dataset (num_proc=2):  34%|███▍      | 103000/300001 [01:12<02:07, 1547.70 examples/s]Running tokenizer on dataset (num_proc=2):  35%|███▍      | 104000/300001 [01:12<02:02, 1593.52 examples/s]Running tokenizer on dataset (num_proc=2):  35%|███▍      | 105000/300001 [01:13<02:01, 1600.53 examples/s]Running tokenizer on dataset (num_proc=2):  35%|███▌      | 106000/300001 [01:13<01:58, 1640.71 examples/s]Running tokenizer on dataset (num_proc=2):  36%|███▌      | 107000/300001 [01:14<02:01, 1591.79 examples/s]Running tokenizer on dataset (num_proc=2):  36%|███▌      | 108000/300001 [01:15<01:59, 1606.22 examples/s]Running tokenizer on dataset (num_proc=2):  36%|███▋      | 109000/300001 [01:15<02:01, 1573.35 examples/s]Running tokenizer on dataset (num_proc=2):  37%|███▋      | 110000/300001 [01:16<01:56, 1635.99 examples/s]Running tokenizer on dataset (num_proc=2):  37%|███▋      | 111000/300001 [01:17<01:59, 1579.39 examples/s]Running tokenizer on dataset (num_proc=2):  37%|███▋      | 112000/300001 [01:17<01:53, 1660.47 examples/s]Running tokenizer on dataset (num_proc=2):  38%|███▊      | 113000/300001 [01:18<01:57, 1590.93 examples/s]Running tokenizer on dataset (num_proc=2):  38%|███▊      | 114000/300001 [01:18<01:55, 1614.93 examples/s]Running tokenizer on dataset (num_proc=2):  38%|███▊      | 115000/300001 [01:19<01:57, 1575.05 examples/s]Running tokenizer on dataset (num_proc=2):  39%|███▊      | 116000/300001 [01:20<01:55, 1599.88 examples/s]Running tokenizer on dataset (num_proc=2):  39%|███▉      | 117000/300001 [01:20<01:55, 1583.25 examples/s]Running tokenizer on dataset (num_proc=2):  39%|███▉      | 118000/300001 [01:21<01:52, 1617.61 examples/s]Running tokenizer on dataset (num_proc=2):  40%|███▉      | 119000/300001 [01:22<01:54, 1585.05 examples/s]Running tokenizer on dataset (num_proc=2):  40%|███▉      | 120000/300001 [01:22<01:49, 1651.00 examples/s]Running tokenizer on dataset (num_proc=2):  40%|████      | 121000/300001 [01:23<01:51, 1605.68 examples/s]Running tokenizer on dataset (num_proc=2):  41%|████      | 122000/300001 [01:23<01:49, 1622.45 examples/s]Running tokenizer on dataset (num_proc=2):  41%|████      | 123000/300001 [01:24<01:53, 1565.72 examples/s]Running tokenizer on dataset (num_proc=2):  41%|████▏     | 124000/300001 [01:25<01:49, 1608.63 examples/s]Running tokenizer on dataset (num_proc=2):  42%|████▏     | 125000/300001 [01:25<01:50, 1579.22 examples/s]Running tokenizer on dataset (num_proc=2):  42%|████▏     | 126000/300001 [01:26<01:52, 1543.41 examples/s]Running tokenizer on dataset (num_proc=2):  42%|████▏     | 127000/300001 [01:27<01:48, 1596.55 examples/s]Running tokenizer on dataset (num_proc=2):  43%|████▎     | 128000/300001 [01:27<01:49, 1566.21 examples/s]Running tokenizer on dataset (num_proc=2):  43%|████▎     | 129000/300001 [01:28<01:44, 1641.77 examples/s]Running tokenizer on dataset (num_proc=2):  43%|████▎     | 130000/300001 [01:28<01:49, 1551.39 examples/s]Running tokenizer on dataset (num_proc=2):  44%|████▎     | 131000/300001 [01:29<01:44, 1616.59 examples/s]Running tokenizer on dataset (num_proc=2):  44%|████▍     | 132000/300001 [01:30<01:47, 1566.81 examples/s]Running tokenizer on dataset (num_proc=2):  44%|████▍     | 133000/300001 [01:30<01:54, 1457.70 examples/s]Running tokenizer on dataset (num_proc=2):  45%|████▍     | 134000/300001 [01:31<02:00, 1380.97 examples/s]Running tokenizer on dataset (num_proc=2):  45%|████▍     | 135000/300001 [01:32<02:02, 1344.39 examples/s]Running tokenizer on dataset (num_proc=2):  45%|████▌     | 136000/300001 [01:33<02:07, 1282.19 examples/s]Running tokenizer on dataset (num_proc=2):  46%|████▌     | 137000/300001 [01:34<02:02, 1331.77 examples/s]Running tokenizer on dataset (num_proc=2):  46%|████▌     | 138000/300001 [01:35<02:10, 1240.88 examples/s]Running tokenizer on dataset (num_proc=2):  46%|████▋     | 139000/300001 [01:35<01:52, 1434.30 examples/s]Running tokenizer on dataset (num_proc=2):  47%|████▋     | 140000/300001 [01:36<02:08, 1247.95 examples/s]Running tokenizer on dataset (num_proc=2):  47%|████▋     | 141000/300001 [01:36<01:46, 1490.93 examples/s]Running tokenizer on dataset (num_proc=2):  47%|████▋     | 142000/300001 [01:38<02:09, 1219.32 examples/s]Running tokenizer on dataset (num_proc=2):  48%|████▊     | 143000/300001 [01:38<01:40, 1566.67 examples/s]Running tokenizer on dataset (num_proc=2):  48%|████▊     | 144000/300001 [01:39<02:10, 1197.37 examples/s]Running tokenizer on dataset (num_proc=2):  49%|████▊     | 146000/300001 [01:40<01:56, 1321.76 examples/s]Running tokenizer on dataset (num_proc=2):  49%|████▉     | 147000/300001 [01:41<01:35, 1607.91 examples/s]Running tokenizer on dataset (num_proc=2):  49%|████▉     | 148000/300001 [01:42<01:50, 1381.07 examples/s]Running tokenizer on dataset (num_proc=2):  50%|████▉     | 149000/300001 [01:43<01:59, 1268.26 examples/s]Running tokenizer on dataset (num_proc=2):  50%|████▉     | 150000/300001 [01:43<01:56, 1291.08 examples/s]Running tokenizer on dataset (num_proc=2):  50%|█████     | 151000/300001 [01:44<01:59, 1244.12 examples/s]Running tokenizer on dataset (num_proc=2):  51%|█████     | 152000/300001 [01:45<01:41, 1463.44 examples/s]Running tokenizer on dataset (num_proc=2):  51%|█████     | 153000/300001 [01:46<01:55, 1276.80 examples/s]Running tokenizer on dataset (num_proc=2):  51%|█████▏    | 154000/300001 [01:46<01:28, 1649.89 examples/s]Running tokenizer on dataset (num_proc=2):  52%|█████▏    | 155000/300001 [01:47<01:49, 1329.98 examples/s]Running tokenizer on dataset (num_proc=2):  52%|█████▏    | 156000/300001 [01:47<01:22, 1738.96 examples/s]Running tokenizer on dataset (num_proc=2):  52%|█████▏    | 157000/300001 [01:48<01:45, 1354.89 examples/s]Running tokenizer on dataset (num_proc=2):  53%|█████▎    | 159000/300001 [01:49<01:37, 1439.05 examples/s]Running tokenizer on dataset (num_proc=2):  54%|█████▎    | 161000/300001 [01:51<01:32, 1501.45 examples/s]Running tokenizer on dataset (num_proc=2):  54%|█████▍    | 162000/300001 [01:51<01:17, 1780.40 examples/s]Running tokenizer on dataset (num_proc=2):  54%|█████▍    | 163000/300001 [01:52<01:33, 1470.19 examples/s]Running tokenizer on dataset (num_proc=2):  55%|█████▍    | 164000/300001 [01:52<01:15, 1803.26 examples/s]Running tokenizer on dataset (num_proc=2):  55%|█████▍    | 165000/300001 [01:53<01:34, 1433.85 examples/s]Running tokenizer on dataset (num_proc=2):  55%|█████▌    | 166000/300001 [01:53<01:15, 1768.46 examples/s]Running tokenizer on dataset (num_proc=2):  56%|█████▌    | 167000/300001 [01:55<01:32, 1433.48 examples/s]Running tokenizer on dataset (num_proc=2):  56%|█████▌    | 168000/300001 [01:55<01:16, 1727.61 examples/s]Running tokenizer on dataset (num_proc=2):  56%|█████▋    | 169000/300001 [01:56<01:33, 1397.65 examples/s]Running tokenizer on dataset (num_proc=2):  57%|█████▋    | 170000/300001 [01:56<01:14, 1753.45 examples/s]Running tokenizer on dataset (num_proc=2):  57%|█████▋    | 171000/300001 [01:57<01:31, 1415.78 examples/s]Running tokenizer on dataset (num_proc=2):  57%|█████▋    | 172000/300001 [01:57<01:17, 1661.93 examples/s]Running tokenizer on dataset (num_proc=2):  58%|█████▊    | 173000/300001 [01:58<01:27, 1451.15 examples/s]Running tokenizer on dataset (num_proc=2):  58%|█████▊    | 174000/300001 [01:59<01:17, 1630.99 examples/s]Running tokenizer on dataset (num_proc=2):  58%|█████▊    | 175000/300001 [02:00<01:26, 1453.46 examples/s]Running tokenizer on dataset (num_proc=2):  59%|█████▊    | 176000/300001 [02:00<01:14, 1666.09 examples/s]Running tokenizer on dataset (num_proc=2):  59%|█████▉    | 177000/300001 [02:01<01:24, 1461.05 examples/s]Running tokenizer on dataset (num_proc=2):  59%|█████▉    | 178000/300001 [02:01<01:11, 1699.49 examples/s]Running tokenizer on dataset (num_proc=2):  60%|█████▉    | 179000/300001 [02:02<01:23, 1448.81 examples/s]Running tokenizer on dataset (num_proc=2):  60%|█████▉    | 180000/300001 [02:03<01:08, 1751.45 examples/s]Running tokenizer on dataset (num_proc=2):  60%|██████    | 181000/300001 [02:04<01:23, 1426.94 examples/s]Running tokenizer on dataset (num_proc=2):  61%|██████    | 182000/300001 [02:04<01:06, 1772.02 examples/s]Running tokenizer on dataset (num_proc=2):  61%|██████    | 183000/300001 [02:05<01:21, 1427.33 examples/s]Running tokenizer on dataset (num_proc=2):  61%|██████▏   | 184000/300001 [02:05<01:03, 1835.72 examples/s]Running tokenizer on dataset (num_proc=2):  62%|██████▏   | 185000/300001 [02:06<01:21, 1410.35 examples/s]Running tokenizer on dataset (num_proc=2):  62%|██████▏   | 186000/300001 [02:06<01:01, 1848.85 examples/s]Running tokenizer on dataset (num_proc=2):  62%|██████▏   | 187000/300001 [02:07<01:19, 1423.36 examples/s]Running tokenizer on dataset (num_proc=2):  63%|██████▎   | 188000/300001 [02:07<01:00, 1862.88 examples/s]Running tokenizer on dataset (num_proc=2):  63%|██████▎   | 189000/300001 [02:09<01:26, 1280.04 examples/s]Running tokenizer on dataset (num_proc=2):  63%|██████▎   | 190000/300001 [02:09<01:08, 1601.14 examples/s]Running tokenizer on dataset (num_proc=2):  64%|██████▎   | 191000/300001 [02:10<01:31, 1196.70 examples/s]Running tokenizer on dataset (num_proc=2):  64%|██████▍   | 192000/300001 [02:11<01:14, 1449.24 examples/s]Running tokenizer on dataset (num_proc=2):  64%|██████▍   | 193000/300001 [02:12<01:30, 1185.00 examples/s]Running tokenizer on dataset (num_proc=2):  65%|██████▍   | 194000/300001 [02:12<01:15, 1396.63 examples/s]Running tokenizer on dataset (num_proc=2):  65%|██████▍   | 195000/300001 [02:13<01:25, 1234.55 examples/s]Running tokenizer on dataset (num_proc=2):  65%|██████▌   | 196000/300001 [02:14<01:14, 1394.00 examples/s]Running tokenizer on dataset (num_proc=2):  66%|██████▌   | 197000/300001 [02:15<01:19, 1299.77 examples/s]Running tokenizer on dataset (num_proc=2):  66%|██████▌   | 198000/300001 [02:15<01:10, 1445.13 examples/s]Running tokenizer on dataset (num_proc=2):  66%|██████▋   | 199000/300001 [02:16<01:12, 1386.42 examples/s]Running tokenizer on dataset (num_proc=2):  67%|██████▋   | 200000/300001 [02:17<01:04, 1559.16 examples/s]Running tokenizer on dataset (num_proc=2):  67%|██████▋   | 201000/300001 [02:17<01:08, 1453.49 examples/s]Running tokenizer on dataset (num_proc=2):  67%|██████▋   | 202000/300001 [02:18<01:01, 1581.70 examples/s]Running tokenizer on dataset (num_proc=2):  68%|██████▊   | 203000/300001 [02:19<01:05, 1485.31 examples/s]Running tokenizer on dataset (num_proc=2):  68%|██████▊   | 204000/300001 [02:19<01:01, 1567.63 examples/s]Running tokenizer on dataset (num_proc=2):  68%|██████▊   | 205000/300001 [02:20<01:03, 1488.57 examples/s]Running tokenizer on dataset (num_proc=2):  69%|██████▊   | 206000/300001 [02:20<01:00, 1542.38 examples/s]Running tokenizer on dataset (num_proc=2):  69%|██████▉   | 207000/300001 [02:21<01:00, 1528.02 examples/s]Running tokenizer on dataset (num_proc=2):  69%|██████▉   | 208000/300001 [02:22<00:59, 1541.25 examples/s]Running tokenizer on dataset (num_proc=2):  70%|██████▉   | 209000/300001 [02:22<00:58, 1553.58 examples/s]Running tokenizer on dataset (num_proc=2):  70%|██████▉   | 210000/300001 [02:23<00:58, 1547.80 examples/s]Running tokenizer on dataset (num_proc=2):  70%|███████   | 211000/300001 [02:24<00:55, 1599.24 examples/s]Running tokenizer on dataset (num_proc=2):  71%|███████   | 212000/300001 [02:24<00:58, 1514.82 examples/s]Running tokenizer on dataset (num_proc=2):  71%|███████   | 213000/300001 [02:25<00:58, 1483.28 examples/s]Running tokenizer on dataset (num_proc=2):  71%|███████▏  | 214000/300001 [02:26<00:54, 1570.30 examples/s]Running tokenizer on dataset (num_proc=2):  72%|███████▏  | 215000/300001 [02:27<01:03, 1332.04 examples/s]Running tokenizer on dataset (num_proc=2):  72%|███████▏  | 216000/300001 [02:27<00:53, 1564.29 examples/s]Running tokenizer on dataset (num_proc=2):  72%|███████▏  | 217000/300001 [02:28<01:06, 1242.82 examples/s]Running tokenizer on dataset (num_proc=2):  73%|███████▎  | 218000/300001 [02:29<00:56, 1439.65 examples/s]Running tokenizer on dataset (num_proc=2):  73%|███████▎  | 219000/300001 [02:30<01:06, 1218.49 examples/s]Running tokenizer on dataset (num_proc=2):  73%|███████▎  | 220000/300001 [02:30<00:57, 1392.11 examples/s]Running tokenizer on dataset (num_proc=2):  74%|███████▎  | 221000/300001 [02:31<01:03, 1244.48 examples/s]Running tokenizer on dataset (num_proc=2):  74%|███████▍  | 222000/300001 [02:32<00:56, 1381.91 examples/s]Running tokenizer on dataset (num_proc=2):  74%|███████▍  | 223000/300001 [02:33<00:59, 1293.40 examples/s]Running tokenizer on dataset (num_proc=2):  75%|███████▍  | 224000/300001 [02:33<00:54, 1395.78 examples/s]Running tokenizer on dataset (num_proc=2):  75%|███████▍  | 225000/300001 [02:34<00:57, 1296.36 examples/s]Running tokenizer on dataset (num_proc=2):  75%|███████▌  | 226000/300001 [02:35<00:48, 1510.91 examples/s]Running tokenizer on dataset (num_proc=2):  76%|███████▌  | 227000/300001 [02:36<00:58, 1237.31 examples/s]Running tokenizer on dataset (num_proc=2):  76%|███████▌  | 228000/300001 [02:36<00:44, 1632.70 examples/s]Running tokenizer on dataset (num_proc=2):  76%|███████▋  | 229000/300001 [02:37<01:00, 1168.07 examples/s]Running tokenizer on dataset (num_proc=2):  77%|███████▋  | 230000/300001 [02:38<00:47, 1461.12 examples/s]Running tokenizer on dataset (num_proc=2):  77%|███████▋  | 231000/300001 [02:39<00:55, 1236.44 examples/s]Running tokenizer on dataset (num_proc=2):  77%|███████▋  | 232000/300001 [02:39<00:49, 1379.05 examples/s]Running tokenizer on dataset (num_proc=2):  78%|███████▊  | 233000/300001 [02:40<00:50, 1313.84 examples/s]Running tokenizer on dataset (num_proc=2):  78%|███████▊  | 234000/300001 [02:41<00:49, 1327.22 examples/s]Running tokenizer on dataset (num_proc=2):  78%|███████▊  | 235000/300001 [02:41<00:44, 1446.78 examples/s]Running tokenizer on dataset (num_proc=2):  79%|███████▊  | 236000/300001 [02:42<00:48, 1311.59 examples/s]Running tokenizer on dataset (num_proc=2):  79%|███████▉  | 237000/300001 [02:43<00:39, 1577.78 examples/s]Running tokenizer on dataset (num_proc=2):  79%|███████▉  | 238000/300001 [02:44<00:49, 1262.15 examples/s]Running tokenizer on dataset (num_proc=2):  80%|███████▉  | 239000/300001 [02:44<00:40, 1522.23 examples/s]Running tokenizer on dataset (num_proc=2):  80%|███████▉  | 240000/300001 [02:45<00:47, 1274.07 examples/s]Running tokenizer on dataset (num_proc=2):  80%|████████  | 241000/300001 [02:46<00:40, 1463.57 examples/s]Running tokenizer on dataset (num_proc=2):  81%|████████  | 242000/300001 [02:47<00:42, 1360.73 examples/s]Running tokenizer on dataset (num_proc=2):  81%|████████  | 243000/300001 [02:48<00:58, 973.30 examples/s] Running tokenizer on dataset (num_proc=2):  81%|████████▏ | 244000/300001 [02:49<00:52, 1058.73 examples/s]Running tokenizer on dataset (num_proc=2):  82%|████████▏ | 245000/300001 [02:50<00:45, 1199.63 examples/s]Running tokenizer on dataset (num_proc=2):  82%|████████▏ | 246000/300001 [02:51<00:49, 1098.82 examples/s]Running tokenizer on dataset (num_proc=2):  82%|████████▏ | 247000/300001 [02:51<00:41, 1277.17 examples/s]Running tokenizer on dataset (num_proc=2):  83%|████████▎ | 248000/300001 [02:52<00:47, 1094.12 examples/s]Running tokenizer on dataset (num_proc=2):  83%|████████▎ | 249000/300001 [02:53<00:38, 1340.89 examples/s]Running tokenizer on dataset (num_proc=2):  83%|████████▎ | 250000/300001 [02:54<00:43, 1141.48 examples/s]Running tokenizer on dataset (num_proc=2):  84%|████████▎ | 251000/300001 [02:54<00:35, 1398.40 examples/s]Running tokenizer on dataset (num_proc=2):  84%|████████▍ | 252000/300001 [02:55<00:41, 1151.61 examples/s]Running tokenizer on dataset (num_proc=2):  84%|████████▍ | 253000/300001 [02:56<00:33, 1410.41 examples/s]Running tokenizer on dataset (num_proc=2):  85%|████████▍ | 254000/300001 [02:57<00:37, 1211.37 examples/s]Running tokenizer on dataset (num_proc=2):  85%|████████▍ | 255000/300001 [02:57<00:29, 1519.29 examples/s]Running tokenizer on dataset (num_proc=2):  85%|████████▌ | 256000/300001 [02:58<00:36, 1194.73 examples/s]Running tokenizer on dataset (num_proc=2):  86%|████████▌ | 257000/300001 [02:59<00:27, 1536.54 examples/s]Running tokenizer on dataset (num_proc=2):  86%|████████▌ | 258000/300001 [03:00<00:32, 1273.43 examples/s]Running tokenizer on dataset (num_proc=2):  86%|████████▋ | 259000/300001 [03:00<00:29, 1412.46 examples/s]Running tokenizer on dataset (num_proc=2):  87%|████████▋ | 260000/300001 [03:01<00:31, 1262.74 examples/s]Running tokenizer on dataset (num_proc=2):  87%|████████▋ | 261000/300001 [03:02<00:29, 1335.47 examples/s]Running tokenizer on dataset (num_proc=2):  87%|████████▋ | 262000/300001 [03:03<00:30, 1243.08 examples/s]Running tokenizer on dataset (num_proc=2):  88%|████████▊ | 263000/300001 [03:03<00:26, 1379.15 examples/s]Running tokenizer on dataset (num_proc=2):  88%|████████▊ | 264000/300001 [03:04<00:29, 1205.84 examples/s]Running tokenizer on dataset (num_proc=2):  88%|████████▊ | 265000/300001 [03:05<00:24, 1421.23 examples/s]Running tokenizer on dataset (num_proc=2):  89%|████████▊ | 266000/300001 [03:06<00:28, 1180.05 examples/s]Running tokenizer on dataset (num_proc=2):  89%|████████▉ | 267000/300001 [03:06<00:23, 1418.77 examples/s]Running tokenizer on dataset (num_proc=2):  89%|████████▉ | 268000/300001 [03:08<00:26, 1186.52 examples/s]Running tokenizer on dataset (num_proc=2):  90%|████████▉ | 269000/300001 [03:08<00:20, 1479.93 examples/s]Running tokenizer on dataset (num_proc=2):  90%|████████▉ | 270000/300001 [03:09<00:25, 1189.14 examples/s]Running tokenizer on dataset (num_proc=2):  90%|█████████ | 271000/300001 [03:09<00:19, 1513.39 examples/s]Running tokenizer on dataset (num_proc=2):  91%|█████████ | 272000/300001 [03:10<00:21, 1280.93 examples/s]Running tokenizer on dataset (num_proc=2):  91%|█████████ | 273000/300001 [03:11<00:16, 1592.91 examples/s]Running tokenizer on dataset (num_proc=2):  91%|█████████▏| 274000/300001 [03:12<00:18, 1407.38 examples/s]Running tokenizer on dataset (num_proc=2):  92%|█████████▏| 275000/300001 [03:12<00:15, 1643.35 examples/s]Running tokenizer on dataset (num_proc=2):  92%|█████████▏| 276000/300001 [03:13<00:16, 1436.53 examples/s]Running tokenizer on dataset (num_proc=2):  92%|█████████▏| 277000/300001 [03:13<00:13, 1744.00 examples/s]Running tokenizer on dataset (num_proc=2):  93%|█████████▎| 278000/300001 [03:14<00:15, 1442.41 examples/s]Running tokenizer on dataset (num_proc=2):  93%|█████████▎| 279000/300001 [03:14<00:11, 1755.71 examples/s]Running tokenizer on dataset (num_proc=2):  93%|█████████▎| 280000/300001 [03:15<00:14, 1406.10 examples/s]Running tokenizer on dataset (num_proc=2):  94%|█████████▎| 281000/300001 [03:16<00:10, 1774.68 examples/s]Running tokenizer on dataset (num_proc=2):  94%|█████████▍| 282000/300001 [03:17<00:13, 1383.56 examples/s]Running tokenizer on dataset (num_proc=2):  95%|█████████▍| 284000/300001 [03:18<00:11, 1336.26 examples/s]Running tokenizer on dataset (num_proc=2):  95%|█████████▌| 286000/300001 [03:20<00:11, 1262.30 examples/s]Running tokenizer on dataset (num_proc=2):  96%|█████████▌| 288000/300001 [03:22<00:09, 1264.16 examples/s]Running tokenizer on dataset (num_proc=2):  96%|█████████▋| 289000/300001 [03:22<00:07, 1500.31 examples/s]Running tokenizer on dataset (num_proc=2):  97%|█████████▋| 290000/300001 [03:23<00:08, 1226.81 examples/s]Running tokenizer on dataset (num_proc=2):  97%|█████████▋| 292000/300001 [03:24<00:06, 1288.49 examples/s]Running tokenizer on dataset (num_proc=2):  98%|█████████▊| 294000/300001 [03:26<00:04, 1339.64 examples/s]Running tokenizer on dataset (num_proc=2):  98%|█████████▊| 295000/300001 [03:26<00:03, 1536.37 examples/s]Running tokenizer on dataset (num_proc=2):  99%|█████████▊| 296000/300001 [03:27<00:03, 1263.72 examples/s]Running tokenizer on dataset (num_proc=2):  99%|█████████▉| 297000/300001 [03:28<00:02, 1491.89 examples/s]Running tokenizer on dataset (num_proc=2):  99%|█████████▉| 298000/300001 [03:29<00:01, 1198.99 examples/s]Running tokenizer on dataset (num_proc=2): 100%|█████████▉| 299000/300001 [03:29<00:00, 1547.71 examples/s]Running tokenizer on dataset (num_proc=2): 100%|██████████| 300001/300001 [03:30<00:00, 1234.73 examples/s]Running tokenizer on dataset (num_proc=2): 100%|██████████| 300001/300001 [03:31<00:00, 1420.64 examples/s]
192.168.0.25: training example:
192.168.0.25: input_ids:
192.168.0.25: [791, 42500, 13935, 8668, 315, 264, 13128, 374, 25, 386, 650, 735, 473, 735, 350, 358, 432, 452, 435, 362, 362, 328, 432, 445, 358, 362, 362, 350, 350, 435, 445, 328, 362, 650, 816, 445, 393, 362, 362, 435, 362, 1229, 350, 358, 650, 452, 452, 358, 816, 328, 469, 735, 480, 445, 362, 480, 350, 386, 328, 1229, 350, 1229, 445, 423, 350, 452, 350, 816, 350, 328, 1229, 362, 432, 435, 480, 468, 452, 452, 432, 432, 358, 650, 358, 423, 469, 735, 445, 735, 445, 423, 423, 735, 480, 432, 358, 445, 480, 435, 469, 362, 1229, 480, 1229, 328, 362, 435, 480, 362, 423, 358, 328, 469, 328, 816, 386, 468, 469, 452, 480, 1229, 362, 735, 468, 1229, 452, 423, 423, 469, 473, 328, 328, 650, 452, 350, 452, 1229, 393, 350, 435, 816, 650, 393, 735, 452, 328, 350, 445, 362, 358, 423, 452, 362, 445, 358, 432, 816, 445, 358, 735, 452, 423, 445, 469, 1229, 358, 735, 650, 445, 393, 473, 480, 735, 650, 350, 435, 350, 735, 445, 423, 350, 650, 735, 445, 480, 423, 469, 1229, 650, 816, 445, 816, 362, 735, 328, 480, 445, 350, 386, 350, 393, 423, 435, 362, 468, 816, 423, 735, 469, 480, 452, 816, 435, 362, 1229, 328, 468, 480, 480, 386, 816, 650, 358, 432, 423, 480, 435, 328, 735, 469, 1229, 435, 423, 1229, 445, 735, 650, 432, 1229, 445, 735, 362, 469, 1229, 362, 816, 445, 469, 452, 445, 362, 469, 1229, 445, 350, 469, 816, 473, 328, 362, 445, 445, 445, 423, 1229, 650, 452, 650, 435, 423, 650, 469, 735, 480, 350, 350, 445, 350, 480, 1229, 1229, 650, 445, 358, 469, 1229, 480, 735, 358, 650, 1229, 650, 362, 393, 735, 358, 469, 445, 480, 735, 735, 650, 362, 350, 650, 452, 480, 1229, 480, 735, 350, 445, 358, 393, 480, 445, 468, 423, 386, 473, 480, 473, 445, 328, 735, 423, 350, 480, 358, 445, 452, 358, 362, 350, 480, 650, 350, 328, 650, 432, 423, 386, 480, 452, 469, 473, 1229, 452, 445, 386, 469, 358, 1229, 362, 445, 435, 423, 350, 480, 735, 650, 445, 480, 350, 432, 650, 816, 432, 362, 480, 435, 386, 423, 735, 816, 328, 469, 452, 328, 362, 480, 445, 328, 650, 735, 350, 445, 469, 469, 362, 445, 469, 386, 650, 423, 435, 435, 362, 423, 452, 480, 816, 650, 1229, 358, 735, 445, 816, 328, 328, 358, 423, 393, 735, 468, 650, 469, 393, 358, 362, 1229, 432, 362, 473, 328, 432, 480, 445, 432, 445, 328, 480, 473, 358, 393, 362, 435, 386, 350, 362, 469, 1229, 362, 650, 473, 362, 480, 816, 452, 469, 358, 1229, 473, 358, 452, 386, 445, 435, 445, 452, 435, 445, 362, 480, 469, 469, 650, 423, 350, 432, 350, 735, 1229, 432, 435, 328, 445, 358, 480, 469, 735, 362, 362, 469, 386, 393, 386, 350, 480, 735, 469, 386, 423, 362, 435, 358, 735, 445, 445, 362, 452, 452, 452, 650, 650, 358, 423, 393, 350, 650, 328, 350, 435, 432, 328, 445, 445, 386, 328, 1229, 452, 735, 1229, 650, 423, 1229, 469, 435, 362, 469, 358, 362, 328, 473, 445, 393, 393, 452, 435, 650, 432, 452, 445, 735, 480, 362, 1229, 386, 1229, 650, 469, 469, 469, 473, 1229, 328, 362, 816, 1229, 452, 328, 480, 423, 362, 445, 445, 735, 386, 650, 735, 358, 445, 816, 423, 362, 480, 650, 393, 386, 650, 362, 480, 350, 423, 328, 650, 393, 480, 435, 350, 445, 445, 432, 469, 445, 469, 445, 816, 350, 386, 362, 480, 358, 393, 350, 350, 469, 650, 445, 735, 386, 362, 350, 358, 423, 328, 362, 432, 445, 386, 480, 650, 362, 473, 1229, 350, 480, 328, 358, 328, 469, 480, 735, 650, 362, 423, 445, 358, 445, 650, 423, 480, 423, 393, 328, 735, 423, 358, 735, 362, 445, 432, 735, 445, 328, 445, 650, 358, 735, 480, 350, 1229, 650, 435, 735, 393, 469, 362, 358, 435, 469, 1229, 358, 480, 358, 362, 393, 435, 350, 735, 362, 328, 432, 358, 650, 445, 1246, 2116, 420, 8668, 11, 279, 6070, 315, 279, 13128, 374, 25, 128750, 128357, 128268, 128672, 128657, 128516, 128335, 128388, 128296, 128335, 128407, 128516, 128497, 128334, 128350, 128334, 128407, 128649, 128661, 128731, 128302, 128335, 128535, 128585, 128302, 128727, 128648, 128756, 128651, 128725, 128270, 128732, 128547, 128343, 128378, 128585, 128513, 128476, 128315, 128487, 128330, 128566, 128510, 128417, 128417, 128520, 128732, 128321, 128433, 128435, 128419, 128386, 128603, 128516, 128366, 128417, 128346, 128441, 128549, 128493, 128344, 128493, 128315, 128581, 128288, 128651, 128343, 128432, 128487, 128746, 128585, 128369, 128651, 128504, 128651, 128592, 128272, 128625, 128531, 128306, 128730, 128366, 128420, 128526, 128446, 128593, 128517, 128420, 128520, 128443, 128334, 128518, 128581, 128347, 128514, 128739, 128593, 128726, 128663, 128586, 128625, 128328, 128695, 128571, 128263, 128739, 128572, 128301, 128366, 128371, 128483, 128432, 128429, 128268, 128671, 128502, 128562, 128330, 128602, 128431, 128430, 128326, 128622, 128713, 128678, 128352, 128742, 128478, 128340, 128584, 128675, 128685, 128476, 128297, 128368, 128588, 128723, 128336, 128272, 128727, 128592, 128258, 128408, 128396, 128369, 128268, 128321, 128705, 128424, 128268, 128321, 128423, 128547, 128624, 128308, 128417, 128261, 128520, 128343, 128543, 128580, 128656, 128411, 128368, 128486, 128334, 128257, 128343, 128569, 128576, 128649, 128303, 128514, 128305, 128413, 128476, 128343, 128354, 128475, 128547, 128512, 128547, 128648, 128493, 128462, 128483, 128256, 128585, 128294, 128398, 128575, 128365, 128331, 128623, 128649, 128593, 128283, 128349, 128281, 128575, 128289, 128582, 128487, 128493, 128524, 128298, 128756, 128706, 128349, 128732, 128649, 128330, 128458, 128305, 128516, 128288, 128514, 128727, 128403, 128702, 128487, 128268, 128353, 128487, 128265, 128286, 128333, 128298, 128366, 128330, 128587, 128298, 128268, 128353, 128518, 128578, 128310, 128620, 128547, 128435, 128592, 128514, 128614, 128432, 128696, 128648, 128365, 128462, 128448, 128360, 128315, 128346, 128306, 128629, 128577, 128305, 128340, 128404, 128462, 128690, 128593, 128593, 128517, 128497, 128285, 128438, 128384, 128310, 128331, 128677, 128492, 128393, 128759, 128305, 128346, 128270, 128667, 128539, 128592, 128364, 128441, 128392, 128446, 128267, 128610, 128603, 128274, 128472, 128694, 128524, 128590, 128469, 128381, 128718, 128525, 128401, 128678, 128635, 128358, 128323, 128483, 128297, 128575, 128444, 128471, 128387, 128483, 128746, 128317, 128487, 128290, 128366, 128629, 128381, 128443, 128547, 128435, 128720, 128547, 128466, 128484, 128693, 128524, 128765, 128271, 128446, 128486]
192.168.0.25: inputs:
192.168.0.25: The amino acid sequence of a protein is: M V K H K T I R N F A A S R L I A A T T F L S A V Y L P A A F A Q T I V N N I Y S E K G L A G T M S Q T Q L D T N T Y T S Q A R F G W N N R R I V I D E K L K L D D K G R I L G F E A Q G Q S A F G A D I S E S Y M W E N G Q A K W Q N D D E H S S V N T N Q P T F Y V P K N S T L A I D N A L I R Y L I K N D L E Q I K V L P H G K V T F T K L D T V K L G D E Q V Y L Y A K S G L T M T P D F A W Y D K E G N Y F A Q S W G G M Y V I R D G F S K E Q F D Q L K V R Q L K A E Q A Y L E N L A E Q L T E Y H S A L L L D Q V N V F D V E K G T T L T G Q Q V L I E Q G K I V Q V A P K I E L G K K V A T V N G Q G K T L I P G L W D M H G H L S K D T G I L N I A T G V T S V R D M G N E H Q N L M E I Q A L F D T G K V L G T R V Y R A G F M D K Y S E N S A G L S V K T L E E A L E M V D F F A D N G Y V Q I K L Y S S I D P K W V E P I A Q R A H S R G L R L S G H I P A F M T A E Q A V H A G Y N E I Q H I N M L F L N F L A G E E V D T R T K Q R F S L I G E K A A E M P M T G K E M D A F I K L L A N N N V V I D P T V S T F R S L L M S Q N K Q V D Q E F A E I A S H L P P N F V R N L K G A Q M Q V E E E H Q S A Y Q N S G D A L L K M V K I L Y D A G V P M V A G T D S V P G F T L L R E L E L Y T M A G I P T T E V L K M A T I D S A R L M G V A H Q T G S I S E G K V A D L I L V D G D P S K D I K A L R K L S L V I K G T Q V F K P E A I F E Q I G I A P F T K A S R I V L.Given this sequence, the structure of the protein is:珖屼袱哻豊筇臧悕傱臧穉筇爲陼玭陼穉栲蒀篨岜臧潋愲岜薠玝棷伌芎瑷阌篣茖諒愲潼矻阇槠恮瘈埘粺粺蕲阌柎稽斾苁魼訞筇咍粺嫺獐昵咥嗋咥阇睔罯伌茖醜槠蔴愲軵伌栶伌鞻稰呪墌謜厓咍鲕滓碟澶鳊鲕蕲鹣陼鵙睔娓濩鹽澶詐匜簰呪鳗韆扞鸓鹽娙濊咍阎馑醜墡袱韪儶焄恮檈鲵犫羝瑰骬饹曋厖躈睭蟺譺鵸矻臡躄麃醿圉稰薠鞻僭慞仂軵袱柎泿殛袱柎璹篣褴岫粺菙蕲茖繖綵愀擥躄穠陼伣茖酎革栲庈濩韣翕矻茖戗虩篣炤篣玝咥辋馑緭愲铏狾裲難寤雊栲澶惔撙瀷裲蟪疿槠咥闚牦棷絡撙阌栲恮漰韣筇罯濩薠篥秺槠袱鎉槠褓銲鬋牦咍恮饠牦袱鎉鵙冔魳鉾篣斾鞻濩姏醜樐玝難辋酬筌阇嫺謜雿磹韣睭遒辋箧澶澶鳊爲趯橅萹魳寤傿劙璲驞韣嫺瑷镏耒鞻镕獐磼碟蝯羆訞枌褕刼闚簪肜岭荓雟敍饹韎聱匫馑臡裲蛖茤稒馑蔴貣槠缌咍雿岭鹣篣斾龃篣籝檗扢闚錜忲碟穠
192.168.0.13: Running tokenizer on dataset (num_proc=2):   0%|          | 0/300001 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=2):   0%|          | 1000/300001 [00:02<10:54, 456.81 examples/s]Running tokenizer on dataset (num_proc=2):   1%|          | 2000/300001 [00:02<05:49, 853.65 examples/s]Running tokenizer on dataset (num_proc=2):   1%|          | 3000/300001 [00:03<05:03, 978.40 examples/s]Running tokenizer on dataset (num_proc=2):   1%|▏         | 4000/300001 [00:03<03:44, 1320.98 examples/s]Running tokenizer on dataset (num_proc=2):   2%|▏         | 5000/300001 [00:05<04:36, 1068.72 examples/s]Running tokenizer on dataset (num_proc=2):   2%|▏         | 7000/300001 [00:06<03:46, 1293.73 examples/s]Running tokenizer on dataset (num_proc=2):   3%|▎         | 9000/300001 [00:07<03:27, 1404.22 examples/s]Running tokenizer on dataset (num_proc=2):   4%|▎         | 11000/300001 [00:08<03:13, 1490.22 examples/s]Running tokenizer on dataset (num_proc=2):   4%|▍         | 12000/300001 [00:08<02:39, 1811.25 examples/s]Running tokenizer on dataset (num_proc=2):   4%|▍         | 13000/300001 [00:10<03:17, 1454.17 examples/s]Running tokenizer on dataset (num_proc=2):   5%|▍         | 15000/300001 [00:11<03:15, 1460.24 examples/s]Running tokenizer on dataset (num_proc=2):   5%|▌         | 16000/300001 [00:11<02:46, 1701.20 examples/s]Running tokenizer on dataset (num_proc=2):   6%|▌         | 17000/300001 [00:12<03:28, 1355.69 examples/s]Running tokenizer on dataset (num_proc=2):   6%|▌         | 18000/300001 [00:12<02:43, 1719.87 examples/s]Running tokenizer on dataset (num_proc=2):   6%|▋         | 19000/300001 [00:14<03:23, 1378.56 examples/s]Running tokenizer on dataset (num_proc=2):   7%|▋         | 21000/300001 [00:15<03:14, 1433.22 examples/s]Running tokenizer on dataset (num_proc=2):   8%|▊         | 23000/300001 [00:16<03:04, 1500.43 examples/s]Running tokenizer on dataset (num_proc=2):   8%|▊         | 25000/300001 [00:17<02:59, 1534.46 examples/s]Running tokenizer on dataset (num_proc=2):   9%|▊         | 26000/300001 [00:18<02:29, 1830.37 examples/s]Running tokenizer on dataset (num_proc=2):   9%|▉         | 27000/300001 [00:19<03:06, 1466.15 examples/s]Running tokenizer on dataset (num_proc=2):  10%|▉         | 29000/300001 [00:21<03:38, 1240.99 examples/s]Running tokenizer on dataset (num_proc=2):  10%|█         | 31000/300001 [00:22<03:18, 1357.93 examples/s]Running tokenizer on dataset (num_proc=2):  11%|█         | 33000/300001 [00:23<03:15, 1362.79 examples/s]Running tokenizer on dataset (num_proc=2):  11%|█▏        | 34000/300001 [00:24<02:50, 1561.94 examples/s]Running tokenizer on dataset (num_proc=2):  12%|█▏        | 35000/300001 [00:25<03:25, 1290.06 examples/s]Running tokenizer on dataset (num_proc=2):  12%|█▏        | 36000/300001 [00:25<02:51, 1542.71 examples/s]Running tokenizer on dataset (num_proc=2):  12%|█▏        | 37000/300001 [00:26<03:24, 1287.63 examples/s]Running tokenizer on dataset (num_proc=2):  13%|█▎        | 38000/300001 [00:27<02:48, 1554.87 examples/s]Running tokenizer on dataset (num_proc=2):  13%|█▎        | 39000/300001 [00:28<03:14, 1341.35 examples/s]Running tokenizer on dataset (num_proc=2):  13%|█▎        | 40000/300001 [00:28<02:54, 1492.29 examples/s]Running tokenizer on dataset (num_proc=2):  14%|█▎        | 41000/300001 [00:29<03:07, 1379.49 examples/s]Running tokenizer on dataset (num_proc=2):  14%|█▍        | 42000/300001 [00:30<03:05, 1389.59 examples/s]Running tokenizer on dataset (num_proc=2):  14%|█▍        | 43000/300001 [00:30<02:53, 1481.70 examples/s]Running tokenizer on dataset (num_proc=2):  15%|█▍        | 44000/300001 [00:31<03:13, 1321.11 examples/s]Running tokenizer on dataset (num_proc=2):  15%|█▍        | 45000/300001 [00:31<02:36, 1632.16 examples/s]Running tokenizer on dataset (num_proc=2):  15%|█▌        | 46000/300001 [00:33<03:16, 1289.49 examples/s]Running tokenizer on dataset (num_proc=2):  16%|█▌        | 47000/300001 [00:33<02:25, 1733.85 examples/s]Running tokenizer on dataset (num_proc=2):  16%|█▌        | 48000/300001 [00:34<03:14, 1294.33 examples/s]Running tokenizer on dataset (num_proc=2):  17%|█▋        | 50000/300001 [00:35<03:03, 1359.45 examples/s]Running tokenizer on dataset (num_proc=2):  17%|█▋        | 51000/300001 [00:35<02:24, 1721.08 examples/s]Running tokenizer on dataset (num_proc=2):  17%|█▋        | 52000/300001 [00:37<03:07, 1323.95 examples/s]Running tokenizer on dataset (num_proc=2):  18%|█▊        | 53000/300001 [00:37<02:33, 1608.15 examples/s]Running tokenizer on dataset (num_proc=2):  18%|█▊        | 54000/300001 [00:38<03:02, 1347.88 examples/s]Running tokenizer on dataset (num_proc=2):  18%|█▊        | 55000/300001 [00:38<02:34, 1584.26 examples/s]Running tokenizer on dataset (num_proc=2):  19%|█▊        | 56000/300001 [00:39<02:51, 1426.62 examples/s]Running tokenizer on dataset (num_proc=2):  19%|█▉        | 57000/300001 [00:40<02:43, 1484.84 examples/s]Running tokenizer on dataset (num_proc=2):  19%|█▉        | 58000/300001 [00:40<02:37, 1538.98 examples/s]Running tokenizer on dataset (num_proc=2):  20%|█▉        | 59000/300001 [00:41<02:54, 1379.92 examples/s]Running tokenizer on dataset (num_proc=2):  20%|█▉        | 60000/300001 [00:42<02:43, 1468.81 examples/s]Running tokenizer on dataset (num_proc=2):  20%|██        | 61000/300001 [00:43<02:58, 1338.15 examples/s]Running tokenizer on dataset (num_proc=2):  21%|██        | 62000/300001 [00:43<02:54, 1362.74 examples/s]Running tokenizer on dataset (num_proc=2):  21%|██        | 63000/300001 [00:44<02:57, 1335.14 examples/s]Running tokenizer on dataset (num_proc=2):  21%|██▏       | 64000/300001 [00:45<03:01, 1302.47 examples/s]Running tokenizer on dataset (num_proc=2):  22%|██▏       | 65000/300001 [00:46<02:54, 1347.18 examples/s]Running tokenizer on dataset (num_proc=2):  22%|██▏       | 66000/300001 [00:47<02:59, 1306.87 examples/s]Running tokenizer on dataset (num_proc=2):  22%|██▏       | 67000/300001 [00:47<02:51, 1360.13 examples/s]Running tokenizer on dataset (num_proc=2):  23%|██▎       | 68000/300001 [00:48<02:48, 1373.98 examples/s]Running tokenizer on dataset (num_proc=2):  23%|██▎       | 69000/300001 [00:49<02:46, 1389.10 examples/s]Running tokenizer on dataset (num_proc=2):  23%|██▎       | 70000/300001 [00:49<02:44, 1398.19 examples/s]Running tokenizer on dataset (num_proc=2):  24%|██▎       | 71000/300001 [00:50<02:48, 1362.90 examples/s]Running tokenizer on dataset (num_proc=2):  24%|██▍       | 72000/300001 [00:51<02:29, 1524.91 examples/s]Running tokenizer on dataset (num_proc=2):  24%|██▍       | 73000/300001 [00:52<02:49, 1337.91 examples/s]Running tokenizer on dataset (num_proc=2):  25%|██▍       | 74000/300001 [00:52<02:13, 1690.41 examples/s]Running tokenizer on dataset (num_proc=2):  25%|██▍       | 75000/300001 [00:54<03:55, 953.88 examples/s] Running tokenizer on dataset (num_proc=2):  26%|██▌       | 77000/300001 [00:55<03:18, 1124.84 examples/s]Running tokenizer on dataset (num_proc=2):  26%|██▋       | 79000/300001 [00:57<02:53, 1276.66 examples/s]Running tokenizer on dataset (num_proc=2):  27%|██▋       | 80000/300001 [00:57<02:27, 1489.57 examples/s]Running tokenizer on dataset (num_proc=2):  27%|██▋       | 81000/300001 [00:58<02:41, 1352.32 examples/s]Running tokenizer on dataset (num_proc=2):  27%|██▋       | 82000/300001 [00:58<02:24, 1507.87 examples/s]Running tokenizer on dataset (num_proc=2):  28%|██▊       | 83000/300001 [00:59<02:45, 1309.91 examples/s]Running tokenizer on dataset (num_proc=2):  28%|██▊       | 84000/300001 [01:00<02:20, 1535.03 examples/s]Running tokenizer on dataset (num_proc=2):  28%|██▊       | 85000/300001 [01:01<02:41, 1331.74 examples/s]Running tokenizer on dataset (num_proc=2):  29%|██▊       | 86000/300001 [01:01<02:14, 1592.17 examples/s]Running tokenizer on dataset (num_proc=2):  29%|██▉       | 87000/300001 [01:02<02:38, 1339.94 examples/s]Running tokenizer on dataset (num_proc=2):  29%|██▉       | 88000/300001 [01:02<02:23, 1481.55 examples/s]Running tokenizer on dataset (num_proc=2):  30%|██▉       | 89000/300001 [01:03<02:31, 1390.39 examples/s]Running tokenizer on dataset (num_proc=2):  30%|██▉       | 90000/300001 [01:04<02:25, 1440.34 examples/s]Running tokenizer on dataset (num_proc=2):  30%|███       | 91000/300001 [01:05<02:22, 1470.73 examples/s]Running tokenizer on dataset (num_proc=2):  31%|███       | 92000/300001 [01:05<02:17, 1512.60 examples/s]Running tokenizer on dataset (num_proc=2):  31%|███       | 93000/300001 [01:06<02:21, 1462.81 examples/s]Running tokenizer on dataset (num_proc=2):  31%|███▏      | 94000/300001 [01:06<02:09, 1587.21 examples/s]Running tokenizer on dataset (num_proc=2):  32%|███▏      | 95000/300001 [01:07<02:14, 1527.59 examples/s]Running tokenizer on dataset (num_proc=2):  32%|███▏      | 96000/300001 [01:08<02:05, 1631.35 examples/s]Running tokenizer on dataset (num_proc=2):  32%|███▏      | 97000/300001 [01:08<02:16, 1490.70 examples/s]Running tokenizer on dataset (num_proc=2):  33%|███▎      | 98000/300001 [01:09<02:09, 1563.90 examples/s]Running tokenizer on dataset (num_proc=2):  33%|███▎      | 99000/300001 [01:10<02:25, 1380.31 examples/s]Running tokenizer on dataset (num_proc=2):  33%|███▎      | 100000/300001 [01:11<02:20, 1423.15 examples/s]Running tokenizer on dataset (num_proc=2):  34%|███▎      | 101000/300001 [01:12<02:32, 1305.63 examples/s]Running tokenizer on dataset (num_proc=2):  34%|███▍      | 102000/300001 [01:12<02:26, 1347.20 examples/s]Running tokenizer on dataset (num_proc=2):  34%|███▍      | 103000/300001 [01:13<02:37, 1254.01 examples/s]Running tokenizer on dataset (num_proc=2):  35%|███▍      | 104000/300001 [01:14<02:13, 1466.52 examples/s]Running tokenizer on dataset (num_proc=2):  35%|███▍      | 105000/300001 [01:15<02:37, 1241.83 examples/s]Running tokenizer on dataset (num_proc=2):  35%|███▌      | 106000/300001 [01:15<02:08, 1515.61 examples/s]Running tokenizer on dataset (num_proc=2):  36%|███▌      | 107000/300001 [01:16<02:37, 1225.53 examples/s]Running tokenizer on dataset (num_proc=2):  36%|███▌      | 108000/300001 [01:16<01:58, 1618.06 examples/s]Running tokenizer on dataset (num_proc=2):  36%|███▋      | 109000/300001 [01:18<02:32, 1254.98 examples/s]Running tokenizer on dataset (num_proc=2):  37%|███▋      | 110000/300001 [01:18<02:09, 1463.99 examples/s]Running tokenizer on dataset (num_proc=2):  37%|███▋      | 111000/300001 [01:19<02:27, 1280.58 examples/s]Running tokenizer on dataset (num_proc=2):  37%|███▋      | 112000/300001 [01:19<02:05, 1498.94 examples/s]Running tokenizer on dataset (num_proc=2):  38%|███▊      | 113000/300001 [01:21<02:32, 1222.93 examples/s]Running tokenizer on dataset (num_proc=2):  38%|███▊      | 114000/300001 [01:21<01:59, 1553.99 examples/s]Running tokenizer on dataset (num_proc=2):  38%|███▊      | 115000/300001 [01:22<02:29, 1237.02 examples/s]Running tokenizer on dataset (num_proc=2):  39%|███▊      | 116000/300001 [01:22<01:53, 1615.07 examples/s]Running tokenizer on dataset (num_proc=2):  39%|███▉      | 117000/300001 [01:23<02:25, 1259.28 examples/s]Running tokenizer on dataset (num_proc=2):  39%|███▉      | 118000/300001 [01:24<01:52, 1617.00 examples/s]Running tokenizer on dataset (num_proc=2):  40%|███▉      | 119000/300001 [01:25<02:18, 1310.14 examples/s]Running tokenizer on dataset (num_proc=2):  40%|███▉      | 120000/300001 [01:25<01:43, 1742.39 examples/s]Running tokenizer on dataset (num_proc=2):  40%|████      | 121000/300001 [01:27<03:15, 915.25 examples/s] Running tokenizer on dataset (num_proc=2):  41%|████      | 122000/300001 [01:27<02:25, 1223.94 examples/s]Running tokenizer on dataset (num_proc=2):  41%|████      | 123000/300001 [01:29<02:51, 1031.06 examples/s]Running tokenizer on dataset (num_proc=2):  41%|████▏     | 124000/300001 [01:29<02:12, 1327.65 examples/s]Running tokenizer on dataset (num_proc=2):  42%|████▏     | 125000/300001 [01:30<02:37, 1110.44 examples/s]Running tokenizer on dataset (num_proc=2):  42%|████▏     | 126000/300001 [01:30<02:05, 1381.45 examples/s]Running tokenizer on dataset (num_proc=2):  42%|████▏     | 127000/300001 [01:31<02:22, 1214.42 examples/s]Running tokenizer on dataset (num_proc=2):  43%|████▎     | 128000/300001 [01:32<01:56, 1471.39 examples/s]Running tokenizer on dataset (num_proc=2):  43%|████▎     | 129000/300001 [01:33<02:06, 1349.31 examples/s]Running tokenizer on dataset (num_proc=2):  43%|████▎     | 130000/300001 [01:33<01:53, 1493.08 examples/s]Running tokenizer on dataset (num_proc=2):  44%|████▎     | 131000/300001 [01:34<02:05, 1347.39 examples/s]Running tokenizer on dataset (num_proc=2):  44%|████▍     | 132000/300001 [01:35<01:58, 1414.62 examples/s]Running tokenizer on dataset (num_proc=2):  44%|████▍     | 133000/300001 [01:36<02:10, 1276.95 examples/s]Running tokenizer on dataset (num_proc=2):  45%|████▍     | 134000/300001 [01:36<01:54, 1445.63 examples/s]Running tokenizer on dataset (num_proc=2):  45%|████▍     | 135000/300001 [01:37<02:07, 1296.84 examples/s]Running tokenizer on dataset (num_proc=2):  45%|████▌     | 136000/300001 [01:38<01:53, 1445.35 examples/s]Running tokenizer on dataset (num_proc=2):  46%|████▌     | 137000/300001 [01:39<02:09, 1256.32 examples/s]Running tokenizer on dataset (num_proc=2):  46%|████▌     | 138000/300001 [01:39<01:56, 1390.85 examples/s]Running tokenizer on dataset (num_proc=2):  46%|████▋     | 139000/300001 [01:40<02:09, 1239.39 examples/s]Running tokenizer on dataset (num_proc=2):  47%|████▋     | 140000/300001 [01:41<01:58, 1346.29 examples/s]Running tokenizer on dataset (num_proc=2):  47%|████▋     | 141000/300001 [01:42<02:08, 1232.72 examples/s]Running tokenizer on dataset (num_proc=2):  47%|████▋     | 142000/300001 [01:42<01:53, 1392.64 examples/s]Running tokenizer on dataset (num_proc=2):  48%|████▊     | 143000/300001 [01:43<02:08, 1222.90 examples/s]Running tokenizer on dataset (num_proc=2):  48%|████▊     | 144000/300001 [01:44<01:46, 1458.35 examples/s]Running tokenizer on dataset (num_proc=2):  48%|████▊     | 145000/300001 [01:45<02:05, 1238.39 examples/s]Running tokenizer on dataset (num_proc=2):  49%|████▊     | 146000/300001 [01:45<01:43, 1482.64 examples/s]Running tokenizer on dataset (num_proc=2):  49%|████▉     | 147000/300001 [01:46<02:03, 1243.73 examples/s]Running tokenizer on dataset (num_proc=2):  49%|████▉     | 148000/300001 [01:47<01:38, 1544.56 examples/s]Running tokenizer on dataset (num_proc=2):  50%|████▉     | 149000/300001 [01:48<01:58, 1278.94 examples/s]Running tokenizer on dataset (num_proc=2):  50%|████▉     | 150000/300001 [01:48<01:35, 1570.39 examples/s]Running tokenizer on dataset (num_proc=2):  50%|█████     | 151000/300001 [01:49<01:56, 1284.18 examples/s]Running tokenizer on dataset (num_proc=2):  51%|█████     | 152000/300001 [01:49<01:38, 1499.67 examples/s]Running tokenizer on dataset (num_proc=2):  51%|█████     | 153000/300001 [01:51<01:58, 1245.30 examples/s]Running tokenizer on dataset (num_proc=2):  51%|█████▏    | 154000/300001 [01:51<01:38, 1484.86 examples/s]Running tokenizer on dataset (num_proc=2):  52%|█████▏    | 155000/300001 [01:52<01:59, 1210.37 examples/s]Running tokenizer on dataset (num_proc=2):  52%|█████▏    | 156000/300001 [01:52<01:34, 1521.15 examples/s]Running tokenizer on dataset (num_proc=2):  52%|█████▏    | 157000/300001 [01:54<01:56, 1231.04 examples/s]Running tokenizer on dataset (num_proc=2):  53%|█████▎    | 158000/300001 [01:54<01:39, 1431.16 examples/s]Running tokenizer on dataset (num_proc=2):  53%|█████▎    | 159000/300001 [01:55<01:55, 1216.69 examples/s]Running tokenizer on dataset (num_proc=2):  53%|█████▎    | 160000/300001 [01:56<01:40, 1398.93 examples/s]Running tokenizer on dataset (num_proc=2):  54%|█████▎    | 161000/300001 [01:57<01:50, 1262.85 examples/s]Running tokenizer on dataset (num_proc=2):  54%|█████▍    | 162000/300001 [01:57<01:33, 1482.12 examples/s]Running tokenizer on dataset (num_proc=2):  54%|█████▍    | 163000/300001 [01:58<01:41, 1348.72 examples/s]Running tokenizer on dataset (num_proc=2):  55%|█████▍    | 164000/300001 [01:58<01:27, 1556.02 examples/s]Running tokenizer on dataset (num_proc=2):  55%|█████▍    | 165000/300001 [01:59<01:37, 1381.00 examples/s]Running tokenizer on dataset (num_proc=2):  55%|█████▌    | 166000/300001 [02:00<01:25, 1573.92 examples/s]Running tokenizer on dataset (num_proc=2):  56%|█████▌    | 167000/300001 [02:01<02:07, 1040.68 examples/s]Running tokenizer on dataset (num_proc=2):  56%|█████▌    | 168000/300001 [02:02<01:48, 1215.92 examples/s]Running tokenizer on dataset (num_proc=2):  56%|█████▋    | 169000/300001 [02:03<01:56, 1120.25 examples/s]Running tokenizer on dataset (num_proc=2):  57%|█████▋    | 170000/300001 [02:03<01:44, 1247.09 examples/s]Running tokenizer on dataset (num_proc=2):  57%|█████▋    | 171000/300001 [02:04<01:49, 1177.68 examples/s]Running tokenizer on dataset (num_proc=2):  57%|█████▋    | 172000/300001 [02:05<01:36, 1331.89 examples/s]Running tokenizer on dataset (num_proc=2):  58%|█████▊    | 173000/300001 [02:06<01:40, 1258.61 examples/s]Running tokenizer on dataset (num_proc=2):  58%|█████▊    | 174000/300001 [02:06<01:34, 1340.31 examples/s]Running tokenizer on dataset (num_proc=2):  58%|█████▊    | 175000/300001 [02:07<01:34, 1327.00 examples/s]Running tokenizer on dataset (num_proc=2):  59%|█████▊    | 176000/300001 [02:08<01:34, 1307.00 examples/s]Running tokenizer on dataset (num_proc=2):  59%|█████▉    | 177000/300001 [02:09<01:24, 1450.01 examples/s]Running tokenizer on dataset (num_proc=2):  59%|█████▉    | 178000/300001 [02:10<01:36, 1261.39 examples/s]Running tokenizer on dataset (num_proc=2):  60%|█████▉    | 179000/300001 [02:10<01:14, 1634.21 examples/s]Running tokenizer on dataset (num_proc=2):  60%|█████▉    | 180000/300001 [02:11<01:35, 1258.63 examples/s]Running tokenizer on dataset (num_proc=2):  61%|██████    | 182000/300001 [02:12<01:25, 1387.35 examples/s]Running tokenizer on dataset (num_proc=2):  61%|██████    | 183000/300001 [02:12<01:08, 1707.61 examples/s]Running tokenizer on dataset (num_proc=2):  61%|██████▏   | 184000/300001 [02:14<01:23, 1392.09 examples/s]Running tokenizer on dataset (num_proc=2):  62%|██████▏   | 185000/300001 [02:14<01:05, 1765.52 examples/s]Running tokenizer on dataset (num_proc=2):  62%|██████▏   | 186000/300001 [02:15<01:25, 1330.90 examples/s]Running tokenizer on dataset (num_proc=2):  62%|██████▏   | 187000/300001 [02:15<01:04, 1765.26 examples/s]Running tokenizer on dataset (num_proc=2):  63%|██████▎   | 188000/300001 [02:16<01:20, 1396.63 examples/s]Running tokenizer on dataset (num_proc=2):  63%|██████▎   | 189000/300001 [02:16<01:08, 1622.97 examples/s]Running tokenizer on dataset (num_proc=2):  63%|██████▎   | 190000/300001 [02:18<01:21, 1348.14 examples/s]Running tokenizer on dataset (num_proc=2):  64%|██████▎   | 191000/300001 [02:18<01:09, 1561.38 examples/s]Running tokenizer on dataset (num_proc=2):  64%|██████▍   | 192000/300001 [02:19<01:26, 1252.27 examples/s]Running tokenizer on dataset (num_proc=2):  64%|██████▍   | 193000/300001 [02:19<01:10, 1517.99 examples/s]Running tokenizer on dataset (num_proc=2):  65%|██████▍   | 194000/300001 [02:21<01:27, 1206.73 examples/s]Running tokenizer on dataset (num_proc=2):  65%|██████▍   | 195000/300001 [02:21<01:12, 1452.35 examples/s]Running tokenizer on dataset (num_proc=2):  65%|██████▌   | 196000/300001 [02:22<01:28, 1180.02 examples/s]Running tokenizer on dataset (num_proc=2):  66%|██████▌   | 197000/300001 [02:22<01:07, 1516.86 examples/s]Running tokenizer on dataset (num_proc=2):  66%|██████▌   | 198000/300001 [02:24<01:23, 1218.37 examples/s]Running tokenizer on dataset (num_proc=2):  66%|██████▋   | 199000/300001 [02:24<01:03, 1601.37 examples/s]Running tokenizer on dataset (num_proc=2):  67%|██████▋   | 200000/300001 [02:25<01:22, 1211.03 examples/s]Running tokenizer on dataset (num_proc=2):  67%|██████▋   | 202000/300001 [02:26<01:11, 1372.53 examples/s]Running tokenizer on dataset (num_proc=2):  68%|██████▊   | 203000/300001 [02:27<00:59, 1619.32 examples/s]Running tokenizer on dataset (num_proc=2):  68%|██████▊   | 204000/300001 [02:28<01:07, 1421.07 examples/s]Running tokenizer on dataset (num_proc=2):  68%|██████▊   | 205000/300001 [02:28<00:56, 1667.23 examples/s]Running tokenizer on dataset (num_proc=2):  69%|██████▊   | 206000/300001 [02:29<01:09, 1355.08 examples/s]Running tokenizer on dataset (num_proc=2):  69%|██████▉   | 207000/300001 [02:29<00:53, 1739.06 examples/s]Running tokenizer on dataset (num_proc=2):  69%|██████▉   | 208000/300001 [02:30<01:11, 1286.27 examples/s]Running tokenizer on dataset (num_proc=2):  70%|██████▉   | 209000/300001 [02:31<00:53, 1708.92 examples/s]Running tokenizer on dataset (num_proc=2):  70%|██████▉   | 210000/300001 [02:32<01:16, 1177.48 examples/s]Running tokenizer on dataset (num_proc=2):  70%|███████   | 211000/300001 [02:32<00:57, 1539.02 examples/s]Running tokenizer on dataset (num_proc=2):  71%|███████   | 212000/300001 [02:34<01:17, 1129.32 examples/s]Running tokenizer on dataset (num_proc=2):  71%|███████   | 213000/300001 [02:35<01:19, 1098.55 examples/s]Running tokenizer on dataset (num_proc=2):  71%|███████▏  | 214000/300001 [02:36<01:22, 1036.94 examples/s]Running tokenizer on dataset (num_proc=2):  72%|███████▏  | 215000/300001 [02:36<01:08, 1234.80 examples/s]Running tokenizer on dataset (num_proc=2):  72%|███████▏  | 216000/300001 [02:37<01:07, 1240.49 examples/s]Running tokenizer on dataset (num_proc=2):  72%|███████▏  | 217000/300001 [02:38<01:04, 1282.67 examples/s]Running tokenizer on dataset (num_proc=2):  73%|███████▎  | 218000/300001 [02:38<00:57, 1418.90 examples/s]Running tokenizer on dataset (num_proc=2):  73%|███████▎  | 219000/300001 [02:39<01:04, 1260.18 examples/s]Running tokenizer on dataset (num_proc=2):  73%|███████▎  | 220000/300001 [02:39<00:49, 1630.26 examples/s]Running tokenizer on dataset (num_proc=2):  74%|███████▎  | 221000/300001 [02:41<01:02, 1269.55 examples/s]Running tokenizer on dataset (num_proc=2):  74%|███████▍  | 222000/300001 [02:41<00:45, 1700.50 examples/s]Running tokenizer on dataset (num_proc=2):  74%|███████▍  | 223000/300001 [02:42<00:59, 1287.72 examples/s]Running tokenizer on dataset (num_proc=2):  75%|███████▍  | 224000/300001 [02:42<00:48, 1566.47 examples/s]Running tokenizer on dataset (num_proc=2):  75%|███████▍  | 225000/300001 [02:43<00:53, 1398.45 examples/s]Running tokenizer on dataset (num_proc=2):  75%|███████▌  | 226000/300001 [02:44<00:50, 1461.07 examples/s]Running tokenizer on dataset (num_proc=2):  76%|███████▌  | 227000/300001 [02:44<00:48, 1492.39 examples/s]Running tokenizer on dataset (num_proc=2):  76%|███████▌  | 228000/300001 [02:45<00:53, 1356.55 examples/s]Running tokenizer on dataset (num_proc=2):  76%|███████▋  | 229000/300001 [02:46<00:49, 1424.82 examples/s]Running tokenizer on dataset (num_proc=2):  77%|███████▋  | 230000/300001 [02:47<00:48, 1430.36 examples/s]Running tokenizer on dataset (num_proc=2):  77%|███████▋  | 231000/300001 [02:47<00:46, 1499.72 examples/s]Running tokenizer on dataset (num_proc=2):  77%|███████▋  | 232000/300001 [02:48<00:45, 1479.37 examples/s]Running tokenizer on dataset (num_proc=2):  78%|███████▊  | 233000/300001 [02:49<00:48, 1371.15 examples/s]Running tokenizer on dataset (num_proc=2):  78%|███████▊  | 234000/300001 [02:49<00:41, 1607.62 examples/s]Running tokenizer on dataset (num_proc=2):  78%|███████▊  | 235000/300001 [02:50<00:48, 1327.84 examples/s]Running tokenizer on dataset (num_proc=2):  79%|███████▊  | 236000/300001 [02:50<00:35, 1792.47 examples/s]Running tokenizer on dataset (num_proc=2):  79%|███████▉  | 237000/300001 [02:52<00:49, 1281.49 examples/s]Running tokenizer on dataset (num_proc=2):  79%|███████▉  | 238000/300001 [02:52<00:37, 1656.64 examples/s]Running tokenizer on dataset (num_proc=2):  80%|███████▉  | 239000/300001 [02:53<00:49, 1240.38 examples/s]Running tokenizer on dataset (num_proc=2):  80%|███████▉  | 240000/300001 [02:53<00:39, 1526.53 examples/s]Running tokenizer on dataset (num_proc=2):  80%|████████  | 241000/300001 [02:55<00:49, 1202.03 examples/s]Running tokenizer on dataset (num_proc=2):  81%|████████  | 242000/300001 [02:55<00:38, 1509.20 examples/s]Running tokenizer on dataset (num_proc=2):  81%|████████  | 243000/300001 [02:56<00:49, 1160.15 examples/s]Running tokenizer on dataset (num_proc=2):  81%|████████▏ | 244000/300001 [02:56<00:35, 1576.43 examples/s]Running tokenizer on dataset (num_proc=2):  82%|████████▏ | 245000/300001 [02:58<00:45, 1208.96 examples/s]Running tokenizer on dataset (num_proc=2):  82%|████████▏ | 247000/300001 [02:59<00:38, 1372.64 examples/s]Running tokenizer on dataset (num_proc=2):  83%|████████▎ | 248000/300001 [02:59<00:31, 1656.21 examples/s]Running tokenizer on dataset (num_proc=2):  83%|████████▎ | 249000/300001 [03:00<00:36, 1406.78 examples/s]Running tokenizer on dataset (num_proc=2):  83%|████████▎ | 250000/300001 [03:00<00:31, 1588.08 examples/s]Running tokenizer on dataset (num_proc=2):  84%|████████▎ | 251000/300001 [03:01<00:36, 1343.38 examples/s]Running tokenizer on dataset (num_proc=2):  84%|████████▍ | 252000/300001 [03:02<00:31, 1529.06 examples/s]Running tokenizer on dataset (num_proc=2):  84%|████████▍ | 253000/300001 [03:03<00:36, 1295.62 examples/s]Running tokenizer on dataset (num_proc=2):  85%|████████▍ | 254000/300001 [03:03<00:32, 1424.30 examples/s]Running tokenizer on dataset (num_proc=2):  85%|████████▍ | 255000/300001 [03:05<00:36, 1233.86 examples/s]Running tokenizer on dataset (num_proc=2):  85%|████████▌ | 256000/300001 [03:05<00:28, 1524.48 examples/s]Running tokenizer on dataset (num_proc=2):  86%|████████▌ | 257000/300001 [03:06<00:33, 1272.71 examples/s]Running tokenizer on dataset (num_proc=2):  86%|████████▌ | 258000/300001 [03:06<00:24, 1680.96 examples/s]Running tokenizer on dataset (num_proc=2):  86%|████████▋ | 259000/300001 [03:08<00:42, 961.36 examples/s] Running tokenizer on dataset (num_proc=2):  87%|████████▋ | 260000/300001 [03:08<00:30, 1302.37 examples/s]Running tokenizer on dataset (num_proc=2):  87%|████████▋ | 261000/300001 [03:10<00:35, 1099.69 examples/s]Running tokenizer on dataset (num_proc=2):  87%|████████▋ | 262000/300001 [03:10<00:28, 1343.35 examples/s]Running tokenizer on dataset (num_proc=2):  88%|████████▊ | 263000/300001 [03:11<00:30, 1203.63 examples/s]Running tokenizer on dataset (num_proc=2):  88%|████████▊ | 264000/300001 [03:11<00:25, 1399.71 examples/s]Running tokenizer on dataset (num_proc=2):  88%|████████▊ | 265000/300001 [03:12<00:26, 1332.91 examples/s]Running tokenizer on dataset (num_proc=2):  89%|████████▊ | 266000/300001 [03:13<00:22, 1517.98 examples/s]Running tokenizer on dataset (num_proc=2):  89%|████████▉ | 267000/300001 [03:14<00:24, 1367.51 examples/s]Running tokenizer on dataset (num_proc=2):  89%|████████▉ | 268000/300001 [03:14<00:19, 1641.59 examples/s]Running tokenizer on dataset (num_proc=2):  90%|████████▉ | 269000/300001 [03:15<00:21, 1416.54 examples/s]Running tokenizer on dataset (num_proc=2):  90%|████████▉ | 270000/300001 [03:15<00:17, 1748.51 examples/s]Running tokenizer on dataset (num_proc=2):  90%|█████████ | 271000/300001 [03:16<00:20, 1428.54 examples/s]Running tokenizer on dataset (num_proc=2):  91%|█████████ | 272000/300001 [03:16<00:15, 1810.84 examples/s]Running tokenizer on dataset (num_proc=2):  91%|█████████ | 273000/300001 [03:17<00:18, 1491.84 examples/s]Running tokenizer on dataset (num_proc=2):  91%|█████████▏| 274000/300001 [03:17<00:14, 1829.45 examples/s]Running tokenizer on dataset (num_proc=2):  92%|█████████▏| 275000/300001 [03:18<00:16, 1478.85 examples/s]Running tokenizer on dataset (num_proc=2):  92%|█████████▏| 276000/300001 [03:19<00:12, 1855.62 examples/s]Running tokenizer on dataset (num_proc=2):  92%|█████████▏| 277000/300001 [03:20<00:15, 1457.93 examples/s]Running tokenizer on dataset (num_proc=2):  93%|█████████▎| 278000/300001 [03:20<00:11, 1937.17 examples/s]Running tokenizer on dataset (num_proc=2):  93%|█████████▎| 279000/300001 [03:21<00:14, 1416.87 examples/s]Running tokenizer on dataset (num_proc=2):  94%|█████████▎| 281000/300001 [03:22<00:12, 1502.57 examples/s]Running tokenizer on dataset (num_proc=2):  94%|█████████▍| 283000/300001 [03:23<00:10, 1555.80 examples/s]Running tokenizer on dataset (num_proc=2):  95%|█████████▍| 285000/300001 [03:25<00:09, 1558.18 examples/s]Running tokenizer on dataset (num_proc=2):  96%|█████████▌| 287000/300001 [03:26<00:08, 1586.15 examples/s]Running tokenizer on dataset (num_proc=2):  96%|█████████▋| 289000/300001 [03:27<00:06, 1597.46 examples/s]Running tokenizer on dataset (num_proc=2):  97%|█████████▋| 291000/300001 [03:28<00:05, 1602.87 examples/s]Running tokenizer on dataset (num_proc=2):  98%|█████████▊| 293000/300001 [03:30<00:04, 1621.52 examples/s]Running tokenizer on dataset (num_proc=2):  98%|█████████▊| 295000/300001 [03:31<00:03, 1619.43 examples/s]Running tokenizer on dataset (num_proc=2):  99%|█████████▊| 296000/300001 [03:31<00:02, 1898.40 examples/s]Running tokenizer on dataset (num_proc=2):  99%|█████████▉| 297000/300001 [03:32<00:01, 1542.84 examples/s]Running tokenizer on dataset (num_proc=2):  99%|█████████▉| 298000/300001 [03:32<00:01, 1828.48 examples/s]Running tokenizer on dataset (num_proc=2): 100%|█████████▉| 299000/300001 [03:33<00:00, 1428.70 examples/s]Running tokenizer on dataset (num_proc=2): 100%|██████████| 300001/300001 [03:34<00:00, 1769.92 examples/s]Running tokenizer on dataset (num_proc=2): 100%|██████████| 300001/300001 [03:34<00:00, 1399.27 examples/s]
192.168.0.13: training example:
192.168.0.13: input_ids:
192.168.0.13: [791, 42500, 13935, 8668, 315, 264, 13128, 374, 25, 386, 650, 735, 473, 735, 350, 358, 432, 452, 435, 362, 362, 328, 432, 445, 358, 362, 362, 350, 350, 435, 445, 328, 362, 650, 816, 445, 393, 362, 362, 435, 362, 1229, 350, 358, 650, 452, 452, 358, 816, 328, 469, 735, 480, 445, 362, 480, 350, 386, 328, 1229, 350, 1229, 445, 423, 350, 452, 350, 816, 350, 328, 1229, 362, 432, 435, 480, 468, 452, 452, 432, 432, 358, 650, 358, 423, 469, 735, 445, 735, 445, 423, 423, 735, 480, 432, 358, 445, 480, 435, 469, 362, 1229, 480, 1229, 328, 362, 435, 480, 362, 423, 358, 328, 469, 328, 816, 386, 468, 469, 452, 480, 1229, 362, 735, 468, 1229, 452, 423, 423, 469, 473, 328, 328, 650, 452, 350, 452, 1229, 393, 350, 435, 816, 650, 393, 735, 452, 328, 350, 445, 362, 358, 423, 452, 362, 445, 358, 432, 816, 445, 358, 735, 452, 423, 445, 469, 1229, 358, 735, 650, 445, 393, 473, 480, 735, 650, 350, 435, 350, 735, 445, 423, 350, 650, 735, 445, 480, 423, 469, 1229, 650, 816, 445, 816, 362, 735, 328, 480, 445, 350, 386, 350, 393, 423, 435, 362, 468, 816, 423, 735, 469, 480, 452, 816, 435, 362, 1229, 328, 468, 480, 480, 386, 816, 650, 358, 432, 423, 480, 435, 328, 735, 469, 1229, 435, 423, 1229, 445, 735, 650, 432, 1229, 445, 735, 362, 469, 1229, 362, 816, 445, 469, 452, 445, 362, 469, 1229, 445, 350, 469, 816, 473, 328, 362, 445, 445, 445, 423, 1229, 650, 452, 650, 435, 423, 650, 469, 735, 480, 350, 350, 445, 350, 480, 1229, 1229, 650, 445, 358, 469, 1229, 480, 735, 358, 650, 1229, 650, 362, 393, 735, 358, 469, 445, 480, 735, 735, 650, 362, 350, 650, 452, 480, 1229, 480, 735, 350, 445, 358, 393, 480, 445, 468, 423, 386, 473, 480, 473, 445, 328, 735, 423, 350, 480, 358, 445, 452, 358, 362, 350, 480, 650, 350, 328, 650, 432, 423, 386, 480, 452, 469, 473, 1229, 452, 445, 386, 469, 358, 1229, 362, 445, 435, 423, 350, 480, 735, 650, 445, 480, 350, 432, 650, 816, 432, 362, 480, 435, 386, 423, 735, 816, 328, 469, 452, 328, 362, 480, 445, 328, 650, 735, 350, 445, 469, 469, 362, 445, 469, 386, 650, 423, 435, 435, 362, 423, 452, 480, 816, 650, 1229, 358, 735, 445, 816, 328, 328, 358, 423, 393, 735, 468, 650, 469, 393, 358, 362, 1229, 432, 362, 473, 328, 432, 480, 445, 432, 445, 328, 480, 473, 358, 393, 362, 435, 386, 350, 362, 469, 1229, 362, 650, 473, 362, 480, 816, 452, 469, 358, 1229, 473, 358, 452, 386, 445, 435, 445, 452, 435, 445, 362, 480, 469, 469, 650, 423, 350, 432, 350, 735, 1229, 432, 435, 328, 445, 358, 480, 469, 735, 362, 362, 469, 386, 393, 386, 350, 480, 735, 469, 386, 423, 362, 435, 358, 735, 445, 445, 362, 452, 452, 452, 650, 650, 358, 423, 393, 350, 650, 328, 350, 435, 432, 328, 445, 445, 386, 328, 1229, 452, 735, 1229, 650, 423, 1229, 469, 435, 362, 469, 358, 362, 328, 473, 445, 393, 393, 452, 435, 650, 432, 452, 445, 735, 480, 362, 1229, 386, 1229, 650, 469, 469, 469, 473, 1229, 328, 362, 816, 1229, 452, 328, 480, 423, 362, 445, 445, 735, 386, 650, 735, 358, 445, 816, 423, 362, 480, 650, 393, 386, 650, 362, 480, 350, 423, 328, 650, 393, 480, 435, 350, 445, 445, 432, 469, 445, 469, 445, 816, 350, 386, 362, 480, 358, 393, 350, 350, 469, 650, 445, 735, 386, 362, 350, 358, 423, 328, 362, 432, 445, 386, 480, 650, 362, 473, 1229, 350, 480, 328, 358, 328, 469, 480, 735, 650, 362, 423, 445, 358, 445, 650, 423, 480, 423, 393, 328, 735, 423, 358, 735, 362, 445, 432, 735, 445, 328, 445, 650, 358, 735, 480, 350, 1229, 650, 435, 735, 393, 469, 362, 358, 435, 469, 1229, 358, 480, 358, 362, 393, 435, 350, 735, 362, 328, 432, 358, 650, 445, 1246, 2116, 420, 8668, 11, 279, 6070, 315, 279, 13128, 374, 25, 128750, 128357, 128268, 128672, 128657, 128516, 128335, 128388, 128296, 128335, 128407, 128516, 128497, 128334, 128350, 128334, 128407, 128649, 128661, 128731, 128302, 128335, 128535, 128585, 128302, 128727, 128648, 128756, 128651, 128725, 128270, 128732, 128547, 128343, 128378, 128585, 128513, 128476, 128315, 128487, 128330, 128566, 128510, 128417, 128417, 128520, 128732, 128321, 128433, 128435, 128419, 128386, 128603, 128516, 128366, 128417, 128346, 128441, 128549, 128493, 128344, 128493, 128315, 128581, 128288, 128651, 128343, 128432, 128487, 128746, 128585, 128369, 128651, 128504, 128651, 128592, 128272, 128625, 128531, 128306, 128730, 128366, 128420, 128526, 128446, 128593, 128517, 128420, 128520, 128443, 128334, 128518, 128581, 128347, 128514, 128739, 128593, 128726, 128663, 128586, 128625, 128328, 128695, 128571, 128263, 128739, 128572, 128301, 128366, 128371, 128483, 128432, 128429, 128268, 128671, 128502, 128562, 128330, 128602, 128431, 128430, 128326, 128622, 128713, 128678, 128352, 128742, 128478, 128340, 128584, 128675, 128685, 128476, 128297, 128368, 128588, 128723, 128336, 128272, 128727, 128592, 128258, 128408, 128396, 128369, 128268, 128321, 128705, 128424, 128268, 128321, 128423, 128547, 128624, 128308, 128417, 128261, 128520, 128343, 128543, 128580, 128656, 128411, 128368, 128486, 128334, 128257, 128343, 128569, 128576, 128649, 128303, 128514, 128305, 128413, 128476, 128343, 128354, 128475, 128547, 128512, 128547, 128648, 128493, 128462, 128483, 128256, 128585, 128294, 128398, 128575, 128365, 128331, 128623, 128649, 128593, 128283, 128349, 128281, 128575, 128289, 128582, 128487, 128493, 128524, 128298, 128756, 128706, 128349, 128732, 128649, 128330, 128458, 128305, 128516, 128288, 128514, 128727, 128403, 128702, 128487, 128268, 128353, 128487, 128265, 128286, 128333, 128298, 128366, 128330, 128587, 128298, 128268, 128353, 128518, 128578, 128310, 128620, 128547, 128435, 128592, 128514, 128614, 128432, 128696, 128648, 128365, 128462, 128448, 128360, 128315, 128346, 128306, 128629, 128577, 128305, 128340, 128404, 128462, 128690, 128593, 128593, 128517, 128497, 128285, 128438, 128384, 128310, 128331, 128677, 128492, 128393, 128759, 128305, 128346, 128270, 128667, 128539, 128592, 128364, 128441, 128392, 128446, 128267, 128610, 128603, 128274, 128472, 128694, 128524, 128590, 128469, 128381, 128718, 128525, 128401, 128678, 128635, 128358, 128323, 128483, 128297, 128575, 128444, 128471, 128387, 128483, 128746, 128317, 128487, 128290, 128366, 128629, 128381, 128443, 128547, 128435, 128720, 128547, 128466, 128484, 128693, 128524, 128765, 128271, 128446, 128486]
192.168.0.13: inputs:
192.168.0.13: The amino acid sequence of a protein is: M V K H K T I R N F A A S R L I A A T T F L S A V Y L P A A F A Q T I V N N I Y S E K G L A G T M S Q T Q L D T N T Y T S Q A R F G W N N R R I V I D E K L K L D D K G R I L G F E A Q G Q S A F G A D I S E S Y M W E N G Q A K W Q N D D E H S S V N T N Q P T F Y V P K N S T L A I D N A L I R Y L I K N D L E Q I K V L P H G K V T F T K L D T V K L G D E Q V Y L Y A K S G L T M T P D F A W Y D K E G N Y F A Q S W G G M Y V I R D G F S K E Q F D Q L K V R Q L K A E Q A Y L E N L A E Q L T E Y H S A L L L D Q V N V F D V E K G T T L T G Q Q V L I E Q G K I V Q V A P K I E L G K K V A T V N G Q G K T L I P G L W D M H G H L S K D T G I L N I A T G V T S V R D M G N E H Q N L M E I Q A L F D T G K V L G T R V Y R A G F M D K Y S E N S A G L S V K T L E E A L E M V D F F A D N G Y V Q I K L Y S S I D P K W V E P I A Q R A H S R G L R L S G H I P A F M T A E Q A V H A G Y N E I Q H I N M L F L N F L A G E E V D T R T K Q R F S L I G E K A A E M P M T G K E M D A F I K L L A N N N V V I D P T V S T F R S L L M S Q N K Q V D Q E F A E I A S H L P P N F V R N L K G A Q M Q V E E E H Q S A Y Q N S G D A L L K M V K I L Y D A G V P M V A G T D S V P G F T L L R E L E L Y T M A G I P T T E V L K M A T I D S A R L M G V A H Q T G S I S E G K V A D L I L V D G D P S K D I K A L R K L S L V I K G T Q V F K P E A I F E Q I G I A P F T K A S R I V L.Given this sequence, the structure of the protein is:珖屼袱哻豊筇臧悕傱臧穉筇爲陼玭陼穉栲蒀篨岜臧潋愲岜薠玝棷伌芎瑷阌篣茖諒愲潼矻阇槠恮瘈埘粺粺蕲阌柎稽斾苁魼訞筇咍粺嫺獐昵咥嗋咥阇睔罯伌茖醜槠蔴愲軵伌栶伌鞻稰呪墌謜厓咍鲕滓碟澶鳊鲕蕲鹣陼鵙睔娓濩鹽澶詐匜簰呪鳗韆扞鸓鹽娙濊咍阎馑醜墡袱韪儶焄恮檈鲵犫羝瑰骬饹曋厖躈睭蟺譺鵸矻臡躄麃醿圉稰薠鞻僭慞仂軵袱柎泿殛袱柎璹篣褴岫粺菙蕲茖繖綵愀擥躄穠陼伣茖酎革栲庈濩韣翕矻茖戗虩篣炤篣玝咥辋馑緭愲铏狾裲難寤雊栲澶惔撙瀷裲蟪疿槠咥闚牦棷絡撙阌栲恮漰韣筇罯濩薠篥秺槠袱鎉槠褓銲鬋牦咍恮饠牦袱鎉鵙冔魳鉾篣斾鞻濩姏醜樐玝難辋酬筌阇嫺謜雿磹韣睭遒辋箧澶澶鳊爲趯橅萹魳寤傿劙璲驞韣嫺瑷镏耒鞻镕獐磼碟蝯羆訞枌褕刼闚簪肜岭荓雟敍饹韎聱匫馑臡裲蛖茤稒馑蔴貣槠缌咍雿岭鹣篣斾龃篣籝檗扢闚錜忲碟穠
192.168.0.149: Running tokenizer on dataset (num_proc=2):   0%|          | 0/300001 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=2):   0%|          | 1000/300001 [00:02<11:49, 421.47 examples/s]Running tokenizer on dataset (num_proc=2):   1%|          | 2000/300001 [00:02<05:34, 889.58 examples/s]Running tokenizer on dataset (num_proc=2):   1%|          | 3000/300001 [00:03<05:18, 932.53 examples/s]Running tokenizer on dataset (num_proc=2):   1%|▏         | 4000/300001 [00:03<03:35, 1372.71 examples/s]Running tokenizer on dataset (num_proc=2):   2%|▏         | 5000/300001 [00:04<04:11, 1174.16 examples/s]Running tokenizer on dataset (num_proc=2):   2%|▏         | 6000/300001 [00:05<03:05, 1583.50 examples/s]Running tokenizer on dataset (num_proc=2):   2%|▏         | 7000/300001 [00:06<03:40, 1328.71 examples/s]Running tokenizer on dataset (num_proc=2):   3%|▎         | 8000/300001 [00:06<02:55, 1659.71 examples/s]Running tokenizer on dataset (num_proc=2):   3%|▎         | 9000/300001 [00:07<03:36, 1343.85 examples/s]Running tokenizer on dataset (num_proc=2):   3%|▎         | 10000/300001 [00:07<02:49, 1714.71 examples/s]Running tokenizer on dataset (num_proc=2):   4%|▎         | 11000/300001 [00:08<03:28, 1384.30 examples/s]Running tokenizer on dataset (num_proc=2):   4%|▍         | 12000/300001 [00:08<02:43, 1760.47 examples/s]Running tokenizer on dataset (num_proc=2):   4%|▍         | 13000/300001 [00:09<03:22, 1417.63 examples/s]Running tokenizer on dataset (num_proc=2):   5%|▍         | 14000/300001 [00:10<02:43, 1752.57 examples/s]Running tokenizer on dataset (num_proc=2):   5%|▍         | 15000/300001 [00:11<03:15, 1457.29 examples/s]Running tokenizer on dataset (num_proc=2):   5%|▌         | 16000/300001 [00:11<02:40, 1765.29 examples/s]Running tokenizer on dataset (num_proc=2):   6%|▌         | 17000/300001 [00:12<03:15, 1448.19 examples/s]Running tokenizer on dataset (num_proc=2):   6%|▌         | 18000/300001 [00:12<02:38, 1779.61 examples/s]Running tokenizer on dataset (num_proc=2):   6%|▋         | 19000/300001 [00:13<03:11, 1463.84 examples/s]Running tokenizer on dataset (num_proc=2):   7%|▋         | 20000/300001 [00:13<02:38, 1764.02 examples/s]Running tokenizer on dataset (num_proc=2):   7%|▋         | 21000/300001 [00:14<03:10, 1468.24 examples/s]Running tokenizer on dataset (num_proc=2):   7%|▋         | 22000/300001 [00:15<02:42, 1705.68 examples/s]Running tokenizer on dataset (num_proc=2):   8%|▊         | 23000/300001 [00:16<03:08, 1470.94 examples/s]Running tokenizer on dataset (num_proc=2):   8%|▊         | 24000/300001 [00:16<03:08, 1461.01 examples/s]Running tokenizer on dataset (num_proc=2):   8%|▊         | 25000/300001 [00:17<02:57, 1549.29 examples/s]Running tokenizer on dataset (num_proc=2):   9%|▊         | 26000/300001 [00:18<03:16, 1397.05 examples/s]Running tokenizer on dataset (num_proc=2):   9%|▉         | 27000/300001 [00:19<04:00, 1134.25 examples/s]Running tokenizer on dataset (num_proc=2):   9%|▉         | 28000/300001 [00:20<03:56, 1149.19 examples/s]Running tokenizer on dataset (num_proc=2):  10%|▉         | 29000/300001 [00:21<03:47, 1191.20 examples/s]Running tokenizer on dataset (num_proc=2):  10%|▉         | 30000/300001 [00:21<03:18, 1363.40 examples/s]Running tokenizer on dataset (num_proc=2):  10%|█         | 31000/300001 [00:22<03:39, 1227.57 examples/s]Running tokenizer on dataset (num_proc=2):  11%|█         | 32000/300001 [00:22<02:53, 1547.99 examples/s]Running tokenizer on dataset (num_proc=2):  11%|█         | 33000/300001 [00:24<03:34, 1244.86 examples/s]Running tokenizer on dataset (num_proc=2):  12%|█▏        | 35000/300001 [00:25<03:10, 1391.67 examples/s]Running tokenizer on dataset (num_proc=2):  12%|█▏        | 36000/300001 [00:25<02:46, 1583.45 examples/s]Running tokenizer on dataset (num_proc=2):  12%|█▏        | 37000/300001 [00:26<03:00, 1453.80 examples/s]Running tokenizer on dataset (num_proc=2):  13%|█▎        | 38000/300001 [00:27<03:09, 1382.82 examples/s]Running tokenizer on dataset (num_proc=2):  13%|█▎        | 39000/300001 [00:27<02:48, 1551.93 examples/s]Running tokenizer on dataset (num_proc=2):  13%|█▎        | 40000/300001 [00:28<03:23, 1279.24 examples/s]Running tokenizer on dataset (num_proc=2):  14%|█▎        | 41000/300001 [00:29<02:31, 1705.10 examples/s]Running tokenizer on dataset (num_proc=2):  14%|█▍        | 42000/300001 [00:30<03:21, 1281.18 examples/s]Running tokenizer on dataset (num_proc=2):  14%|█▍        | 43000/300001 [00:30<02:40, 1596.93 examples/s]Running tokenizer on dataset (num_proc=2):  15%|█▍        | 44000/300001 [00:31<03:05, 1379.57 examples/s]Running tokenizer on dataset (num_proc=2):  15%|█▍        | 45000/300001 [00:31<02:47, 1525.99 examples/s]Running tokenizer on dataset (num_proc=2):  15%|█▌        | 46000/300001 [00:32<02:51, 1482.81 examples/s]Running tokenizer on dataset (num_proc=2):  16%|█▌        | 47000/300001 [00:33<02:55, 1442.45 examples/s]Running tokenizer on dataset (num_proc=2):  16%|█▌        | 48000/300001 [00:34<02:58, 1409.31 examples/s]Running tokenizer on dataset (num_proc=2):  16%|█▋        | 49000/300001 [00:34<03:05, 1356.25 examples/s]Running tokenizer on dataset (num_proc=2):  17%|█▋        | 50000/300001 [00:35<03:16, 1275.30 examples/s]Running tokenizer on dataset (num_proc=2):  17%|█▋        | 51000/300001 [00:36<02:58, 1398.09 examples/s]Running tokenizer on dataset (num_proc=2):  17%|█▋        | 52000/300001 [00:37<03:27, 1197.32 examples/s]Running tokenizer on dataset (num_proc=2):  18%|█▊        | 53000/300001 [00:37<02:49, 1456.06 examples/s]Running tokenizer on dataset (num_proc=2):  18%|█▊        | 54000/300001 [00:39<03:39, 1121.36 examples/s]Running tokenizer on dataset (num_proc=2):  18%|█▊        | 55000/300001 [00:39<02:56, 1391.39 examples/s]Running tokenizer on dataset (num_proc=2):  19%|█▊        | 56000/300001 [00:40<03:45, 1080.65 examples/s]Running tokenizer on dataset (num_proc=2):  19%|█▉        | 58000/300001 [00:42<03:22, 1196.86 examples/s]Running tokenizer on dataset (num_proc=2):  20%|█▉        | 59000/300001 [00:42<02:47, 1443.00 examples/s]Running tokenizer on dataset (num_proc=2):  20%|█▉        | 60000/300001 [00:44<03:34, 1119.48 examples/s]Running tokenizer on dataset (num_proc=2):  20%|██        | 61000/300001 [00:44<02:43, 1461.75 examples/s]Running tokenizer on dataset (num_proc=2):  21%|██        | 62000/300001 [00:45<03:24, 1161.30 examples/s]Running tokenizer on dataset (num_proc=2):  21%|██▏       | 64000/300001 [00:46<02:59, 1311.25 examples/s]Running tokenizer on dataset (num_proc=2):  22%|██▏       | 65000/300001 [00:47<02:24, 1624.20 examples/s]Running tokenizer on dataset (num_proc=2):  22%|██▏       | 66000/300001 [00:48<02:57, 1315.22 examples/s]Running tokenizer on dataset (num_proc=2):  22%|██▏       | 67000/300001 [00:48<02:21, 1641.11 examples/s]Running tokenizer on dataset (num_proc=2):  23%|██▎       | 68000/300001 [00:49<02:52, 1344.69 examples/s]Running tokenizer on dataset (num_proc=2):  23%|██▎       | 69000/300001 [00:49<02:18, 1665.20 examples/s]Running tokenizer on dataset (num_proc=2):  23%|██▎       | 70000/300001 [00:50<02:46, 1385.51 examples/s]Running tokenizer on dataset (num_proc=2):  24%|██▎       | 71000/300001 [00:51<02:14, 1696.44 examples/s]Running tokenizer on dataset (num_proc=2):  24%|██▍       | 72000/300001 [00:52<03:24, 1117.06 examples/s]Running tokenizer on dataset (num_proc=2):  24%|██▍       | 73000/300001 [00:53<03:08, 1207.38 examples/s]Running tokenizer on dataset (num_proc=2):  25%|██▍       | 74000/300001 [00:55<04:04, 924.43 examples/s] Running tokenizer on dataset (num_proc=2):  25%|██▍       | 75000/300001 [00:55<03:24, 1099.25 examples/s]Running tokenizer on dataset (num_proc=2):  25%|██▌       | 76000/300001 [00:56<03:24, 1093.94 examples/s]Running tokenizer on dataset (num_proc=2):  26%|██▌       | 77000/300001 [00:57<03:10, 1171.62 examples/s]Running tokenizer on dataset (num_proc=2):  26%|██▌       | 78000/300001 [00:57<02:57, 1248.10 examples/s]Running tokenizer on dataset (num_proc=2):  26%|██▋       | 79000/300001 [00:58<03:09, 1166.98 examples/s]Running tokenizer on dataset (num_proc=2):  27%|██▋       | 80000/300001 [00:59<02:40, 1367.06 examples/s]Running tokenizer on dataset (num_proc=2):  27%|██▋       | 81000/300001 [01:00<02:59, 1219.43 examples/s]Running tokenizer on dataset (num_proc=2):  27%|██▋       | 82000/300001 [01:00<02:38, 1371.75 examples/s]Running tokenizer on dataset (num_proc=2):  28%|██▊       | 83000/300001 [01:01<02:53, 1249.91 examples/s]Running tokenizer on dataset (num_proc=2):  28%|██▊       | 84000/300001 [01:02<02:47, 1287.62 examples/s]Running tokenizer on dataset (num_proc=2):  28%|██▊       | 85000/300001 [01:03<02:56, 1215.68 examples/s]Running tokenizer on dataset (num_proc=2):  29%|██▊       | 86000/300001 [01:04<02:52, 1242.68 examples/s]Running tokenizer on dataset (num_proc=2):  29%|██▉       | 87000/300001 [01:05<02:51, 1239.20 examples/s]Running tokenizer on dataset (num_proc=2):  29%|██▉       | 88000/300001 [01:05<02:54, 1212.28 examples/s]Running tokenizer on dataset (num_proc=2):  30%|██▉       | 89000/300001 [01:06<02:42, 1297.75 examples/s]Running tokenizer on dataset (num_proc=2):  30%|██▉       | 90000/300001 [01:07<02:35, 1350.91 examples/s]Running tokenizer on dataset (num_proc=2):  30%|███       | 91000/300001 [01:07<02:37, 1326.83 examples/s]Running tokenizer on dataset (num_proc=2):  31%|███       | 92000/300001 [01:08<02:17, 1510.76 examples/s]Running tokenizer on dataset (num_proc=2):  31%|███       | 93000/300001 [01:09<02:47, 1232.84 examples/s]Running tokenizer on dataset (num_proc=2):  31%|███▏      | 94000/300001 [01:09<02:11, 1570.31 examples/s]Running tokenizer on dataset (num_proc=2):  32%|███▏      | 95000/300001 [01:10<02:44, 1246.52 examples/s]Running tokenizer on dataset (num_proc=2):  32%|███▏      | 97000/300001 [01:12<02:28, 1364.04 examples/s]Running tokenizer on dataset (num_proc=2):  33%|███▎      | 98000/300001 [01:12<02:07, 1579.34 examples/s]Running tokenizer on dataset (num_proc=2):  33%|███▎      | 99000/300001 [01:13<02:43, 1226.01 examples/s]Running tokenizer on dataset (num_proc=2):  33%|███▎      | 100000/300001 [01:14<02:24, 1380.09 examples/s]Running tokenizer on dataset (num_proc=2):  34%|███▎      | 101000/300001 [01:15<02:49, 1176.69 examples/s]Running tokenizer on dataset (num_proc=2):  34%|███▍      | 102000/300001 [01:16<02:26, 1350.23 examples/s]Running tokenizer on dataset (num_proc=2):  34%|███▍      | 103000/300001 [01:17<02:51, 1148.94 examples/s]Running tokenizer on dataset (num_proc=2):  35%|███▍      | 104000/300001 [01:17<02:27, 1328.88 examples/s]Running tokenizer on dataset (num_proc=2):  35%|███▍      | 105000/300001 [01:18<02:44, 1185.25 examples/s]Running tokenizer on dataset (num_proc=2):  35%|███▌      | 106000/300001 [01:19<02:29, 1298.83 examples/s]Running tokenizer on dataset (num_proc=2):  36%|███▌      | 107000/300001 [01:20<02:43, 1181.14 examples/s]Running tokenizer on dataset (num_proc=2):  36%|███▌      | 108000/300001 [01:21<02:29, 1286.32 examples/s]Running tokenizer on dataset (num_proc=2):  36%|███▋      | 109000/300001 [01:21<02:28, 1289.54 examples/s]Running tokenizer on dataset (num_proc=2):  37%|███▋      | 110000/300001 [01:22<02:26, 1294.14 examples/s]Running tokenizer on dataset (num_proc=2):  37%|███▋      | 111000/300001 [01:23<02:24, 1307.54 examples/s]Running tokenizer on dataset (num_proc=2):  37%|███▋      | 112000/300001 [01:24<02:28, 1269.28 examples/s]Running tokenizer on dataset (num_proc=2):  38%|███▊      | 113000/300001 [01:24<02:24, 1294.62 examples/s]Running tokenizer on dataset (num_proc=2):  38%|███▊      | 114000/300001 [01:25<02:25, 1281.25 examples/s]Running tokenizer on dataset (num_proc=2):  38%|███▊      | 115000/300001 [01:26<02:23, 1291.97 examples/s]Running tokenizer on dataset (num_proc=2):  39%|███▊      | 116000/300001 [01:27<02:26, 1256.63 examples/s]Running tokenizer on dataset (num_proc=2):  39%|███▉      | 117000/300001 [01:28<03:06, 982.41 examples/s] Running tokenizer on dataset (num_proc=2):  40%|███▉      | 119000/300001 [01:30<02:35, 1166.09 examples/s]Running tokenizer on dataset (num_proc=2):  40%|███▉      | 120000/300001 [01:30<02:11, 1371.21 examples/s]Running tokenizer on dataset (num_proc=2):  40%|████      | 121000/300001 [01:31<02:23, 1246.93 examples/s]Running tokenizer on dataset (num_proc=2):  41%|████      | 122000/300001 [01:32<02:36, 1139.16 examples/s]Running tokenizer on dataset (num_proc=2):  41%|████      | 123000/300001 [01:33<02:20, 1256.91 examples/s]Running tokenizer on dataset (num_proc=2):  41%|████▏     | 124000/300001 [01:34<02:27, 1195.05 examples/s]Running tokenizer on dataset (num_proc=2):  42%|████▏     | 125000/300001 [01:34<02:19, 1251.80 examples/s]Running tokenizer on dataset (num_proc=2):  42%|████▏     | 126000/300001 [01:35<02:16, 1275.47 examples/s]Running tokenizer on dataset (num_proc=2):  42%|████▏     | 127000/300001 [01:36<02:17, 1255.69 examples/s]Running tokenizer on dataset (num_proc=2):  43%|████▎     | 128000/300001 [01:37<02:10, 1318.64 examples/s]Running tokenizer on dataset (num_proc=2):  43%|████▎     | 129000/300001 [01:38<02:20, 1218.55 examples/s]Running tokenizer on dataset (num_proc=2):  43%|████▎     | 130000/300001 [01:38<01:58, 1435.77 examples/s]Running tokenizer on dataset (num_proc=2):  44%|████▎     | 131000/300001 [01:39<02:19, 1208.72 examples/s]Running tokenizer on dataset (num_proc=2):  44%|████▍     | 132000/300001 [01:40<02:00, 1391.38 examples/s]Running tokenizer on dataset (num_proc=2):  44%|████▍     | 133000/300001 [01:41<02:13, 1253.45 examples/s]Running tokenizer on dataset (num_proc=2):  45%|████▍     | 134000/300001 [01:41<02:03, 1346.50 examples/s]Running tokenizer on dataset (num_proc=2):  45%|████▍     | 135000/300001 [01:42<02:12, 1248.87 examples/s]Running tokenizer on dataset (num_proc=2):  45%|████▌     | 136000/300001 [01:43<01:55, 1422.17 examples/s]Running tokenizer on dataset (num_proc=2):  46%|████▌     | 137000/300001 [01:44<02:14, 1210.41 examples/s]Running tokenizer on dataset (num_proc=2):  46%|████▌     | 138000/300001 [01:44<01:53, 1431.06 examples/s]Running tokenizer on dataset (num_proc=2):  46%|████▋     | 139000/300001 [01:45<02:20, 1141.89 examples/s]Running tokenizer on dataset (num_proc=2):  47%|████▋     | 140000/300001 [01:45<01:44, 1536.77 examples/s]Running tokenizer on dataset (num_proc=2):  47%|████▋     | 141000/300001 [01:47<02:22, 1113.48 examples/s]Running tokenizer on dataset (num_proc=2):  48%|████▊     | 143000/300001 [01:49<02:14, 1166.53 examples/s]Running tokenizer on dataset (num_proc=2):  48%|████▊     | 144000/300001 [01:49<01:46, 1470.21 examples/s]Running tokenizer on dataset (num_proc=2):  48%|████▊     | 145000/300001 [01:50<02:11, 1182.95 examples/s]Running tokenizer on dataset (num_proc=2):  49%|████▊     | 146000/300001 [01:50<01:51, 1381.38 examples/s]Running tokenizer on dataset (num_proc=2):  49%|████▉     | 147000/300001 [01:52<02:05, 1217.14 examples/s]Running tokenizer on dataset (num_proc=2):  49%|████▉     | 148000/300001 [01:52<01:54, 1323.16 examples/s]Running tokenizer on dataset (num_proc=2):  50%|████▉     | 149000/300001 [01:53<02:03, 1222.52 examples/s]Running tokenizer on dataset (num_proc=2):  50%|████▉     | 150000/300001 [01:53<01:39, 1503.17 examples/s]Running tokenizer on dataset (num_proc=2):  50%|█████     | 151000/300001 [01:55<02:02, 1211.63 examples/s]Running tokenizer on dataset (num_proc=2):  51%|█████     | 153000/300001 [01:56<01:54, 1286.56 examples/s]Running tokenizer on dataset (num_proc=2):  52%|█████▏    | 155000/300001 [01:58<01:52, 1283.47 examples/s]Running tokenizer on dataset (num_proc=2):  52%|█████▏    | 157000/300001 [01:59<01:46, 1339.66 examples/s]Running tokenizer on dataset (num_proc=2):  53%|█████▎    | 158000/300001 [01:59<01:33, 1522.83 examples/s]Running tokenizer on dataset (num_proc=2):  53%|█████▎    | 159000/300001 [02:00<01:46, 1324.86 examples/s]Running tokenizer on dataset (num_proc=2):  53%|█████▎    | 160000/300001 [02:01<01:38, 1419.96 examples/s]Running tokenizer on dataset (num_proc=2):  54%|█████▎    | 161000/300001 [02:02<01:49, 1274.51 examples/s]Running tokenizer on dataset (num_proc=2):  54%|█████▍    | 162000/300001 [02:02<01:35, 1440.81 examples/s]Running tokenizer on dataset (num_proc=2):  54%|█████▍    | 163000/300001 [02:04<02:09, 1054.15 examples/s]Running tokenizer on dataset (num_proc=2):  55%|█████▍    | 164000/300001 [02:05<01:59, 1142.62 examples/s]Running tokenizer on dataset (num_proc=2):  55%|█████▍    | 165000/300001 [02:05<01:55, 1168.31 examples/s]Running tokenizer on dataset (num_proc=2):  55%|█████▌    | 166000/300001 [02:06<01:54, 1168.36 examples/s]Running tokenizer on dataset (num_proc=2):  56%|█████▌    | 167000/300001 [02:08<02:15, 981.41 examples/s] Running tokenizer on dataset (num_proc=2):  56%|█████▌    | 168000/300001 [02:08<01:47, 1224.93 examples/s]Running tokenizer on dataset (num_proc=2):  56%|█████▋    | 169000/300001 [02:09<01:52, 1164.13 examples/s]Running tokenizer on dataset (num_proc=2):  57%|█████▋    | 170000/300001 [02:09<01:30, 1437.11 examples/s]Running tokenizer on dataset (num_proc=2):  57%|█████▋    | 171000/300001 [02:10<01:40, 1283.11 examples/s]Running tokenizer on dataset (num_proc=2):  57%|█████▋    | 172000/300001 [02:11<01:34, 1355.72 examples/s]Running tokenizer on dataset (num_proc=2):  58%|█████▊    | 173000/300001 [02:12<01:29, 1424.24 examples/s]Running tokenizer on dataset (num_proc=2):  58%|█████▊    | 174000/300001 [02:12<01:30, 1393.37 examples/s]Running tokenizer on dataset (num_proc=2):  58%|█████▊    | 175000/300001 [02:13<01:22, 1515.66 examples/s]Running tokenizer on dataset (num_proc=2):  59%|█████▊    | 176000/300001 [02:14<01:24, 1463.29 examples/s]Running tokenizer on dataset (num_proc=2):  59%|█████▉    | 177000/300001 [02:14<01:20, 1529.02 examples/s]Running tokenizer on dataset (num_proc=2):  59%|█████▉    | 178000/300001 [02:15<01:19, 1541.61 examples/s]Running tokenizer on dataset (num_proc=2):  60%|█████▉    | 179000/300001 [02:16<01:32, 1311.92 examples/s]Running tokenizer on dataset (num_proc=2):  60%|█████▉    | 180000/300001 [02:16<01:25, 1406.98 examples/s]Running tokenizer on dataset (num_proc=2):  60%|██████    | 181000/300001 [02:18<01:38, 1211.03 examples/s]Running tokenizer on dataset (num_proc=2):  61%|██████    | 182000/300001 [02:18<01:20, 1458.23 examples/s]Running tokenizer on dataset (num_proc=2):  61%|██████    | 183000/300001 [02:19<01:39, 1170.24 examples/s]Running tokenizer on dataset (num_proc=2):  61%|██████▏   | 184000/300001 [02:19<01:21, 1427.87 examples/s]Running tokenizer on dataset (num_proc=2):  62%|██████▏   | 185000/300001 [02:21<01:41, 1131.35 examples/s]Running tokenizer on dataset (num_proc=2):  62%|██████▏   | 186000/300001 [02:21<01:16, 1499.28 examples/s]Running tokenizer on dataset (num_proc=2):  62%|██████▏   | 187000/300001 [02:22<01:38, 1146.73 examples/s]Running tokenizer on dataset (num_proc=2):  63%|██████▎   | 188000/300001 [02:22<01:12, 1536.08 examples/s]Running tokenizer on dataset (num_proc=2):  63%|██████▎   | 189000/300001 [02:24<01:28, 1255.50 examples/s]Running tokenizer on dataset (num_proc=2):  63%|██████▎   | 190000/300001 [02:24<01:08, 1601.53 examples/s]Running tokenizer on dataset (num_proc=2):  64%|██████▎   | 191000/300001 [02:25<01:19, 1371.53 examples/s]Running tokenizer on dataset (num_proc=2):  64%|██████▍   | 192000/300001 [02:25<01:05, 1642.04 examples/s]Running tokenizer on dataset (num_proc=2):  64%|██████▍   | 193000/300001 [02:26<01:12, 1473.95 examples/s]Running tokenizer on dataset (num_proc=2):  65%|██████▍   | 194000/300001 [02:26<01:05, 1628.31 examples/s]Running tokenizer on dataset (num_proc=2):  65%|██████▍   | 195000/300001 [02:27<01:09, 1511.51 examples/s]Running tokenizer on dataset (num_proc=2):  65%|██████▌   | 196000/300001 [02:28<01:04, 1619.88 examples/s]Running tokenizer on dataset (num_proc=2):  66%|██████▌   | 197000/300001 [02:29<01:10, 1463.35 examples/s]Running tokenizer on dataset (num_proc=2):  66%|██████▌   | 198000/300001 [02:29<01:01, 1655.18 examples/s]Running tokenizer on dataset (num_proc=2):  66%|██████▋   | 199000/300001 [02:30<01:07, 1505.62 examples/s]Running tokenizer on dataset (num_proc=2):  67%|██████▋   | 200000/300001 [02:30<01:01, 1635.76 examples/s]Running tokenizer on dataset (num_proc=2):  67%|██████▋   | 201000/300001 [02:31<01:06, 1494.71 examples/s]Running tokenizer on dataset (num_proc=2):  67%|██████▋   | 202000/300001 [02:32<00:59, 1644.15 examples/s]Running tokenizer on dataset (num_proc=2):  68%|██████▊   | 203000/300001 [02:32<01:04, 1514.10 examples/s]Running tokenizer on dataset (num_proc=2):  68%|██████▊   | 204000/300001 [02:33<00:58, 1632.09 examples/s]Running tokenizer on dataset (num_proc=2):  68%|██████▊   | 205000/300001 [02:34<01:02, 1524.64 examples/s]Running tokenizer on dataset (num_proc=2):  69%|██████▊   | 206000/300001 [02:34<00:56, 1656.43 examples/s]Running tokenizer on dataset (num_proc=2):  69%|██████▉   | 207000/300001 [02:35<01:01, 1522.51 examples/s]Running tokenizer on dataset (num_proc=2):  69%|██████▉   | 208000/300001 [02:35<00:55, 1656.52 examples/s]Running tokenizer on dataset (num_proc=2):  70%|██████▉   | 209000/300001 [02:36<01:03, 1431.22 examples/s]Running tokenizer on dataset (num_proc=2):  70%|██████▉   | 210000/300001 [02:38<01:20, 1118.72 examples/s]Running tokenizer on dataset (num_proc=2):  70%|███████   | 211000/300001 [02:38<01:03, 1409.69 examples/s]Running tokenizer on dataset (num_proc=2):  71%|███████   | 212000/300001 [02:39<01:18, 1116.27 examples/s]Running tokenizer on dataset (num_proc=2):  71%|███████   | 213000/300001 [02:40<01:26, 1009.96 examples/s]Running tokenizer on dataset (num_proc=2):  71%|███████▏  | 214000/300001 [02:41<01:10, 1217.27 examples/s]Running tokenizer on dataset (num_proc=2):  72%|███████▏  | 215000/300001 [02:42<01:13, 1153.85 examples/s]Running tokenizer on dataset (num_proc=2):  72%|███████▏  | 216000/300001 [02:42<01:04, 1309.02 examples/s]Running tokenizer on dataset (num_proc=2):  72%|███████▏  | 217000/300001 [02:43<01:03, 1317.45 examples/s]Running tokenizer on dataset (num_proc=2):  73%|███████▎  | 218000/300001 [02:44<01:00, 1363.67 examples/s]Running tokenizer on dataset (num_proc=2):  73%|███████▎  | 219000/300001 [02:45<01:06, 1213.32 examples/s]Running tokenizer on dataset (num_proc=2):  73%|███████▎  | 220000/300001 [02:45<00:55, 1454.32 examples/s]Running tokenizer on dataset (num_proc=2):  74%|███████▎  | 221000/300001 [02:46<01:02, 1266.45 examples/s]Running tokenizer on dataset (num_proc=2):  74%|███████▍  | 222000/300001 [02:47<00:53, 1469.39 examples/s]Running tokenizer on dataset (num_proc=2):  74%|███████▍  | 223000/300001 [02:47<00:56, 1359.70 examples/s]Running tokenizer on dataset (num_proc=2):  75%|███████▍  | 224000/300001 [02:48<00:51, 1474.73 examples/s]Running tokenizer on dataset (num_proc=2):  75%|███████▍  | 225000/300001 [02:49<00:53, 1397.76 examples/s]Running tokenizer on dataset (num_proc=2):  75%|███████▌  | 226000/300001 [02:49<00:47, 1553.99 examples/s]Running tokenizer on dataset (num_proc=2):  76%|███████▌  | 227000/300001 [02:50<00:49, 1462.11 examples/s]Running tokenizer on dataset (num_proc=2):  76%|███████▌  | 228000/300001 [02:51<00:45, 1598.26 examples/s]Running tokenizer on dataset (num_proc=2):  76%|███████▋  | 229000/300001 [02:51<00:47, 1498.92 examples/s]Running tokenizer on dataset (num_proc=2):  77%|███████▋  | 230000/300001 [02:52<00:41, 1696.31 examples/s]Running tokenizer on dataset (num_proc=2):  77%|███████▋  | 231000/300001 [02:53<00:47, 1457.53 examples/s]Running tokenizer on dataset (num_proc=2):  77%|███████▋  | 232000/300001 [02:53<00:39, 1723.77 examples/s]Running tokenizer on dataset (num_proc=2):  78%|███████▊  | 233000/300001 [02:54<00:45, 1481.77 examples/s]Running tokenizer on dataset (num_proc=2):  78%|███████▊  | 234000/300001 [02:54<00:37, 1762.33 examples/s]Running tokenizer on dataset (num_proc=2):  78%|███████▊  | 235000/300001 [02:55<00:44, 1477.23 examples/s]Running tokenizer on dataset (num_proc=2):  79%|███████▊  | 236000/300001 [02:55<00:36, 1731.19 examples/s]Running tokenizer on dataset (num_proc=2):  79%|███████▉  | 237000/300001 [02:56<00:41, 1510.02 examples/s]Running tokenizer on dataset (num_proc=2):  79%|███████▉  | 238000/300001 [02:57<00:36, 1719.57 examples/s]Running tokenizer on dataset (num_proc=2):  80%|███████▉  | 239000/300001 [02:58<00:41, 1470.47 examples/s]Running tokenizer on dataset (num_proc=2):  80%|███████▉  | 240000/300001 [02:58<00:34, 1727.74 examples/s]Running tokenizer on dataset (num_proc=2):  80%|████████  | 241000/300001 [02:59<00:40, 1470.60 examples/s]Running tokenizer on dataset (num_proc=2):  81%|████████  | 242000/300001 [02:59<00:33, 1719.58 examples/s]Running tokenizer on dataset (num_proc=2):  81%|████████  | 243000/300001 [03:00<00:38, 1480.90 examples/s]Running tokenizer on dataset (num_proc=2):  81%|████████▏ | 244000/300001 [03:00<00:31, 1771.18 examples/s]Running tokenizer on dataset (num_proc=2):  82%|████████▏ | 245000/300001 [03:01<00:37, 1472.16 examples/s]Running tokenizer on dataset (num_proc=2):  82%|████████▏ | 246000/300001 [03:02<00:32, 1663.15 examples/s]Running tokenizer on dataset (num_proc=2):  82%|████████▏ | 247000/300001 [03:03<00:35, 1478.42 examples/s]Running tokenizer on dataset (num_proc=2):  83%|████████▎ | 248000/300001 [03:03<00:31, 1668.14 examples/s]Running tokenizer on dataset (num_proc=2):  83%|████████▎ | 249000/300001 [03:04<00:33, 1507.82 examples/s]Running tokenizer on dataset (num_proc=2):  83%|████████▎ | 250000/300001 [03:04<00:31, 1596.88 examples/s]Running tokenizer on dataset (num_proc=2):  84%|████████▎ | 251000/300001 [03:05<00:31, 1552.67 examples/s]Running tokenizer on dataset (num_proc=2):  84%|████████▍ | 252000/300001 [03:06<00:31, 1529.14 examples/s]Running tokenizer on dataset (num_proc=2):  84%|████████▍ | 253000/300001 [03:06<00:29, 1579.71 examples/s]Running tokenizer on dataset (num_proc=2):  85%|████████▍ | 254000/300001 [03:07<00:29, 1570.72 examples/s]Running tokenizer on dataset (num_proc=2):  85%|████████▍ | 255000/300001 [03:08<00:27, 1622.83 examples/s]Running tokenizer on dataset (num_proc=2):  85%|████████▌ | 256000/300001 [03:09<00:40, 1073.69 examples/s]Running tokenizer on dataset (num_proc=2):  86%|████████▌ | 258000/300001 [03:11<00:37, 1116.75 examples/s]Running tokenizer on dataset (num_proc=2):  86%|████████▋ | 259000/300001 [03:12<00:33, 1205.97 examples/s]Running tokenizer on dataset (num_proc=2):  87%|████████▋ | 260000/300001 [03:13<00:36, 1109.65 examples/s]Running tokenizer on dataset (num_proc=2):  87%|████████▋ | 261000/300001 [03:13<00:30, 1282.50 examples/s]Running tokenizer on dataset (num_proc=2):  87%|████████▋ | 262000/300001 [03:14<00:32, 1166.12 examples/s]Running tokenizer on dataset (num_proc=2):  88%|████████▊ | 263000/300001 [03:15<00:29, 1268.42 examples/s]Running tokenizer on dataset (num_proc=2):  88%|████████▊ | 264000/300001 [03:16<00:29, 1211.72 examples/s]Running tokenizer on dataset (num_proc=2):  88%|████████▊ | 265000/300001 [03:16<00:27, 1281.99 examples/s]Running tokenizer on dataset (num_proc=2):  89%|████████▊ | 266000/300001 [03:17<00:26, 1277.85 examples/s]Running tokenizer on dataset (num_proc=2):  89%|████████▉ | 267000/300001 [03:18<00:24, 1366.39 examples/s]Running tokenizer on dataset (num_proc=2):  89%|████████▉ | 268000/300001 [03:19<00:24, 1319.77 examples/s]Running tokenizer on dataset (num_proc=2):  90%|████████▉ | 269000/300001 [03:19<00:22, 1400.23 examples/s]Running tokenizer on dataset (num_proc=2):  90%|████████▉ | 270000/300001 [03:20<00:22, 1344.51 examples/s]Running tokenizer on dataset (num_proc=2):  90%|█████████ | 271000/300001 [03:21<00:21, 1323.64 examples/s]Running tokenizer on dataset (num_proc=2):  91%|█████████ | 272000/300001 [03:21<00:18, 1505.81 examples/s]Running tokenizer on dataset (num_proc=2):  91%|█████████ | 273000/300001 [03:22<00:21, 1267.27 examples/s]Running tokenizer on dataset (num_proc=2):  91%|█████████▏| 274000/300001 [03:22<00:15, 1695.53 examples/s]Running tokenizer on dataset (num_proc=2):  92%|█████████▏| 275000/300001 [03:24<00:19, 1309.93 examples/s]Running tokenizer on dataset (num_proc=2):  92%|█████████▏| 277000/300001 [03:25<00:18, 1243.51 examples/s]Running tokenizer on dataset (num_proc=2):  93%|█████████▎| 279000/300001 [03:27<00:17, 1210.23 examples/s]Running tokenizer on dataset (num_proc=2):  94%|█████████▎| 281000/300001 [03:29<00:15, 1252.40 examples/s]Running tokenizer on dataset (num_proc=2):  94%|█████████▍| 282000/300001 [03:29<00:11, 1501.67 examples/s]Running tokenizer on dataset (num_proc=2):  94%|█████████▍| 283000/300001 [03:30<00:13, 1217.02 examples/s]Running tokenizer on dataset (num_proc=2):  95%|█████████▍| 284000/300001 [03:30<00:11, 1449.44 examples/s]Running tokenizer on dataset (num_proc=2):  95%|█████████▍| 285000/300001 [03:32<00:12, 1220.88 examples/s]Running tokenizer on dataset (num_proc=2):  95%|█████████▌| 286000/300001 [03:32<00:09, 1404.59 examples/s]Running tokenizer on dataset (num_proc=2):  96%|█████████▌| 287000/300001 [03:33<00:09, 1337.42 examples/s]Running tokenizer on dataset (num_proc=2):  96%|█████████▌| 288000/300001 [03:34<00:09, 1311.99 examples/s]Running tokenizer on dataset (num_proc=2):  96%|█████████▋| 289000/300001 [03:34<00:07, 1490.03 examples/s]Running tokenizer on dataset (num_proc=2):  97%|█████████▋| 290000/300001 [03:35<00:07, 1286.30 examples/s]Running tokenizer on dataset (num_proc=2):  97%|█████████▋| 291000/300001 [03:36<00:06, 1456.38 examples/s]Running tokenizer on dataset (num_proc=2):  97%|█████████▋| 292000/300001 [03:37<00:06, 1249.91 examples/s]Running tokenizer on dataset (num_proc=2):  98%|█████████▊| 293000/300001 [03:37<00:05, 1322.65 examples/s]Running tokenizer on dataset (num_proc=2):  98%|█████████▊| 294000/300001 [03:38<00:04, 1254.53 examples/s]Running tokenizer on dataset (num_proc=2):  98%|█████████▊| 295000/300001 [03:39<00:03, 1275.00 examples/s]Running tokenizer on dataset (num_proc=2):  99%|█████████▊| 296000/300001 [03:40<00:03, 1313.52 examples/s]Running tokenizer on dataset (num_proc=2):  99%|█████████▉| 297000/300001 [03:41<00:02, 1246.27 examples/s]Running tokenizer on dataset (num_proc=2):  99%|█████████▉| 298001/300001 [03:41<00:01, 1273.04 examples/s]Running tokenizer on dataset (num_proc=2): 100%|█████████▉| 299001/300001 [03:43<00:01, 984.54 examples/s] Running tokenizer on dataset (num_proc=2): 100%|██████████| 300001/300001 [03:44<00:00, 879.65 examples/s]Running tokenizer on dataset (num_proc=2): 100%|██████████| 300001/300001 [03:45<00:00, 1333.17 examples/s]
192.168.0.149: training example:
192.168.0.149: input_ids:
192.168.0.149: [791, 42500, 13935, 8668, 315, 264, 13128, 374, 25, 386, 650, 735, 473, 735, 350, 358, 432, 452, 435, 362, 362, 328, 432, 445, 358, 362, 362, 350, 350, 435, 445, 328, 362, 650, 816, 445, 393, 362, 362, 435, 362, 1229, 350, 358, 650, 452, 452, 358, 816, 328, 469, 735, 480, 445, 362, 480, 350, 386, 328, 1229, 350, 1229, 445, 423, 350, 452, 350, 816, 350, 328, 1229, 362, 432, 435, 480, 468, 452, 452, 432, 432, 358, 650, 358, 423, 469, 735, 445, 735, 445, 423, 423, 735, 480, 432, 358, 445, 480, 435, 469, 362, 1229, 480, 1229, 328, 362, 435, 480, 362, 423, 358, 328, 469, 328, 816, 386, 468, 469, 452, 480, 1229, 362, 735, 468, 1229, 452, 423, 423, 469, 473, 328, 328, 650, 452, 350, 452, 1229, 393, 350, 435, 816, 650, 393, 735, 452, 328, 350, 445, 362, 358, 423, 452, 362, 445, 358, 432, 816, 445, 358, 735, 452, 423, 445, 469, 1229, 358, 735, 650, 445, 393, 473, 480, 735, 650, 350, 435, 350, 735, 445, 423, 350, 650, 735, 445, 480, 423, 469, 1229, 650, 816, 445, 816, 362, 735, 328, 480, 445, 350, 386, 350, 393, 423, 435, 362, 468, 816, 423, 735, 469, 480, 452, 816, 435, 362, 1229, 328, 468, 480, 480, 386, 816, 650, 358, 432, 423, 480, 435, 328, 735, 469, 1229, 435, 423, 1229, 445, 735, 650, 432, 1229, 445, 735, 362, 469, 1229, 362, 816, 445, 469, 452, 445, 362, 469, 1229, 445, 350, 469, 816, 473, 328, 362, 445, 445, 445, 423, 1229, 650, 452, 650, 435, 423, 650, 469, 735, 480, 350, 350, 445, 350, 480, 1229, 1229, 650, 445, 358, 469, 1229, 480, 735, 358, 650, 1229, 650, 362, 393, 735, 358, 469, 445, 480, 735, 735, 650, 362, 350, 650, 452, 480, 1229, 480, 735, 350, 445, 358, 393, 480, 445, 468, 423, 386, 473, 480, 473, 445, 328, 735, 423, 350, 480, 358, 445, 452, 358, 362, 350, 480, 650, 350, 328, 650, 432, 423, 386, 480, 452, 469, 473, 1229, 452, 445, 386, 469, 358, 1229, 362, 445, 435, 423, 350, 480, 735, 650, 445, 480, 350, 432, 650, 816, 432, 362, 480, 435, 386, 423, 735, 816, 328, 469, 452, 328, 362, 480, 445, 328, 650, 735, 350, 445, 469, 469, 362, 445, 469, 386, 650, 423, 435, 435, 362, 423, 452, 480, 816, 650, 1229, 358, 735, 445, 816, 328, 328, 358, 423, 393, 735, 468, 650, 469, 393, 358, 362, 1229, 432, 362, 473, 328, 432, 480, 445, 432, 445, 328, 480, 473, 358, 393, 362, 435, 386, 350, 362, 469, 1229, 362, 650, 473, 362, 480, 816, 452, 469, 358, 1229, 473, 358, 452, 386, 445, 435, 445, 452, 435, 445, 362, 480, 469, 469, 650, 423, 350, 432, 350, 735, 1229, 432, 435, 328, 445, 358, 480, 469, 735, 362, 362, 469, 386, 393, 386, 350, 480, 735, 469, 386, 423, 362, 435, 358, 735, 445, 445, 362, 452, 452, 452, 650, 650, 358, 423, 393, 350, 650, 328, 350, 435, 432, 328, 445, 445, 386, 328, 1229, 452, 735, 1229, 650, 423, 1229, 469, 435, 362, 469, 358, 362, 328, 473, 445, 393, 393, 452, 435, 650, 432, 452, 445, 735, 480, 362, 1229, 386, 1229, 650, 469, 469, 469, 473, 1229, 328, 362, 816, 1229, 452, 328, 480, 423, 362, 445, 445, 735, 386, 650, 735, 358, 445, 816, 423, 362, 480, 650, 393, 386, 650, 362, 480, 350, 423, 328, 650, 393, 480, 435, 350, 445, 445, 432, 469, 445, 469, 445, 816, 350, 386, 362, 480, 358, 393, 350, 350, 469, 650, 445, 735, 386, 362, 350, 358, 423, 328, 362, 432, 445, 386, 480, 650, 362, 473, 1229, 350, 480, 328, 358, 328, 469, 480, 735, 650, 362, 423, 445, 358, 445, 650, 423, 480, 423, 393, 328, 735, 423, 358, 735, 362, 445, 432, 735, 445, 328, 445, 650, 358, 735, 480, 350, 1229, 650, 435, 735, 393, 469, 362, 358, 435, 469, 1229, 358, 480, 358, 362, 393, 435, 350, 735, 362, 328, 432, 358, 650, 445, 1246, 2116, 420, 8668, 11, 279, 6070, 315, 279, 13128, 374, 25, 128750, 128357, 128268, 128672, 128657, 128516, 128335, 128388, 128296, 128335, 128407, 128516, 128497, 128334, 128350, 128334, 128407, 128649, 128661, 128731, 128302, 128335, 128535, 128585, 128302, 128727, 128648, 128756, 128651, 128725, 128270, 128732, 128547, 128343, 128378, 128585, 128513, 128476, 128315, 128487, 128330, 128566, 128510, 128417, 128417, 128520, 128732, 128321, 128433, 128435, 128419, 128386, 128603, 128516, 128366, 128417, 128346, 128441, 128549, 128493, 128344, 128493, 128315, 128581, 128288, 128651, 128343, 128432, 128487, 128746, 128585, 128369, 128651, 128504, 128651, 128592, 128272, 128625, 128531, 128306, 128730, 128366, 128420, 128526, 128446, 128593, 128517, 128420, 128520, 128443, 128334, 128518, 128581, 128347, 128514, 128739, 128593, 128726, 128663, 128586, 128625, 128328, 128695, 128571, 128263, 128739, 128572, 128301, 128366, 128371, 128483, 128432, 128429, 128268, 128671, 128502, 128562, 128330, 128602, 128431, 128430, 128326, 128622, 128713, 128678, 128352, 128742, 128478, 128340, 128584, 128675, 128685, 128476, 128297, 128368, 128588, 128723, 128336, 128272, 128727, 128592, 128258, 128408, 128396, 128369, 128268, 128321, 128705, 128424, 128268, 128321, 128423, 128547, 128624, 128308, 128417, 128261, 128520, 128343, 128543, 128580, 128656, 128411, 128368, 128486, 128334, 128257, 128343, 128569, 128576, 128649, 128303, 128514, 128305, 128413, 128476, 128343, 128354, 128475, 128547, 128512, 128547, 128648, 128493, 128462, 128483, 128256, 128585, 128294, 128398, 128575, 128365, 128331, 128623, 128649, 128593, 128283, 128349, 128281, 128575, 128289, 128582, 128487, 128493, 128524, 128298, 128756, 128706, 128349, 128732, 128649, 128330, 128458, 128305, 128516, 128288, 128514, 128727, 128403, 128702, 128487, 128268, 128353, 128487, 128265, 128286, 128333, 128298, 128366, 128330, 128587, 128298, 128268, 128353, 128518, 128578, 128310, 128620, 128547, 128435, 128592, 128514, 128614, 128432, 128696, 128648, 128365, 128462, 128448, 128360, 128315, 128346, 128306, 128629, 128577, 128305, 128340, 128404, 128462, 128690, 128593, 128593, 128517, 128497, 128285, 128438, 128384, 128310, 128331, 128677, 128492, 128393, 128759, 128305, 128346, 128270, 128667, 128539, 128592, 128364, 128441, 128392, 128446, 128267, 128610, 128603, 128274, 128472, 128694, 128524, 128590, 128469, 128381, 128718, 128525, 128401, 128678, 128635, 128358, 128323, 128483, 128297, 128575, 128444, 128471, 128387, 128483, 128746, 128317, 128487, 128290, 128366, 128629, 128381, 128443, 128547, 128435, 128720, 128547, 128466, 128484, 128693, 128524, 128765, 128271, 128446, 128486]
192.168.0.149: inputs:
192.168.0.149: The amino acid sequence of a protein is: M V K H K T I R N F A A S R L I A A T T F L S A V Y L P A A F A Q T I V N N I Y S E K G L A G T M S Q T Q L D T N T Y T S Q A R F G W N N R R I V I D E K L K L D D K G R I L G F E A Q G Q S A F G A D I S E S Y M W E N G Q A K W Q N D D E H S S V N T N Q P T F Y V P K N S T L A I D N A L I R Y L I K N D L E Q I K V L P H G K V T F T K L D T V K L G D E Q V Y L Y A K S G L T M T P D F A W Y D K E G N Y F A Q S W G G M Y V I R D G F S K E Q F D Q L K V R Q L K A E Q A Y L E N L A E Q L T E Y H S A L L L D Q V N V F D V E K G T T L T G Q Q V L I E Q G K I V Q V A P K I E L G K K V A T V N G Q G K T L I P G L W D M H G H L S K D T G I L N I A T G V T S V R D M G N E H Q N L M E I Q A L F D T G K V L G T R V Y R A G F M D K Y S E N S A G L S V K T L E E A L E M V D F F A D N G Y V Q I K L Y S S I D P K W V E P I A Q R A H S R G L R L S G H I P A F M T A E Q A V H A G Y N E I Q H I N M L F L N F L A G E E V D T R T K Q R F S L I G E K A A E M P M T G K E M D A F I K L L A N N N V V I D P T V S T F R S L L M S Q N K Q V D Q E F A E I A S H L P P N F V R N L K G A Q M Q V E E E H Q S A Y Q N S G D A L L K M V K I L Y D A G V P M V A G T D S V P G F T L L R E L E L Y T M A G I P T T E V L K M A T I D S A R L M G V A H Q T G S I S E G K V A D L I L V D G D P S K D I K A L R K L S L V I K G T Q V F K P E A I F E Q I G I A P F T K A S R I V L.Given this sequence, the structure of the protein is:珖屼袱哻豊筇臧悕傱臧穉筇爲陼玭陼穉栲蒀篨岜臧潋愲岜薠玝棷伌芎瑷阌篣茖諒愲潼矻阇槠恮瘈埘粺粺蕲阌柎稽斾苁魼訞筇咍粺嫺獐昵咥嗋咥阇睔罯伌茖醜槠蔴愲軵伌栶伌鞻稰呪墌謜厓咍鲕滓碟澶鳊鲕蕲鹣陼鵙睔娓濩鹽澶詐匜簰呪鳗韆扞鸓鹽娙濊咍阎馑醜墡袱韪儶焄恮檈鲵犫羝瑰骬饹曋厖躈睭蟺譺鵸矻臡躄麃醿圉稰薠鞻僭慞仂軵袱柎泿殛袱柎璹篣褴岫粺菙蕲茖繖綵愀擥躄穠陼伣茖酎革栲庈濩韣翕矻茖戗虩篣炤篣玝咥辋馑緭愲铏狾裲難寤雊栲澶惔撙瀷裲蟪疿槠咥闚牦棷絡撙阌栲恮漰韣筇罯濩薠篥秺槠袱鎉槠褓銲鬋牦咍恮饠牦袱鎉鵙冔魳鉾篣斾鞻濩姏醜樐玝難辋酬筌阇嫺謜雿磹韣睭遒辋箧澶澶鳊爲趯橅萹魳寤傿劙璲驞韣嫺瑷镏耒鞻镕獐磼碟蝯羆訞枌褕刼闚簪肜岭荓雟敍饹韎聱匫馑臡裲蛖茤稒馑蔴貣槠缌咍雿岭鹣篣斾龃篣籝檗扢闚錜忲碟穠
192.168.0.13: [INFO|configuration_utils.py:731] 2024-09-29 11:30:59,320 >> loading configuration file /root/fcl/Meta-Llama-3-8B_512_tokenizer/config.json
192.168.0.149: [INFO|configuration_utils.py:731] 2024-09-29 11:31:01,901 >> loading configuration file /root/fcl/Meta-Llama-3-8B_512_tokenizer/config.json
192.168.0.25: [INFO|configuration_utils.py:731] 2024-09-29 11:30:58,878 >> loading configuration file /root/fcl/Meta-Llama-3-8B_512_tokenizer/config.json
192.168.0.89: [INFO|configuration_utils.py:731] 2024-09-29 11:30:43,244 >> loading configuration file /root/fcl/Meta-Llama-3-8B_512_tokenizer/config.json
192.168.0.13: [INFO|configuration_utils.py:800] 2024-09-29 11:30:59,322 >> Model config LlamaConfig {
192.168.0.13:   "_name_or_path": "/root/fcl/Meta-Llama-3-8B_512_tokenizer",
192.168.0.13:   "architectures": [
192.168.0.13:     "LlamaForCausalLM"
192.168.0.13:   ],
192.168.0.13:   "attention_bias": false,
192.168.0.13:   "attention_dropout": 0.0,
192.168.0.13:   "bos_token_id": 128000,
192.168.0.13:   "eos_token_id": 128001,
192.168.0.13:   "hidden_act": "silu",
192.168.0.13:   "hidden_size": 4096,
192.168.0.13:   "initializer_range": 0.02,
192.168.0.13:   "intermediate_size": 14336,
192.168.0.13:   "max_position_embeddings": 8192,
192.168.0.13:   "mlp_bias": false,
192.168.0.13:   "model_type": "llama",
192.168.0.13:   "num_attention_heads": 32,
192.168.0.13:   "num_hidden_layers": 32,
192.168.0.13:   "num_key_value_heads": 8,
192.168.0.13:   "pretraining_tp": 1,
192.168.0.13:   "rms_norm_eps": 1e-05,
192.168.0.13:   "rope_scaling": null,
192.168.0.13:   "rope_theta": 500000.0,
192.168.0.13:   "tie_word_embeddings": false,
192.168.0.13:   "torch_dtype": "bfloat16",
192.168.0.13:   "transformers_version": "4.44.2",
192.168.0.13:   "use_cache": true,
192.168.0.13:   "vocab_size": 128256
192.168.0.13: }
192.168.0.13: 
192.168.0.149: [INFO|configuration_utils.py:800] 2024-09-29 11:31:01,904 >> Model config LlamaConfig {
192.168.0.25: [INFO|configuration_utils.py:800] 2024-09-29 11:30:58,881 >> Model config LlamaConfig {
192.168.0.25:   "_name_or_path": "/root/fcl/Meta-Llama-3-8B_512_tokenizer",
192.168.0.25:   "architectures": [
192.168.0.149:   "_name_or_path": "/root/fcl/Meta-Llama-3-8B_512_tokenizer",
192.168.0.149:   "architectures": [
192.168.0.25:     "LlamaForCausalLM"
192.168.0.25:   ],
192.168.0.25:   "attention_bias": false,
192.168.0.149:     "LlamaForCausalLM"
192.168.0.149:   ],
192.168.0.25:   "attention_dropout": 0.0,
192.168.0.89: [INFO|configuration_utils.py:800] 2024-09-29 11:30:43,247 >> Model config LlamaConfig {
192.168.0.149:   "attention_bias": false,
192.168.0.25:   "bos_token_id": 128000,
192.168.0.89:   "_name_or_path": "/root/fcl/Meta-Llama-3-8B_512_tokenizer",
192.168.0.149:   "attention_dropout": 0.0,
192.168.0.25:   "eos_token_id": 128001,
192.168.0.89:   "architectures": [
192.168.0.149:   "bos_token_id": 128000,
192.168.0.25:   "hidden_act": "silu",
192.168.0.89:     "LlamaForCausalLM"
192.168.0.149:   "eos_token_id": 128001,
192.168.0.25:   "hidden_size": 4096,
192.168.0.89:   ],
192.168.0.149:   "hidden_act": "silu",
192.168.0.25:   "initializer_range": 0.02,
192.168.0.89:   "attention_bias": false,
192.168.0.149:   "hidden_size": 4096,
192.168.0.25:   "intermediate_size": 14336,
192.168.0.89:   "attention_dropout": 0.0,
192.168.0.149:   "initializer_range": 0.02,
192.168.0.25:   "max_position_embeddings": 8192,
192.168.0.89:   "bos_token_id": 128000,
192.168.0.149:   "intermediate_size": 14336,
192.168.0.25:   "mlp_bias": false,
192.168.0.89:   "eos_token_id": 128001,
192.168.0.149:   "max_position_embeddings": 8192,
192.168.0.25:   "model_type": "llama",
192.168.0.89:   "hidden_act": "silu",
192.168.0.149:   "mlp_bias": false,
192.168.0.25:   "num_attention_heads": 32,
192.168.0.89:   "hidden_size": 4096,
192.168.0.149:   "model_type": "llama",
192.168.0.25:   "num_hidden_layers": 32,
192.168.0.89:   "initializer_range": 0.02,
192.168.0.149:   "num_attention_heads": 32,
192.168.0.25:   "num_key_value_heads": 8,
192.168.0.89:   "intermediate_size": 14336,
192.168.0.149:   "num_hidden_layers": 32,
192.168.0.25:   "pretraining_tp": 1,
192.168.0.89:   "max_position_embeddings": 8192,
192.168.0.149:   "num_key_value_heads": 8,
192.168.0.25:   "rms_norm_eps": 1e-05,
192.168.0.89:   "mlp_bias": false,
192.168.0.149:   "pretraining_tp": 1,
192.168.0.25:   "rope_scaling": null,
192.168.0.89:   "model_type": "llama",
192.168.0.149:   "rms_norm_eps": 1e-05,
192.168.0.25:   "rope_theta": 500000.0,
192.168.0.89:   "num_attention_heads": 32,
192.168.0.149:   "rope_scaling": null,
192.168.0.25:   "tie_word_embeddings": false,
192.168.0.89:   "num_hidden_layers": 32,
192.168.0.149:   "rope_theta": 500000.0,
192.168.0.25:   "torch_dtype": "bfloat16",
192.168.0.89:   "num_key_value_heads": 8,
192.168.0.149:   "tie_word_embeddings": false,
192.168.0.25:   "transformers_version": "4.44.2",
192.168.0.89:   "pretraining_tp": 1,
192.168.0.149:   "torch_dtype": "bfloat16",
192.168.0.25:   "use_cache": true,
192.168.0.89:   "rms_norm_eps": 1e-05,
192.168.0.149:   "transformers_version": "4.44.2",
192.168.0.25:   "vocab_size": 128256
192.168.0.89:   "rope_scaling": null,
192.168.0.149:   "use_cache": true,
192.168.0.25: }
192.168.0.89:   "rope_theta": 500000.0,
192.168.0.149:   "vocab_size": 128256
192.168.0.25: 
192.168.0.89:   "tie_word_embeddings": false,
192.168.0.149: }
192.168.0.89:   "torch_dtype": "bfloat16",
192.168.0.149: 
192.168.0.89:   "transformers_version": "4.44.2",
192.168.0.89:   "use_cache": true,
192.168.0.89:   "vocab_size": 128256
192.168.0.89: }
192.168.0.89: 
192.168.0.25: [INFO|modeling_utils.py:3675] 2024-09-29 11:30:58,948 >> loading weights file /root/fcl/Meta-Llama-3-8B_512_tokenizer/model.safetensors.index.json
192.168.0.25: [INFO|modeling_utils.py:3820] 2024-09-29 11:30:58,948 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
192.168.0.13: [INFO|modeling_utils.py:3675] 2024-09-29 11:30:59,393 >> loading weights file /root/fcl/Meta-Llama-3-8B_512_tokenizer/model.safetensors.index.json
192.168.0.149: [INFO|modeling_utils.py:3675] 2024-09-29 11:31:01,975 >> loading weights file /root/fcl/Meta-Llama-3-8B_512_tokenizer/model.safetensors.index.json
192.168.0.13: [INFO|modeling_utils.py:3820] 2024-09-29 11:30:59,394 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
192.168.0.149: [INFO|modeling_utils.py:3820] 2024-09-29 11:31:01,976 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
192.168.0.89: [INFO|modeling_utils.py:3675] 2024-09-29 11:30:43,323 >> loading weights file /root/fcl/Meta-Llama-3-8B_512_tokenizer/model.safetensors.index.json
192.168.0.89: [INFO|modeling_utils.py:3820] 2024-09-29 11:30:43,323 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
192.168.0.13: [INFO|configuration_utils.py:1038] 2024-09-29 11:30:59,419 >> Generate config GenerationConfig {
192.168.0.13:   "bos_token_id": 128000,
192.168.0.13:   "eos_token_id": 128001
192.168.0.13: }
192.168.0.13: 
192.168.0.25: [INFO|configuration_utils.py:1038] 2024-09-29 11:30:58,977 >> Generate config GenerationConfig {
192.168.0.25:   "bos_token_id": 128000,
192.168.0.25:   "eos_token_id": 128001
192.168.0.25: }
192.168.0.25: 
192.168.0.149: [INFO|configuration_utils.py:1038] 2024-09-29 11:31:02,001 >> Generate config GenerationConfig {
192.168.0.149:   "bos_token_id": 128000,
192.168.0.149:   "eos_token_id": 128001
192.168.0.149: }
192.168.0.149: 
192.168.0.89: [INFO|configuration_utils.py:1038] 2024-09-29 11:30:43,347 >> Generate config GenerationConfig {
192.168.0.89:   "bos_token_id": 128000,
192.168.0.89:   "eos_token_id": 128001
192.168.0.89: }
192.168.0.89: 
192.168.0.25: [2024-09-29 11:30:59,417] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 291, num_elems = 8.03B
192.168.0.89: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.37s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.45s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.16s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.86s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.94s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.06s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.59s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.95s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.36s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:06,  3.14s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:07,  3.57s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.50s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.48s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.40s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.32s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:07,  3.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.50s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.47s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.36s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.62s/it]
192.168.0.89: Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.29s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.58s/it]
192.168.0.13: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.54s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.55s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.50s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.88s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.37s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.89s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.16s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.11s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.34s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.30s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.35s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.48s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.46s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:07,  3.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:07,  3.63s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.49s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.45s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.79s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.73s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.81s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.29s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.62s/it]
192.168.0.89: Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.78s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.84s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.81s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.31s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.68s/it]
192.168.0.25: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.44s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.80s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.02s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.06s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.02s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.05s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.17s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:17,  5.95s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.38s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.47s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:07,  3.50s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:07,  3.63s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:07,  3.70s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:07,  3.58s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:07,  3.73s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.82s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.57s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.77s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.84s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.42s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.70s/it]
192.168.0.149: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.46s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.76s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.73s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.93s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.71s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.02s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.12s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.51s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.23s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.41s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.40s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:07,  3.57s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.46s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.45s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.42s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:07,  3.55s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.58s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.76s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.81s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.81s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.37s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.66s/it]
192.168.0.25: Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.40s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.72s/it]
192.168.0.89: Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.41s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.70s/it]
192.168.0.149: Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.45s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.72s/it]
192.168.0.13: Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.77s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.73s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.44s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.70s/it]
192.168.0.149: Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.46s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.74s/it]
192.168.0.25: Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.51s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.81s/it]
192.168.0.25: Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.53s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.83s/it]
192.168.0.13: Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.52s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.78s/it]
192.168.0.89: Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.89s/it]
192.168.0.89: [INFO|modeling_utils.py:4507] 2024-09-29 11:30:55,785 >> All model checkpoint weights were used when initializing LlamaForCausalLM.
192.168.0.89: 
192.168.0.25: Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.53s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.82s/it]
192.168.0.89: [INFO|modeling_utils.py:4515] 2024-09-29 11:30:55,786 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /root/fcl/Meta-Llama-3-8B_512_tokenizer.
192.168.0.89: If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
192.168.0.89: [INFO|configuration_utils.py:991] 2024-09-29 11:30:55,794 >> loading configuration file /root/fcl/Meta-Llama-3-8B_512_tokenizer/generation_config.json
192.168.0.89: [INFO|configuration_utils.py:1038] 2024-09-29 11:30:55,795 >> Generate config GenerationConfig {
192.168.0.89:   "bos_token_id": 128000,
192.168.0.89:   "do_sample": true,
192.168.0.89:   "eos_token_id": 128001,
192.168.0.89:   "max_length": 4096,
192.168.0.89:   "temperature": 0.6,
192.168.0.89:   "top_p": 0.9
192.168.0.89: }
192.168.0.89: 
192.168.0.149: Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.91s/it]
192.168.0.149: [INFO|modeling_utils.py:4507] 2024-09-29 11:31:14,483 >> All model checkpoint weights were used when initializing LlamaForCausalLM.
192.168.0.149: 
192.168.0.149: [INFO|modeling_utils.py:4515] 2024-09-29 11:31:14,483 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /root/fcl/Meta-Llama-3-8B_512_tokenizer.
192.168.0.149: If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
192.168.0.89: Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.55s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.83s/it]
192.168.0.149: [INFO|configuration_utils.py:991] 2024-09-29 11:31:14,491 >> loading configuration file /root/fcl/Meta-Llama-3-8B_512_tokenizer/generation_config.json
192.168.0.149: [INFO|configuration_utils.py:1038] 2024-09-29 11:31:14,492 >> Generate config GenerationConfig {
192.168.0.149:   "bos_token_id": 128000,
192.168.0.149:   "do_sample": true,
192.168.0.149:   "eos_token_id": 128001,
192.168.0.149:   "max_length": 4096,
192.168.0.149:   "temperature": 0.6,
192.168.0.149:   "top_p": 0.9
192.168.0.149: }
192.168.0.149: 
192.168.0.25: Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.88s/it]
192.168.0.149: Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.84s/it]
192.168.0.89: Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.57s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.85s/it]
192.168.0.13: Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.95s/it]
192.168.0.13: [INFO|modeling_utils.py:4507] 2024-09-29 11:31:12,027 >> All model checkpoint weights were used when initializing LlamaForCausalLM.
192.168.0.13: 
192.168.0.13: [INFO|modeling_utils.py:4515] 2024-09-29 11:31:12,027 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /root/fcl/Meta-Llama-3-8B_512_tokenizer.
192.168.0.13: If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
192.168.0.25: Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.85s/it]
192.168.0.13: Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.84s/it]
192.168.0.13: [INFO|configuration_utils.py:991] 2024-09-29 11:31:12,035 >> loading configuration file /root/fcl/Meta-Llama-3-8B_512_tokenizer/generation_config.json
192.168.0.13: [INFO|configuration_utils.py:1038] 2024-09-29 11:31:12,036 >> Generate config GenerationConfig {
192.168.0.13:   "bos_token_id": 128000,
192.168.0.13:   "do_sample": true,
192.168.0.13:   "eos_token_id": 128001,
192.168.0.13:   "max_length": 4096,
192.168.0.13:   "temperature": 0.6,
192.168.0.13:   "top_p": 0.9
192.168.0.13: }
192.168.0.13: 
192.168.0.89: Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.66s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.89s/it]
192.168.0.13: Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.85s/it]
192.168.0.13: Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.67s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.89s/it]
192.168.0.149: Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.69s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.88s/it]
192.168.0.149: Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.83s/it]
192.168.0.13: Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.87s/it]
192.168.0.149: Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.69s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.87s/it]
192.168.0.25: Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.84s/it]
192.168.0.25: [INFO|modeling_utils.py:4507] 2024-09-29 11:31:14,879 >> All model checkpoint weights were used when initializing LlamaForCausalLM.
192.168.0.25: 
192.168.0.25: [INFO|modeling_utils.py:4515] 2024-09-29 11:31:14,879 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /root/fcl/Meta-Llama-3-8B_512_tokenizer.
192.168.0.25: If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
192.168.0.25: [INFO|configuration_utils.py:991] 2024-09-29 11:31:14,886 >> loading configuration file /root/fcl/Meta-Llama-3-8B_512_tokenizer/generation_config.json
192.168.0.25: [INFO|configuration_utils.py:1038] 2024-09-29 11:31:14,887 >> Generate config GenerationConfig {
192.168.0.25:   "bos_token_id": 128000,
192.168.0.25:   "do_sample": true,
192.168.0.25:   "eos_token_id": 128001,
192.168.0.25:   "max_length": 4096,
192.168.0.25:   "temperature": 0.6,
192.168.0.25:   "top_p": 0.9
192.168.0.25: }
192.168.0.25: 
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.model_utils.embedding - Resized token embeddings from 128256 to 128768.
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.model_utils.embedding - Resized token embeddings from 128256 to 128768.
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.model_utils.embedding - Resized token embeddings from 128256 to 128768.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.model_utils.embedding - Resized token embeddings from 128256 to 128768.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.model_utils.embedding - Resized token embeddings from 128256 to 128768.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.model_utils.embedding - Resized token embeddings from 128256 to 128768.
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.model_utils.embedding - Resized token embeddings from 128256 to 128768.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.model_utils.embedding - Resized token embeddings from 128256 to 128768.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.model_utils.embedding - Resized token embeddings from 128256 to 128768.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.model_utils.embedding - Resized token embeddings from 128256 to 128768.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.model_utils.embedding - Resized token embeddings from 128256 to 128768.
192.168.0.89: 09/29/2024 11:31:57 - INFO - llamafactory.model.model_utils.embedding - Resized token embeddings from 128256 to 128768.
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.model_utils.embedding - Resized token embeddings from 128256 to 128768.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.model_utils.embedding - Resized token embeddings from 128256 to 128768.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.model_utils.embedding - Resized token embeddings from 128256 to 128768.
192.168.0.89: 09/29/2024 11:31:57 - INFO - llamafactory.model.model_utils.embedding - Resized token embeddings from 128256 to 128768.
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.model_utils.embedding - Resized token embeddings from 128256 to 128768.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
192.168.0.89: 09/29/2024 11:31:57 - INFO - llamafactory.model.model_utils.embedding - Resized token embeddings from 128256 to 128768.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.adapter - Fine-tuning method: Full
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.adapter - Fine-tuning method: Full
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.adapter - Fine-tuning method: Full
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
192.168.0.89: 09/29/2024 11:31:57 - INFO - llamafactory.model.model_utils.embedding - Resized token embeddings from 128256 to 128768.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.model_utils.embedding - Resized token embeddings from 128256 to 128768.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.adapter - Fine-tuning method: Full
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.model_utils.embedding - Resized token embeddings from 128256 to 128768.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.adapter - Fine-tuning method: Full
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.adapter - Fine-tuning method: Full
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.adapter - Fine-tuning method: Full
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.model_utils.embedding - Resized token embeddings from 128256 to 128768.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.model_utils.embedding - Resized token embeddings from 128256 to 128768.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
192.168.0.89: 09/29/2024 11:31:57 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
192.168.0.89: 09/29/2024 11:31:57 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.adapter - Fine-tuning method: Full
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.adapter - Fine-tuning method: Full
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.adapter - Fine-tuning method: Full
192.168.0.89: 09/29/2024 11:31:57 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
192.168.0.89: 09/29/2024 11:31:57 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
192.168.0.89: 09/29/2024 11:31:57 - INFO - llamafactory.model.adapter - Fine-tuning method: Full
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.adapter - Fine-tuning method: Full
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
192.168.0.89: 09/29/2024 11:31:57 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.adapter - Fine-tuning method: Full
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.adapter - Fine-tuning method: Full
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.model_utils.embedding - Resized token embeddings from 128256 to 128768.
192.168.0.89: 09/29/2024 11:31:57 - INFO - llamafactory.model.model_utils.embedding - Resized token embeddings from 128256 to 128768.
192.168.0.89: 09/29/2024 11:31:57 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
192.168.0.89: 09/29/2024 11:31:57 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
192.168.0.89: 09/29/2024 11:31:57 - INFO - llamafactory.model.adapter - Fine-tuning method: Full
192.168.0.89: 09/29/2024 11:31:57 - INFO - llamafactory.model.model_utils.embedding - Resized token embeddings from 128256 to 128768.
192.168.0.89: 09/29/2024 11:31:57 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
192.168.0.89: 09/29/2024 11:31:57 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
192.168.0.89: 09/29/2024 11:31:57 - INFO - llamafactory.model.model_utils.embedding - Resized token embeddings from 128256 to 128768.
192.168.0.89: 09/29/2024 11:31:57 - INFO - llamafactory.model.adapter - Fine-tuning method: Full
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.model_utils.embedding - Resized token embeddings from 128256 to 128768.
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.adapter - Fine-tuning method: Full
192.168.0.89: 09/29/2024 11:31:57 - INFO - llamafactory.model.model_utils.embedding - Resized token embeddings from 128256 to 128768.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.model_utils.embedding - Resized token embeddings from 128256 to 128768.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.loader - trainable params: 8,034,455,552 || all params: 8,034,455,552 || trainable%: 100.0000
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.loader - trainable params: 8,034,455,552 || all params: 8,034,455,552 || trainable%: 100.0000
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.loader - trainable params: 8,034,455,552 || all params: 8,034,455,552 || trainable%: 100.0000
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.model_utils.embedding - Resized token embeddings from 128256 to 128768.
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.adapter - Fine-tuning method: Full
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.loader - trainable params: 8,034,455,552 || all params: 8,034,455,552 || trainable%: 100.0000
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.adapter - Fine-tuning method: Full
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.adapter - Fine-tuning method: Full
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.loader - trainable params: 8,034,455,552 || all params: 8,034,455,552 || trainable%: 100.0000
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
192.168.0.89: 09/29/2024 11:31:57 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
192.168.0.89: 09/29/2024 11:31:57 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
192.168.0.89: 09/29/2024 11:31:57 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
192.168.0.89: 09/29/2024 11:31:57 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
192.168.0.89: 09/29/2024 11:31:57 - INFO - llamafactory.model.adapter - Fine-tuning method: Full
192.168.0.89: 09/29/2024 11:31:57 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.adapter - Fine-tuning method: Full
192.168.0.89: 09/29/2024 11:31:57 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
192.168.0.89: 09/29/2024 11:31:57 - INFO - llamafactory.model.adapter - Fine-tuning method: Full
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.loader - trainable params: 8,034,455,552 || all params: 8,034,455,552 || trainable%: 100.0000
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
192.168.0.89: 09/29/2024 11:31:57 - INFO - llamafactory.model.loader - trainable params: 8,034,455,552 || all params: 8,034,455,552 || trainable%: 100.0000
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.loader - trainable params: 8,034,455,552 || all params: 8,034,455,552 || trainable%: 100.0000
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.loader - trainable params: 8,034,455,552 || all params: 8,034,455,552 || trainable%: 100.0000
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.loader - trainable params: 8,034,455,552 || all params: 8,034,455,552 || trainable%: 100.0000
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.adapter - Fine-tuning method: Full
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.loader - trainable params: 8,034,455,552 || all params: 8,034,455,552 || trainable%: 100.0000
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.adapter - Fine-tuning method: Full
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.adapter - Fine-tuning method: Full
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.loader - trainable params: 8,034,455,552 || all params: 8,034,455,552 || trainable%: 100.0000
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.model_utils.embedding - Resized token embeddings from 128256 to 128768.
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.loader - trainable params: 8,034,455,552 || all params: 8,034,455,552 || trainable%: 100.0000
192.168.0.89: 09/29/2024 11:31:58 - INFO - llamafactory.model.loader - trainable params: 8,034,455,552 || all params: 8,034,455,552 || trainable%: 100.0000
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.loader - trainable params: 8,034,455,552 || all params: 8,034,455,552 || trainable%: 100.0000
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.adapter - Fine-tuning method: Full
192.168.0.89: 09/29/2024 11:31:58 - INFO - llamafactory.model.loader - trainable params: 8,034,455,552 || all params: 8,034,455,552 || trainable%: 100.0000
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.loader - trainable params: 8,034,455,552 || all params: 8,034,455,552 || trainable%: 100.0000
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.adapter - Fine-tuning method: Full
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.loader - trainable params: 8,034,455,552 || all params: 8,034,455,552 || trainable%: 100.0000
192.168.0.89: 09/29/2024 11:31:58 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.loader - trainable params: 8,034,455,552 || all params: 8,034,455,552 || trainable%: 100.0000
192.168.0.89: 09/29/2024 11:31:58 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.loader - trainable params: 8,034,455,552 || all params: 8,034,455,552 || trainable%: 100.0000
192.168.0.89: 09/29/2024 11:31:58 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
192.168.0.89: 09/29/2024 11:31:58 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
192.168.0.89: 09/29/2024 11:31:58 - INFO - llamafactory.model.adapter - Fine-tuning method: Full
192.168.0.89: 09/29/2024 11:31:58 - INFO - llamafactory.model.loader - trainable params: 8,034,455,552 || all params: 8,034,455,552 || trainable%: 100.0000
192.168.0.89: 09/29/2024 11:31:58 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
192.168.0.89: 09/29/2024 11:31:58 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
192.168.0.89: 09/29/2024 11:31:58 - INFO - llamafactory.model.loader - trainable params: 8,034,455,552 || all params: 8,034,455,552 || trainable%: 100.0000
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.loader - trainable params: 8,034,455,552 || all params: 8,034,455,552 || trainable%: 100.0000
192.168.0.89: 09/29/2024 11:31:58 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
192.168.0.89: 09/29/2024 11:31:58 - INFO - llamafactory.model.adapter - Fine-tuning method: Full
192.168.0.89: 09/29/2024 11:31:58 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.loader - trainable params: 8,034,455,552 || all params: 8,034,455,552 || trainable%: 100.0000
192.168.0.25: 09/29/2024 11:32:13 - INFO - llamafactory.model.loader - trainable params: 8,034,455,552 || all params: 8,034,455,552 || trainable%: 100.0000
192.168.0.89: 09/29/2024 11:31:58 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
192.168.0.89: 09/29/2024 11:31:58 - INFO - llamafactory.model.adapter - Fine-tuning method: Full
192.168.0.13: 09/29/2024 11:32:14 - INFO - llamafactory.model.loader - trainable params: 8,034,455,552 || all params: 8,034,455,552 || trainable%: 100.0000
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.adapter - Fine-tuning method: Full
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.loader - trainable params: 8,034,455,552 || all params: 8,034,455,552 || trainable%: 100.0000
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.loader - trainable params: 8,034,455,552 || all params: 8,034,455,552 || trainable%: 100.0000
192.168.0.89: 09/29/2024 11:31:58 - INFO - llamafactory.model.loader - trainable params: 8,034,455,552 || all params: 8,034,455,552 || trainable%: 100.0000
192.168.0.89: 09/29/2024 11:31:58 - INFO - llamafactory.model.loader - trainable params: 8,034,455,552 || all params: 8,034,455,552 || trainable%: 100.0000
192.168.0.89: 09/29/2024 11:31:58 - INFO - llamafactory.model.loader - trainable params: 8,034,455,552 || all params: 8,034,455,552 || trainable%: 100.0000
192.168.0.149: 09/29/2024 11:32:16 - INFO - llamafactory.model.loader - trainable params: 8,034,455,552 || all params: 8,034,455,552 || trainable%: 100.0000
192.168.0.25: Detected kernel version 4.19.36, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
192.168.0.25: [INFO|trainer.py:648] 2024-09-29 11:32:13,658 >> Using auto half precision backend
192.168.0.89: [INFO|trainer.py:648] 2024-09-29 11:31:58,036 >> Using auto half precision backend
192.168.0.13: [INFO|trainer.py:648] 2024-09-29 11:32:14,119 >> Using auto half precision backend
192.168.0.149: [INFO|trainer.py:648] 2024-09-29 11:32:16,706 >> Using auto half precision backend
192.168.0.25: [2024-09-29 11:32:15,096] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.4, git-hash=unknown, git-branch=unknown
192.168.0.25: [2024-09-29 11:32:15,118] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
192.168.0.25: [2024-09-29 11:32:15,121] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
192.168.0.25: [2024-09-29 11:32:15,121] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
192.168.0.25: [2024-09-29 11:32:15,152] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
192.168.0.25: [2024-09-29 11:32:15,152] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch_npu.utils._optim.partialclass.<locals>.NewCls'>
192.168.0.25: [2024-09-29 11:32:15,152] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
192.168.0.25: [2024-09-29 11:32:15,152] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer
192.168.0.25: [2024-09-29 11:32:15,595] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
192.168.0.25: [2024-09-29 11:32:15,597] [INFO] [utils.py:782:see_memory_usage] MA 2.44 GB         Max_MA 4.5 GB         CA 14.77 GB         Max_CA 24 GB 
192.168.0.25: [2024-09-29 11:32:15,597] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 61.97 GB, percent = 8.2%
192.168.0.25: [2024-09-29 11:32:15,601] [INFO] [stage3.py:130:__init__] Reduce bucket size 16777216
192.168.0.25: [2024-09-29 11:32:15,602] [INFO] [stage3.py:131:__init__] Prefetch bucket size 15099494
192.168.0.25: [2024-09-29 11:32:16,037] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
192.168.0.25: [2024-09-29 11:32:16,038] [INFO] [utils.py:782:see_memory_usage] MA 2.44 GB         Max_MA 2.44 GB         CA 14.77 GB         Max_CA 15 GB 
192.168.0.25: [2024-09-29 11:32:16,038] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 61.97 GB, percent = 8.2%
192.168.0.25: Parameter Offload: Total persistent parameters: 266240 in 65 params
192.168.0.25: [2024-09-29 11:32:16,545] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
192.168.0.25: [2024-09-29 11:32:16,546] [INFO] [utils.py:782:see_memory_usage] MA 0.53 GB         Max_MA 2.47 GB         CA 14.77 GB         Max_CA 15 GB 
192.168.0.25: [2024-09-29 11:32:16,547] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 61.97 GB, percent = 8.2%
192.168.0.25: [2024-09-29 11:32:16,986] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
192.168.0.25: [2024-09-29 11:32:16,988] [INFO] [utils.py:782:see_memory_usage] MA 0.53 GB         Max_MA 0.53 GB         CA 14.77 GB         Max_CA 15 GB 
192.168.0.25: [2024-09-29 11:32:16,988] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 61.97 GB, percent = 8.2%
192.168.0.25: [2024-09-29 11:32:19,025] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 2
192.168.0.25: [2024-09-29 11:32:19,026] [INFO] [utils.py:782:see_memory_usage] MA 0.47 GB         Max_MA 0.53 GB         CA 0.47 GB         Max_CA 15 GB 
192.168.0.25: [2024-09-29 11:32:19,026] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 63.01 GB, percent = 8.3%
192.168.0.25: [2024-09-29 11:32:19,458] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
192.168.0.25: [2024-09-29 11:32:19,459] [INFO] [utils.py:782:see_memory_usage] MA 0.47 GB         Max_MA 0.47 GB         CA 0.47 GB         Max_CA 0 GB 
192.168.0.25: [2024-09-29 11:32:19,460] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 61.96 GB, percent = 8.2%
192.168.0.25: [2024-09-29 11:32:19,890] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
192.168.0.25: [2024-09-29 11:32:19,891] [INFO] [utils.py:782:see_memory_usage] MA 1.4 GB         Max_MA 1.87 GB         CA 1.88 GB         Max_CA 2 GB 
192.168.0.25: [2024-09-29 11:32:19,892] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 61.96 GB, percent = 8.2%
192.168.0.25: [2024-09-29 11:32:20,692] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
192.168.0.25: [2024-09-29 11:32:20,694] [INFO] [utils.py:782:see_memory_usage] MA 1.4 GB         Max_MA 1.4 GB         CA 1.88 GB         Max_CA 2 GB 
192.168.0.25: [2024-09-29 11:32:20,694] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 61.96 GB, percent = 8.2%
192.168.0.25: [2024-09-29 11:32:21,131] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
192.168.0.25: [2024-09-29 11:32:21,132] [INFO] [utils.py:782:see_memory_usage] MA 1.4 GB         Max_MA 2.34 GB         CA 2.81 GB         Max_CA 3 GB 
192.168.0.25: [2024-09-29 11:32:21,132] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 61.95 GB, percent = 8.2%
192.168.0.25: [2024-09-29 11:32:21,135] [INFO] [stage3.py:486:_setup_for_real_optimizer] optimizer state initialized
192.168.0.89: [INFO|trainer.py:2134] 2024-09-29 11:32:07,388 >> ***** Running training *****
192.168.0.89: [INFO|trainer.py:2135] 2024-09-29 11:32:07,388 >>   Num examples = 132,364
192.168.0.89: [INFO|trainer.py:2136] 2024-09-29 11:32:07,388 >>   Num Epochs = 3
192.168.0.89: [INFO|trainer.py:2137] 2024-09-29 11:32:07,388 >>   Instantaneous batch size per device = 8
192.168.0.89: [INFO|trainer.py:2140] 2024-09-29 11:32:07,388 >>   Total train batch size (w. parallel, distributed & accumulation) = 256
192.168.0.89: [INFO|trainer.py:2141] 2024-09-29 11:32:07,388 >>   Gradient Accumulation steps = 1
192.168.0.89: [INFO|trainer.py:2142] 2024-09-29 11:32:07,388 >>   Total optimization steps = 1,554
192.168.0.13: [INFO|trainer.py:2134] 2024-09-29 11:32:23,466 >> ***** Running training *****
192.168.0.13: [INFO|trainer.py:2135] 2024-09-29 11:32:23,466 >>   Num examples = 132,364
192.168.0.13: [INFO|trainer.py:2136] 2024-09-29 11:32:23,466 >>   Num Epochs = 3
192.168.0.13: [INFO|trainer.py:2137] 2024-09-29 11:32:23,466 >>   Instantaneous batch size per device = 8
192.168.0.13: [INFO|trainer.py:2140] 2024-09-29 11:32:23,466 >>   Total train batch size (w. parallel, distributed & accumulation) = 256
192.168.0.13: [INFO|trainer.py:2141] 2024-09-29 11:32:23,466 >>   Gradient Accumulation steps = 1
192.168.0.13: [INFO|trainer.py:2142] 2024-09-29 11:32:23,466 >>   Total optimization steps = 1,554
192.168.0.149: [INFO|trainer.py:2134] 2024-09-29 11:32:26,048 >> ***** Running training *****
192.168.0.149: [INFO|trainer.py:2135] 2024-09-29 11:32:26,048 >>   Num examples = 132,364
192.168.0.149: [INFO|trainer.py:2136] 2024-09-29 11:32:26,048 >>   Num Epochs = 3
192.168.0.149: [INFO|trainer.py:2137] 2024-09-29 11:32:26,048 >>   Instantaneous batch size per device = 8
192.168.0.149: [INFO|trainer.py:2140] 2024-09-29 11:32:26,048 >>   Total train batch size (w. parallel, distributed & accumulation) = 256
192.168.0.89: [INFO|trainer.py:2143] 2024-09-29 11:32:07,391 >>   Number of trainable parameters = 8,034,455,552
192.168.0.149: [INFO|trainer.py:2141] 2024-09-29 11:32:26,048 >>   Gradient Accumulation steps = 1
192.168.0.149: [INFO|trainer.py:2142] 2024-09-29 11:32:26,048 >>   Total optimization steps = 1,554
192.168.0.13: [INFO|trainer.py:2143] 2024-09-29 11:32:23,469 >>   Number of trainable parameters = 8,034,455,552
192.168.0.149: [INFO|trainer.py:2143] 2024-09-29 11:32:26,051 >>   Number of trainable parameters = 8,034,455,552
192.168.0.25: [2024-09-29 11:32:23,461] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
192.168.0.25: [2024-09-29 11:32:23,463] [INFO] [utils.py:782:see_memory_usage] MA 1.9 GB         Max_MA 3.87 GB         CA 11.43 GB         Max_CA 11 GB 
192.168.0.25: [2024-09-29 11:32:23,463] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 62.0 GB, percent = 8.2%
192.168.0.25: [2024-09-29 11:32:23,464] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
192.168.0.25: [2024-09-29 11:32:23,464] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
192.168.0.25: [2024-09-29 11:32:23,464] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
192.168.0.25: [2024-09-29 11:32:23,464] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]
192.168.0.25: [2024-09-29 11:32:23,466] [INFO] [config.py:997:print] DeepSpeedEngine configuration:
192.168.0.25: [2024-09-29 11:32:23,467] [INFO] [config.py:1001:print]   activation_checkpointing_config  {
192.168.0.25:     "partition_activations": false, 
192.168.0.25:     "contiguous_memory_optimization": false, 
192.168.0.25:     "cpu_checkpointing": false, 
192.168.0.25:     "number_checkpoints": null, 
192.168.0.25:     "synchronize_checkpoint_boundary": false, 
192.168.0.25:     "profile": false
192.168.0.25: }
192.168.0.25: [2024-09-29 11:32:23,467] [INFO] [config.py:1001:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
192.168.0.25: [2024-09-29 11:32:23,467] [INFO] [config.py:1001:print]   amp_enabled .................. False
192.168.0.25: [2024-09-29 11:32:23,467] [INFO] [config.py:1001:print]   amp_params ................... False
192.168.0.25: [2024-09-29 11:32:23,468] [INFO] [config.py:1001:print]   autotuning_config ............ {
192.168.0.25:     "enabled": false, 
192.168.0.25:     "start_step": null, 
192.168.0.25:     "end_step": null, 
192.168.0.25:     "metric_path": null, 
192.168.0.25:     "arg_mappings": null, 
192.168.0.25:     "metric": "throughput", 
192.168.0.25:     "model_info": null, 
192.168.0.25:     "results_dir": "autotuning_results", 
192.168.0.25:     "exps_dir": "autotuning_exps", 
192.168.0.25:     "overwrite": true, 
192.168.0.25:     "fast": true, 
192.168.0.25:     "start_profile_step": 3, 
192.168.0.25:     "end_profile_step": 5, 
192.168.0.25:     "tuner_type": "gridsearch", 
192.168.0.25:     "tuner_early_stopping": 5, 
192.168.0.25:     "tuner_num_trials": 50, 
192.168.0.25:     "model_info_path": null, 
192.168.0.25:     "mp_size": 1, 
192.168.0.25:     "max_train_batch_size": null, 
192.168.0.25:     "min_train_batch_size": 1, 
192.168.0.25:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
192.168.0.25:     "min_train_micro_batch_size_per_gpu": 1, 
192.168.0.25:     "num_tuning_micro_batch_sizes": 3
192.168.0.25: }
192.168.0.25: [2024-09-29 11:32:23,468] [INFO] [config.py:1001:print]   bfloat16_enabled ............. False
192.168.0.25: [2024-09-29 11:32:23,468] [INFO] [config.py:1001:print]   bfloat16_immediate_grad_update  False
192.168.0.25: [2024-09-29 11:32:23,468] [INFO] [config.py:1001:print]   checkpoint_parallel_write_pipeline  False
192.168.0.25: [2024-09-29 11:32:23,468] [INFO] [config.py:1001:print]   checkpoint_tag_validation_enabled  True
192.168.0.25: [2024-09-29 11:32:23,468] [INFO] [config.py:1001:print]   checkpoint_tag_validation_fail  False
192.168.0.25: [2024-09-29 11:32:23,468] [INFO] [config.py:1001:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0xffff48898790>
192.168.0.25: [2024-09-29 11:32:23,468] [INFO] [config.py:1001:print]   communication_data_type ...... None
192.168.0.25: [2024-09-29 11:32:23,468] [INFO] [config.py:1001:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
192.168.0.25: [2024-09-29 11:32:23,468] [INFO] [config.py:1001:print]   curriculum_enabled_legacy .... False
192.168.0.25: [2024-09-29 11:32:23,468] [INFO] [config.py:1001:print]   curriculum_params_legacy ..... False
192.168.0.25: [2024-09-29 11:32:23,468] [INFO] [config.py:1001:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
192.168.0.25: [2024-09-29 11:32:23,469] [INFO] [config.py:1001:print]   data_efficiency_enabled ...... False
192.168.0.25: [2024-09-29 11:32:23,469] [INFO] [config.py:1001:print]   dataloader_drop_last ......... False
192.168.0.25: [2024-09-29 11:32:23,469] [INFO] [config.py:1001:print]   disable_allgather ............ False
192.168.0.25: [2024-09-29 11:32:23,469] [INFO] [config.py:1001:print]   dump_state ................... False
192.168.0.25: [2024-09-29 11:32:23,469] [INFO] [config.py:1001:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
192.168.0.25: [2024-09-29 11:32:23,469] [INFO] [config.py:1001:print]   eigenvalue_enabled ........... False
192.168.0.25: [2024-09-29 11:32:23,469] [INFO] [config.py:1001:print]   eigenvalue_gas_boundary_resolution  1
192.168.0.25: [2024-09-29 11:32:23,469] [INFO] [config.py:1001:print]   eigenvalue_layer_name ........ bert.encoder.layer
192.168.0.25: [2024-09-29 11:32:23,469] [INFO] [config.py:1001:print]   eigenvalue_layer_num ......... 0
192.168.0.25: [2024-09-29 11:32:23,469] [INFO] [config.py:1001:print]   eigenvalue_max_iter .......... 100
192.168.0.25: [2024-09-29 11:32:23,469] [INFO] [config.py:1001:print]   eigenvalue_stability ......... 1e-06
192.168.0.25: [2024-09-29 11:32:23,469] [INFO] [config.py:1001:print]   eigenvalue_tol ............... 0.01
192.168.0.25: [2024-09-29 11:32:23,469] [INFO] [config.py:1001:print]   eigenvalue_verbose ........... False
192.168.0.25: [2024-09-29 11:32:23,469] [INFO] [config.py:1001:print]   elasticity_enabled ........... False
192.168.0.25: [2024-09-29 11:32:23,469] [INFO] [config.py:1001:print]   flops_profiler_config ........ {
192.168.0.25:     "enabled": false, 
192.168.0.25:     "recompute_fwd_factor": 0.0, 
192.168.0.25:     "profile_step": 1, 
192.168.0.25:     "module_depth": -1, 
192.168.0.25:     "top_modules": 1, 
192.168.0.25:     "detailed": true, 
192.168.0.25:     "output_file": null
192.168.0.25: }
192.168.0.25: [2024-09-29 11:32:23,469] [INFO] [config.py:1001:print]   fp16_auto_cast ............... False
192.168.0.25: [2024-09-29 11:32:23,469] [INFO] [config.py:1001:print]   fp16_enabled ................. True
192.168.0.25: [2024-09-29 11:32:23,469] [INFO] [config.py:1001:print]   fp16_master_weights_and_gradients  False
192.168.0.25: [2024-09-29 11:32:23,470] [INFO] [config.py:1001:print]   global_rank .................. 0
192.168.0.25: [2024-09-29 11:32:23,470] [INFO] [config.py:1001:print]   grad_accum_dtype ............. None
192.168.0.25: [2024-09-29 11:32:23,470] [INFO] [config.py:1001:print]   gradient_accumulation_steps .. 1
192.168.0.25: [2024-09-29 11:32:23,470] [INFO] [config.py:1001:print]   gradient_clipping ............ 1.0
192.168.0.25: [2024-09-29 11:32:23,470] [INFO] [config.py:1001:print]   gradient_predivide_factor .... 1.0
192.168.0.25: [2024-09-29 11:32:23,470] [INFO] [config.py:1001:print]   graph_harvesting ............. False
192.168.0.25: [2024-09-29 11:32:23,470] [INFO] [config.py:1001:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
192.168.0.25: [2024-09-29 11:32:23,470] [INFO] [config.py:1001:print]   initial_dynamic_scale ........ 65536
192.168.0.25: [2024-09-29 11:32:23,470] [INFO] [config.py:1001:print]   load_universal_checkpoint .... False
192.168.0.25: [2024-09-29 11:32:23,470] [INFO] [config.py:1001:print]   loss_scale ................... 0
192.168.0.25: [2024-09-29 11:32:23,470] [INFO] [config.py:1001:print]   memory_breakdown ............. False
192.168.0.25: [2024-09-29 11:32:23,470] [INFO] [config.py:1001:print]   mics_hierarchial_params_gather  False
192.168.0.25: [2024-09-29 11:32:23,470] [INFO] [config.py:1001:print]   mics_shard_size .............. -1
192.168.0.25: [2024-09-29 11:32:23,470] [INFO] [config.py:1001:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
192.168.0.25: [2024-09-29 11:32:23,470] [INFO] [config.py:1001:print]   nebula_config ................ {
192.168.0.25:     "enabled": false, 
192.168.0.25:     "persistent_storage_path": null, 
192.168.0.25:     "persistent_time_interval": 100, 
192.168.0.25:     "num_of_version_in_retention": 2, 
192.168.0.25:     "enable_nebula_load": true, 
192.168.0.25:     "load_path": null
192.168.0.25: }
192.168.0.25: [2024-09-29 11:32:23,470] [INFO] [config.py:1001:print]   optimizer_legacy_fusion ...... False
192.168.0.25: [2024-09-29 11:32:23,471] [INFO] [config.py:1001:print]   optimizer_name ............... None
192.168.0.25: [2024-09-29 11:32:23,471] [INFO] [config.py:1001:print]   optimizer_params ............. None
192.168.0.25: [2024-09-29 11:32:23,471] [INFO] [config.py:1001:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
192.168.0.25: [2024-09-29 11:32:23,471] [INFO] [config.py:1001:print]   pld_enabled .................. False
192.168.0.25: [2024-09-29 11:32:23,471] [INFO] [config.py:1001:print]   pld_params ................... False
192.168.0.25: [2024-09-29 11:32:23,471] [INFO] [config.py:1001:print]   prescale_gradients ........... False
192.168.0.25: [2024-09-29 11:32:23,471] [INFO] [config.py:1001:print]   scheduler_name ............... None
192.168.0.25: [2024-09-29 11:32:23,471] [INFO] [config.py:1001:print]   scheduler_params ............. None
192.168.0.25: [2024-09-29 11:32:23,471] [INFO] [config.py:1001:print]   seq_parallel_communication_data_type  torch.float32
192.168.0.25: [2024-09-29 11:32:23,471] [INFO] [config.py:1001:print]   sparse_attention ............. None
192.168.0.25: [2024-09-29 11:32:23,471] [INFO] [config.py:1001:print]   sparse_gradients_enabled ..... False
192.168.0.25: [2024-09-29 11:32:23,471] [INFO] [config.py:1001:print]   steps_per_print .............. inf
192.168.0.25: [2024-09-29 11:32:23,471] [INFO] [config.py:1001:print]   timers_config ................ enabled=True synchronized=True
192.168.0.25: [2024-09-29 11:32:23,471] [INFO] [config.py:1001:print]   train_batch_size ............. 256
192.168.0.25: [2024-09-29 11:32:23,471] [INFO] [config.py:1001:print]   train_micro_batch_size_per_gpu  8
192.168.0.25: [2024-09-29 11:32:23,471] [INFO] [config.py:1001:print]   use_data_before_expert_parallel_  False
192.168.0.25: [2024-09-29 11:32:23,471] [INFO] [config.py:1001:print]   use_node_local_storage ....... False
192.168.0.25: [2024-09-29 11:32:23,471] [INFO] [config.py:1001:print]   wall_clock_breakdown ......... False
192.168.0.25: [2024-09-29 11:32:23,471] [INFO] [config.py:1001:print]   weight_quantization_config ... None
192.168.0.25: [2024-09-29 11:32:23,472] [INFO] [config.py:1001:print]   world_size ................... 32
192.168.0.25: [2024-09-29 11:32:23,472] [INFO] [config.py:1001:print]   zero_allow_untested_optimizer  True
192.168.0.25: [2024-09-29 11:32:23,472] [INFO] [config.py:1001:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=16777216 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=15099494 param_persistence_threshold=40960 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
192.168.0.25: [2024-09-29 11:32:23,472] [INFO] [config.py:1001:print]   zero_enabled ................. True
192.168.0.25: [2024-09-29 11:32:23,472] [INFO] [config.py:1001:print]   zero_force_ds_cpu_optimizer .. True
192.168.0.25: [2024-09-29 11:32:23,472] [INFO] [config.py:1001:print]   zero_optimization_stage ...... 3
192.168.0.25: [2024-09-29 11:32:23,472] [INFO] [config.py:987:print_user_config]   json = {
192.168.0.25:     "train_batch_size": 256, 
192.168.0.25:     "train_micro_batch_size_per_gpu": 8, 
192.168.0.25:     "gradient_accumulation_steps": 1, 
192.168.0.25:     "gradient_clipping": 1.0, 
192.168.0.25:     "zero_allow_untested_optimizer": true, 
192.168.0.25:     "fp16": {
192.168.0.25:         "enabled": true, 
192.168.0.25:         "loss_scale": 0, 
192.168.0.25:         "loss_scale_window": 1000, 
192.168.0.25:         "initial_scale_power": 16, 
192.168.0.25:         "hysteresis": 2, 
192.168.0.25:         "min_loss_scale": 1
192.168.0.25:     }, 
192.168.0.25:     "bf16": {
192.168.0.25:         "enabled": false
192.168.0.25:     }, 
192.168.0.25:     "zero_optimization": {
192.168.0.25:         "stage": 3, 
192.168.0.25:         "overlap_comm": true, 
192.168.0.25:         "contiguous_gradients": true, 
192.168.0.25:         "sub_group_size": 1.000000e+09, 
192.168.0.25:         "reduce_bucket_size": 1.677722e+07, 
192.168.0.25:         "stage3_prefetch_bucket_size": 1.509949e+07, 
192.168.0.25:         "stage3_param_persistence_threshold": 4.096000e+04, 
192.168.0.25:         "stage3_max_live_parameters": 1.000000e+09, 
192.168.0.25:         "stage3_max_reuse_distance": 1.000000e+09, 
192.168.0.25:         "stage3_gather_16bit_weights_on_model_save": true
192.168.0.25:     }, 
192.168.0.25:     "steps_per_print": inf
192.168.0.25: }
192.168.0.25: [INFO|trainer.py:2134] 2024-09-29 11:32:23,472 >> ***** Running training *****
192.168.0.25: [INFO|trainer.py:2135] 2024-09-29 11:32:23,472 >>   Num examples = 132,364
192.168.0.25: [INFO|trainer.py:2136] 2024-09-29 11:32:23,473 >>   Num Epochs = 3
192.168.0.25: [INFO|trainer.py:2137] 2024-09-29 11:32:23,473 >>   Instantaneous batch size per device = 8
192.168.0.25: [INFO|trainer.py:2140] 2024-09-29 11:32:23,473 >>   Total train batch size (w. parallel, distributed & accumulation) = 256
192.168.0.25: [INFO|trainer.py:2141] 2024-09-29 11:32:23,473 >>   Gradient Accumulation steps = 1
192.168.0.25: [INFO|trainer.py:2142] 2024-09-29 11:32:23,473 >>   Total optimization steps = 1,554
192.168.0.25: [INFO|trainer.py:2143] 2024-09-29 11:32:23,475 >>   Number of trainable parameters = 8,034,455,552
192.168.0.25: Warning: Device do not support double dtype now, dtype cast repalce with float.
192.168.0.25: Warning: Device do not support double dtype now, dtype cast repalce with float.Warning: Device do not support double dtype now, dtype cast repalce with float.Warning: Device do not support double dtype now, dtype cast repalce with float.
192.168.0.25: 
192.168.0.25: 
192.168.0.25: Warning: Device do not support double dtype now, dtype cast repalce with float.
192.168.0.25: Warning: Device do not support double dtype now, dtype cast repalce with float.
192.168.0.25: Warning: Device do not support double dtype now, dtype cast repalce with float.
192.168.0.25: Warning: Device do not support double dtype now, dtype cast repalce with float.
192.168.0.89: Warning: Device do not support double dtype now, dtype cast repalce with float.Warning: Device do not support double dtype now, dtype cast repalce with float.Warning: Device do not support double dtype now, dtype cast repalce with float.Warning: Device do not support double dtype now, dtype cast repalce with float.Warning: Device do not support double dtype now, dtype cast repalce with float.
192.168.0.89: 
192.168.0.89: 
192.168.0.89: 
192.168.0.89: 
192.168.0.89: Warning: Device do not support double dtype now, dtype cast repalce with float.
192.168.0.89: Warning: Device do not support double dtype now, dtype cast repalce with float.Warning: Device do not support double dtype now, dtype cast repalce with float.
192.168.0.89: 
192.168.0.13: Warning: Device do not support double dtype now, dtype cast repalce with float.
192.168.0.13: Warning: Device do not support double dtype now, dtype cast repalce with float.Warning: Device do not support double dtype now, dtype cast repalce with float.Warning: Device do not support double dtype now, dtype cast repalce with float.
192.168.0.13: 
192.168.0.13: 
192.168.0.13: Warning: Device do not support double dtype now, dtype cast repalce with float.Warning: Device do not support double dtype now, dtype cast repalce with float.
192.168.0.13: 
192.168.0.13: Warning: Device do not support double dtype now, dtype cast repalce with float.Warning: Device do not support double dtype now, dtype cast repalce with float.
192.168.0.13: 
192.168.0.149: Warning: Device do not support double dtype now, dtype cast repalce with float.Warning: Device do not support double dtype now, dtype cast repalce with float.Warning: Device do not support double dtype now, dtype cast repalce with float.
192.168.0.149: 
192.168.0.149: 
192.168.0.149: Warning: Device do not support double dtype now, dtype cast repalce with float.Warning: Device do not support double dtype now, dtype cast repalce with float.Warning: Device do not support double dtype now, dtype cast repalce with float.
192.168.0.149: 
192.168.0.149: 
192.168.0.149: Warning: Device do not support double dtype now, dtype cast repalce with float.
192.168.0.149: Warning: Device do not support double dtype now, dtype cast repalce with float.
192.168.0.25: [2024-09-29 11:33:04,722] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
192.168.0.25: {'loss': 8.9843, 'grad_norm': 16.412402120328853, 'learning_rate': 5e-06, 'epoch': 0.02}
192.168.0.25: {'loss': 5.8709, 'grad_norm': 10.666150636906442, 'learning_rate': 1e-05, 'epoch': 0.04}
192.168.0.25: {'loss': 5.0773, 'grad_norm': 5.038020536349829, 'learning_rate': 9.998951486913015e-06, 'epoch': 0.06}
192.168.0.25: {'loss': 4.8538, 'grad_norm': 2.6282574560146186, 'learning_rate': 9.995806387403935e-06, 'epoch': 0.08}
192.168.0.25: {'loss': 4.7611, 'grad_norm': 2.3710798737624237, 'learning_rate': 9.99056602054396e-06, 'epoch': 0.1}
192.168.0.25: {'loss': 4.714, 'grad_norm': 3.581505390534371, 'learning_rate': 9.98323258417038e-06, 'epoch': 0.12}
192.168.0.25: {'loss': 4.6379, 'grad_norm': 3.0152476969075117, 'learning_rate': 9.973809153964803e-06, 'epoch': 0.14}
192.168.0.25: {'loss': 4.5374, 'grad_norm': 7.418647015956702, 'learning_rate': 9.962299682163185e-06, 'epoch': 0.15}
192.168.0.25: {'loss': 4.4195, 'grad_norm': 3.0806821473962764, 'learning_rate': 9.948708995898251e-06, 'epoch': 0.17}
192.168.0.25: {'loss': 4.3151, 'grad_norm': 4.3054347004216345, 'learning_rate': 9.933042795174964e-06, 'epoch': 0.19}
192.168.0.25: {'loss': 4.2634, 'grad_norm': 4.107145567709461, 'learning_rate': 9.915307650479915e-06, 'epoch': 0.21}
192.168.0.25: {'loss': 4.1983, 'grad_norm': 4.247054032684653, 'learning_rate': 9.89551100002563e-06, 'epoch': 0.23}
192.168.0.25: {'loss': 4.1461, 'grad_norm': 3.15817670274391, 'learning_rate': 9.87366114663094e-06, 'epoch': 0.25}
192.168.0.25: {'loss': 4.1326, 'grad_norm': 5.178834410505942, 'learning_rate': 9.849767254238741e-06, 'epoch': 0.27}
192.168.0.25: {'loss': 4.0897, 'grad_norm': 6.582636575002789, 'learning_rate': 9.823839344072582e-06, 'epoch': 0.29}
192.168.0.25: {'loss': 4.0664, 'grad_norm': 6.307402412666605, 'learning_rate': 9.795888290433709e-06, 'epoch': 0.31}
192.168.0.25: {'loss': 4.0209, 'grad_norm': 3.0268787445290952, 'learning_rate': 9.76592581614034e-06, 'epoch': 0.33}
192.168.0.25: {'loss': 4.0087, 'grad_norm': 5.995962055760618, 'learning_rate': 9.733964487611044e-06, 'epoch': 0.35}
192.168.0.25: {'loss': 3.9779, 'grad_norm': 5.9338589396178945, 'learning_rate': 9.70001770959431e-06, 'epoch': 0.37}
192.168.0.25: {'loss': 3.9689, 'grad_norm': 4.683155131541554, 'learning_rate': 9.664099719546547e-06, 'epoch': 0.39}
192.168.0.25: {'loss': 3.931, 'grad_norm': 4.980131345801589, 'learning_rate': 9.626225581660802e-06, 'epoch': 0.41}
192.168.0.25: {'loss': 3.9229, 'grad_norm': 4.452830950749439, 'learning_rate': 9.586411180548771e-06, 'epoch': 0.42}
192.168.0.25: {'loss': 3.8999, 'grad_norm': 5.086309697702489, 'learning_rate': 9.544673214578699e-06, 'epoch': 0.44}
192.168.0.25: {'loss': 3.8845, 'grad_norm': 5.538128213144276, 'learning_rate': 9.501029188872005e-06, 'epoch': 0.46}
192.168.0.25: {'loss': 3.8593, 'grad_norm': 5.26340317756901, 'learning_rate': 9.455497407961533e-06, 'epoch': 0.48}
192.168.0.25: {'loss': 3.8479, 'grad_norm': 4.054533909588009, 'learning_rate': 9.40809696811455e-06, 'epoch': 0.5}
192.168.0.25: {'loss': 3.8229, 'grad_norm': 3.8191978529116635, 'learning_rate': 9.35884774932366e-06, 'epoch': 0.52}
192.168.0.25: {'loss': 3.7993, 'grad_norm': 3.152293076042909, 'learning_rate': 9.307770406969032e-06, 'epoch': 0.54}
192.168.0.25: {'loss': 3.8095, 'grad_norm': 4.458603229850182, 'learning_rate': 9.254886363155429e-06, 'epoch': 0.56}
192.168.0.25: {'loss': 3.789, 'grad_norm': 5.093962682484715, 'learning_rate': 9.200217797727663e-06, 'epoch': 0.58}
192.168.0.25: {'loss': 3.7566, 'grad_norm': 5.773861674624299, 'learning_rate': 9.143787638968255e-06, 'epoch': 0.6}
192.168.0.25: {'loss': 3.7531, 'grad_norm': 4.246875736553698, 'learning_rate': 9.085619553981186e-06, 'epoch': 0.62}
192.168.0.25: {'loss': 3.7429, 'grad_norm': 6.007717255828253, 'learning_rate': 9.025737938765803e-06, 'epoch': 0.64}
192.168.0.25: {'loss': 3.7246, 'grad_norm': 4.118709738828378, 'learning_rate': 8.964167907984989e-06, 'epoch': 0.66}
192.168.0.25: {'loss': 3.7154, 'grad_norm': 4.81252457253264, 'learning_rate': 8.900935284431962e-06, 'epoch': 0.68}
192.168.0.25: {'loss': 3.6852, 'grad_norm': 3.73207142540555, 'learning_rate': 8.836066588200052e-06, 'epoch': 0.69}
192.168.0.25: {'loss': 3.6818, 'grad_norm': 3.8984919297693885, 'learning_rate': 8.76958902556003e-06, 'epoch': 0.71}
192.168.0.25: {'loss': 3.6656, 'grad_norm': 6.05727042940396, 'learning_rate': 8.701530477549666e-06, 'epoch': 0.73}
192.168.0.25: {'loss': 3.6595, 'grad_norm': 6.887000966902447, 'learning_rate': 8.631919488280267e-06, 'epoch': 0.75}
192.168.0.25: {'loss': 3.6666, 'grad_norm': 4.273428350295849, 'learning_rate': 8.560785252965131e-06, 'epoch': 0.77}
192.168.0.25: {'loss': 3.6554, 'grad_norm': 3.182646403335141, 'learning_rate': 8.488157605674924e-06, 'epoch': 0.79}
192.168.0.25: {'loss': 3.6532, 'grad_norm': 5.6682591538069955, 'learning_rate': 8.414067006825108e-06, 'epoch': 0.81}
192.168.0.25: {'loss': 3.631, 'grad_norm': 3.679620114497109, 'learning_rate': 8.338544530400693e-06, 'epoch': 0.83}
192.168.0.25: {'loss': 3.6055, 'grad_norm': 5.613243193155612, 'learning_rate': 8.261621850923634e-06, 'epoch': 0.85}
192.168.0.25: {'loss': 3.6012, 'grad_norm': 4.218385807847895, 'learning_rate': 8.183331230168376e-06, 'epoch': 0.87}
192.168.0.25: {'loss': 3.6164, 'grad_norm': 4.82696795873793, 'learning_rate': 8.103705503631103e-06, 'epoch': 0.89}
192.168.0.25: {'loss': 3.5846, 'grad_norm': 4.484321833172413, 'learning_rate': 8.022778066758348e-06, 'epoch': 0.91}
192.168.0.25: {'loss': 3.5682, 'grad_norm': 4.022698847548298, 'learning_rate': 7.940582860940771e-06, 'epoch': 0.93}
192.168.0.25: {'loss': 3.5788, 'grad_norm': 3.504060569605478, 'learning_rate': 7.857154359277972e-06, 'epoch': 0.95}
192.168.0.25: {'loss': 3.5711, 'grad_norm': 8.015466045268457, 'learning_rate': 7.772527552120274e-06, 'epoch': 0.97}
192.168.0.13: [INFO|trainer.py:3819] 2024-09-29 13:03:55,422 >> 
192.168.0.13: ***** Running Evaluation *****
192.168.0.13: [INFO|trainer.py:3821] 2024-09-29 13:03:55,422 >>   Num examples = 14708
192.168.0.13: [INFO|trainer.py:3824] 2024-09-29 13:03:55,422 >>   Batch size = 1
192.168.0.25:   0%|          | 0/1554 [00:00<?, ?it/s]  0%|          | 1/1554 [00:12<5:24:53, 12.55s/it]  0%|          | 2/1554 [00:40<9:25:15, 21.85s/it]  0%|          | 3/1554 [00:55<7:53:37, 18.32s/it]  0%|          | 4/1554 [01:33<11:21:15, 26.37s/it]  0%|          | 5/1554 [01:43<8:44:59, 20.34s/it]   0%|          | 6/1554 [01:53<7:10:55, 16.70s/it]  0%|          | 7/1554 [02:02<6:12:02, 14.43s/it]  1%|          | 8/1554 [02:12<5:33:24, 12.94s/it]  1%|          | 9/1554 [02:22<5:06:55, 11.92s/it]  1%|          | 10/1554 [02:31<4:49:18, 11.24s/it]                                                     1%|          | 10/1554 [02:31<4:49:18, 11.24s/it]  1%|          | 11/1554 [02:41<4:37:04, 10.77s/it]  1%|          | 12/1554 [02:51<4:28:33, 10.45s/it]  1%|          | 13/1554 [03:01<4:22:28, 10.22s/it]  1%|          | 14/1554 [03:10<4:17:56, 10.05s/it]  1%|          | 15/1554 [03:20<4:14:07,  9.91s/it]  1%|          | 16/1554 [03:29<4:12:25,  9.85s/it]  1%|          | 17/1554 [03:39<4:10:52,  9.79s/it]  1%|          | 18/1554 [03:49<4:09:47,  9.76s/it]  1%|          | 19/1554 [03:59<4:09:47,  9.76s/it]  1%|▏         | 20/1554 [04:08<4:09:40,  9.77s/it]                                                     1%|▏         | 20/1554 [04:08<4:09:40,  9.77s/it]  1%|▏         | 21/1554 [04:18<4:09:59,  9.78s/it]  1%|▏         | 22/1554 [04:28<4:10:59,  9.83s/it]  1%|▏         | 23/1554 [04:38<4:11:57,  9.87s/it]  2%|▏         | 24/1554 [04:48<4:13:05,  9.93s/it]  2%|▏         | 25/1554 [04:58<4:15:37, 10.03s/it]  2%|▏         | 26/1554 [05:09<4:16:25, 10.07s/it]  2%|▏         | 27/1554 [05:19<4:16:58, 10.10s/it]  2%|▏         | 28/1554 [05:29<4:19:22, 10.20s/it]  2%|▏         | 29/1554 [05:40<4:21:11, 10.28s/it]  2%|▏         | 30/1554 [05:50<4:22:16, 10.33s/it]                                                     2%|▏         | 30/1554 [05:50<4:22:16, 10.33s/it]  2%|▏         | 31/1554 [06:01<4:23:08, 10.37s/it]  2%|▏         | 32/1554 [06:11<4:23:59, 10.41s/it]  2%|▏         | 33/1554 [06:21<4:23:34, 10.40s/it]  2%|▏         | 34/1554 [06:32<4:24:21, 10.44s/it]  2%|▏         | 35/1554 [06:43<4:26:15, 10.52s/it]  2%|▏         | 36/1554 [06:53<4:26:37, 10.54s/it]  2%|▏         | 37/1554 [07:04<4:26:03, 10.52s/it]  2%|▏         | 38/1554 [07:14<4:24:09, 10.45s/it]  3%|▎         | 39/1554 [07:25<4:25:05, 10.50s/it]  3%|▎         | 40/1554 [07:35<4:26:44, 10.57s/it]                                                     3%|▎         | 40/1554 [07:35<4:26:44, 10.57s/it]  3%|▎         | 41/1554 [07:46<4:25:35, 10.53s/it]  3%|▎         | 42/1554 [07:56<4:24:01, 10.48s/it]  3%|▎         | 43/1554 [08:07<4:28:39, 10.67s/it]  3%|▎         | 44/1554 [08:18<4:27:57, 10.65s/it]  3%|▎         | 45/1554 [08:28<4:25:07, 10.54s/it]  3%|▎         | 46/1554 [08:39<4:23:42, 10.49s/it]  3%|▎         | 47/1554 [08:49<4:24:50, 10.54s/it]  3%|▎         | 48/1554 [09:00<4:23:25, 10.50s/it]  3%|▎         | 49/1554 [09:10<4:22:06, 10.45s/it]  3%|▎         | 50/1554 [09:21<4:23:21, 10.51s/it]                                                     3%|▎         | 50/1554 [09:21<4:23:21, 10.51s/it]  3%|▎         | 51/1554 [09:31<4:23:03, 10.50s/it]  3%|▎         | 52/1554 [09:41<4:21:06, 10.43s/it]  3%|▎         | 53/1554 [09:52<4:22:25, 10.49s/it]  3%|▎         | 54/1554 [10:02<4:22:24, 10.50s/it]  4%|▎         | 55/1554 [10:13<4:22:46, 10.52s/it]  4%|▎         | 56/1554 [10:24<4:22:29, 10.51s/it]  4%|▎         | 57/1554 [10:34<4:22:48, 10.53s/it]  4%|▎         | 58/1554 [10:45<4:24:48, 10.62s/it]  4%|▍         | 59/1554 [10:55<4:21:48, 10.51s/it]  4%|▍         | 60/1554 [11:06<4:22:39, 10.55s/it]                                                     4%|▍         | 60/1554 [11:06<4:22:39, 10.55s/it]  4%|▍         | 61/1554 [11:16<4:23:15, 10.58s/it]  4%|▍         | 62/1554 [11:27<4:24:17, 10.63s/it]  4%|▍         | 63/1554 [11:38<4:23:25, 10.60s/it]  4%|▍         | 64/1554 [11:48<4:22:57, 10.59s/it]  4%|▍         | 65/1554 [11:59<4:21:54, 10.55s/it]  4%|▍         | 66/1554 [12:09<4:22:39, 10.59s/it]  4%|▍         | 67/1554 [12:20<4:24:33, 10.67s/it]  4%|▍         | 68/1554 [12:31<4:22:12, 10.59s/it]  4%|▍         | 69/1554 [12:41<4:22:42, 10.61s/it]  5%|▍         | 70/1554 [12:52<4:23:26, 10.65s/it]                                                     5%|▍         | 70/1554 [12:52<4:23:26, 10.65s/it]  5%|▍         | 71/1554 [13:03<4:24:04, 10.68s/it]  5%|▍         | 72/1554 [13:13<4:23:12, 10.66s/it]  5%|▍         | 73/1554 [13:25<4:25:44, 10.77s/it]  5%|▍         | 74/1554 [13:35<4:25:36, 10.77s/it]  5%|▍         | 75/1554 [13:46<4:25:20, 10.76s/it]  5%|▍         | 76/1554 [13:57<4:24:31, 10.74s/it]  5%|▍         | 77/1554 [14:08<4:25:30, 10.79s/it]  5%|▌         | 78/1554 [14:18<4:24:57, 10.77s/it]  5%|▌         | 79/1554 [14:29<4:24:15, 10.75s/it]  5%|▌         | 80/1554 [14:40<4:24:21, 10.76s/it]                                                     5%|▌         | 80/1554 [14:40<4:24:21, 10.76s/it]  5%|▌         | 81/1554 [14:51<4:26:11, 10.84s/it]  5%|▌         | 82/1554 [15:01<4:24:19, 10.77s/it]  5%|▌         | 83/1554 [15:12<4:24:05, 10.77s/it]  5%|▌         | 84/1554 [15:23<4:24:37, 10.80s/it]  5%|▌         | 85/1554 [15:34<4:23:53, 10.78s/it]  6%|▌         | 86/1554 [15:45<4:24:54, 10.83s/it]  6%|▌         | 87/1554 [15:55<4:23:38, 10.78s/it]  6%|▌         | 88/1554 [16:06<4:22:25, 10.74s/it]  6%|▌         | 89/1554 [16:17<4:22:34, 10.75s/it]  6%|▌         | 90/1554 [16:28<4:21:29, 10.72s/it]                                                     6%|▌         | 90/1554 [16:28<4:21:29, 10.72s/it]  6%|▌         | 91/1554 [16:38<4:20:40, 10.69s/it]  6%|▌         | 92/1554 [16:49<4:22:00, 10.75s/it]  6%|▌         | 93/1554 [17:00<4:22:24, 10.78s/it]  6%|▌         | 94/1554 [17:11<4:23:14, 10.82s/it]  6%|▌         | 95/1554 [17:21<4:21:05, 10.74s/it]  6%|▌         | 96/1554 [17:32<4:20:34, 10.72s/it]  6%|▌         | 97/1554 [17:43<4:21:04, 10.75s/it]  6%|▋         | 98/1554 [17:54<4:20:31, 10.74s/it]  6%|▋         | 99/1554 [18:05<4:22:01, 10.81s/it]  6%|▋         | 100/1554 [18:15<4:21:00, 10.77s/it]                                                      6%|▋         | 100/1554 [18:15<4:21:00, 10.77s/it]  6%|▋         | 101/1554 [18:26<4:20:59, 10.78s/it]  7%|▋         | 102/1554 [18:37<4:22:40, 10.85s/it]  7%|▋         | 103/1554 [18:48<4:23:21, 10.89s/it]  7%|▋         | 104/1554 [18:59<4:22:15, 10.85s/it]  7%|▋         | 105/1554 [19:10<4:21:43, 10.84s/it]  7%|▋         | 106/1554 [19:20<4:21:40, 10.84s/it]  7%|▋         | 107/1554 [19:31<4:21:34, 10.85s/it]  7%|▋         | 108/1554 [19:42<4:22:03, 10.87s/it]  7%|▋         | 109/1554 [19:53<4:21:07, 10.84s/it]  7%|▋         | 110/1554 [20:04<4:20:20, 10.82s/it]                                                      7%|▋         | 110/1554 [20:04<4:20:20, 10.82s/it]  7%|▋         | 111/1554 [20:15<4:20:08, 10.82s/it]  7%|▋         | 112/1554 [20:26<4:23:17, 10.96s/it]  7%|▋         | 113/1554 [20:37<4:22:52, 10.95s/it]  7%|▋         | 114/1554 [20:48<4:21:59, 10.92s/it]  7%|▋         | 115/1554 [20:58<4:20:42, 10.87s/it]  7%|▋         | 116/1554 [21:09<4:19:18, 10.82s/it]  8%|▊         | 117/1554 [21:20<4:20:12, 10.86s/it]  8%|▊         | 118/1554 [21:31<4:18:38, 10.81s/it]  8%|▊         | 119/1554 [21:42<4:18:27, 10.81s/it]  8%|▊         | 120/1554 [21:53<4:19:52, 10.87s/it]                                                      8%|▊         | 120/1554 [21:53<4:19:52, 10.87s/it]  8%|▊         | 121/1554 [22:04<4:21:36, 10.95s/it]  8%|▊         | 122/1554 [22:15<4:21:05, 10.94s/it]  8%|▊         | 123/1554 [22:26<4:21:25, 10.96s/it]  8%|▊         | 124/1554 [22:37<4:21:00, 10.95s/it]  8%|▊         | 125/1554 [22:48<4:21:18, 10.97s/it]  8%|▊         | 126/1554 [22:59<4:21:38, 10.99s/it]  8%|▊         | 127/1554 [23:09<4:19:36, 10.92s/it]  8%|▊         | 128/1554 [23:20<4:17:09, 10.82s/it]  8%|▊         | 129/1554 [23:31<4:17:07, 10.83s/it]  8%|▊         | 130/1554 [23:42<4:17:00, 10.83s/it]                                                      8%|▊         | 130/1554 [23:42<4:17:00, 10.83s/it]  8%|▊         | 131/1554 [23:53<4:18:03, 10.88s/it]  8%|▊         | 132/1554 [24:04<4:19:00, 10.93s/it]  9%|▊         | 133/1554 [24:15<4:19:59, 10.98s/it]  9%|▊         | 134/1554 [24:26<4:18:42, 10.93s/it]  9%|▊         | 135/1554 [24:37<4:20:03, 11.00s/it]  9%|▉         | 136/1554 [24:48<4:19:26, 10.98s/it]  9%|▉         | 137/1554 [24:59<4:19:39, 10.99s/it]  9%|▉         | 138/1554 [25:10<4:19:17, 10.99s/it]  9%|▉         | 139/1554 [25:21<4:20:09, 11.03s/it]  9%|▉         | 140/1554 [25:32<4:18:32, 10.97s/it]                                                      9%|▉         | 140/1554 [25:32<4:18:32, 10.97s/it]  9%|▉         | 141/1554 [25:43<4:19:04, 11.00s/it]  9%|▉         | 142/1554 [25:54<4:19:06, 11.01s/it]  9%|▉         | 143/1554 [26:05<4:18:33, 10.99s/it]  9%|▉         | 144/1554 [26:16<4:17:13, 10.95s/it]  9%|▉         | 145/1554 [26:27<4:18:28, 11.01s/it]  9%|▉         | 146/1554 [26:37<4:16:52, 10.95s/it]  9%|▉         | 147/1554 [26:48<4:16:52, 10.95s/it] 10%|▉         | 148/1554 [27:00<4:17:36, 10.99s/it] 10%|▉         | 149/1554 [27:10<4:17:07, 10.98s/it] 10%|▉         | 150/1554 [27:21<4:16:33, 10.96s/it]                                                     10%|▉         | 150/1554 [27:21<4:16:33, 10.96s/it] 10%|▉         | 151/1554 [27:32<4:16:37, 10.97s/it] 10%|▉         | 152/1554 [27:43<4:15:40, 10.94s/it] 10%|▉         | 153/1554 [27:54<4:16:13, 10.97s/it] 10%|▉         | 154/1554 [28:05<4:17:05, 11.02s/it] 10%|▉         | 155/1554 [28:16<4:16:01, 10.98s/it] 10%|█         | 156/1554 [28:27<4:14:18, 10.91s/it] 10%|█         | 157/1554 [28:38<4:13:47, 10.90s/it] 10%|█         | 158/1554 [28:49<4:15:12, 10.97s/it] 10%|█         | 159/1554 [29:00<4:15:02, 10.97s/it] 10%|█         | 160/1554 [29:11<4:16:41, 11.05s/it]                                                     10%|█         | 160/1554 [29:11<4:16:41, 11.05s/it] 10%|█         | 161/1554 [29:22<4:14:47, 10.97s/it] 10%|█         | 162/1554 [29:33<4:13:37, 10.93s/it] 10%|█         | 163/1554 [29:44<4:13:31, 10.94s/it] 11%|█         | 164/1554 [29:55<4:12:38, 10.91s/it] 11%|█         | 165/1554 [30:06<4:13:13, 10.94s/it] 11%|█         | 166/1554 [30:17<4:12:28, 10.91s/it] 11%|█         | 167/1554 [30:28<4:12:53, 10.94s/it] 11%|█         | 168/1554 [30:39<4:14:20, 11.01s/it] 11%|█         | 169/1554 [30:50<4:14:06, 11.01s/it] 11%|█         | 170/1554 [31:01<4:12:14, 10.94s/it]                                                     11%|█         | 170/1554 [31:01<4:12:14, 10.94s/it] 11%|█         | 171/1554 [31:11<4:11:21, 10.90s/it] 11%|█         | 172/1554 [31:22<4:11:58, 10.94s/it] 11%|█         | 173/1554 [31:33<4:12:11, 10.96s/it] 11%|█         | 174/1554 [31:44<4:11:11, 10.92s/it] 11%|█▏        | 175/1554 [31:55<4:10:27, 10.90s/it] 11%|█▏        | 176/1554 [32:06<4:09:22, 10.86s/it] 11%|█▏        | 177/1554 [32:17<4:10:48, 10.93s/it] 11%|█▏        | 178/1554 [32:28<4:11:14, 10.96s/it] 12%|█▏        | 179/1554 [32:39<4:11:25, 10.97s/it] 12%|█▏        | 180/1554 [32:50<4:10:48, 10.95s/it]                                                     12%|█▏        | 180/1554 [32:50<4:10:48, 10.95s/it] 12%|█▏        | 181/1554 [33:01<4:10:40, 10.95s/it] 12%|█▏        | 182/1554 [33:12<4:10:41, 10.96s/it] 12%|█▏        | 183/1554 [33:22<4:08:33, 10.88s/it] 12%|█▏        | 184/1554 [33:33<4:07:32, 10.84s/it] 12%|█▏        | 185/1554 [33:44<4:09:15, 10.92s/it] 12%|█▏        | 186/1554 [33:55<4:10:07, 10.97s/it] 12%|█▏        | 187/1554 [34:07<4:11:20, 11.03s/it] 12%|█▏        | 188/1554 [34:18<4:10:56, 11.02s/it] 12%|█▏        | 189/1554 [34:28<4:09:03, 10.95s/it] 12%|█▏        | 190/1554 [34:39<4:09:29, 10.97s/it]                                                     12%|█▏        | 190/1554 [34:39<4:09:29, 10.97s/it] 12%|█▏        | 191/1554 [34:51<4:10:57, 11.05s/it] 12%|█▏        | 192/1554 [35:02<4:09:34, 10.99s/it] 12%|█▏        | 193/1554 [35:12<4:08:17, 10.95s/it] 12%|█▏        | 194/1554 [35:23<4:07:12, 10.91s/it] 13%|█▎        | 195/1554 [35:34<4:07:35, 10.93s/it] 13%|█▎        | 196/1554 [35:45<4:06:37, 10.90s/it] 13%|█▎        | 197/1554 [35:56<4:08:54, 11.01s/it] 13%|█▎        | 198/1554 [36:07<4:08:44, 11.01s/it] 13%|█▎        | 199/1554 [36:18<4:08:24, 11.00s/it] 13%|█▎        | 200/1554 [36:29<4:08:46, 11.02s/it]                                                     13%|█▎        | 200/1554 [36:29<4:08:46, 11.02s/it] 13%|█▎        | 201/1554 [36:40<4:07:57, 11.00s/it] 13%|█▎        | 202/1554 [36:51<4:07:06, 10.97s/it] 13%|█▎        | 203/1554 [37:02<4:06:54, 10.97s/it] 13%|█▎        | 204/1554 [37:13<4:06:58, 10.98s/it] 13%|█▎        | 205/1554 [37:24<4:06:34, 10.97s/it] 13%|█▎        | 206/1554 [37:35<4:05:35, 10.93s/it] 13%|█▎        | 207/1554 [37:46<4:04:32, 10.89s/it] 13%|█▎        | 208/1554 [37:57<4:05:07, 10.93s/it] 13%|█▎        | 209/1554 [38:08<4:05:44, 10.96s/it] 14%|█▎        | 210/1554 [38:19<4:05:45, 10.97s/it]                                                     14%|█▎        | 210/1554 [38:19<4:05:45, 10.97s/it] 14%|█▎        | 211/1554 [38:30<4:05:35, 10.97s/it] 14%|█▎        | 212/1554 [38:41<4:04:55, 10.95s/it] 14%|█▎        | 213/1554 [38:51<4:03:45, 10.91s/it] 14%|█▍        | 214/1554 [39:02<4:04:00, 10.93s/it] 14%|█▍        | 215/1554 [39:13<4:04:48, 10.97s/it] 14%|█▍        | 216/1554 [39:24<4:03:49, 10.93s/it] 14%|█▍        | 217/1554 [39:35<4:04:27, 10.97s/it] 14%|█▍        | 218/1554 [39:46<4:03:33, 10.94s/it] 14%|█▍        | 219/1554 [39:57<4:02:18, 10.89s/it] 14%|█▍        | 220/1554 [40:08<4:03:05, 10.93s/it]                                                     14%|█▍        | 220/1554 [40:08<4:03:05, 10.93s/it] 14%|█▍        | 221/1554 [40:19<4:03:57, 10.98s/it] 14%|█▍        | 222/1554 [40:30<4:03:31, 10.97s/it] 14%|█▍        | 223/1554 [40:41<4:02:31, 10.93s/it] 14%|█▍        | 224/1554 [40:52<4:03:20, 10.98s/it] 14%|█▍        | 225/1554 [41:03<4:03:17, 10.98s/it] 15%|█▍        | 226/1554 [41:14<4:04:37, 11.05s/it] 15%|█▍        | 227/1554 [41:25<4:05:29, 11.10s/it] 15%|█▍        | 228/1554 [41:36<4:05:07, 11.09s/it] 15%|█▍        | 229/1554 [41:48<4:04:43, 11.08s/it] 15%|█▍        | 230/1554 [41:59<4:04:05, 11.06s/it]                                                     15%|█▍        | 230/1554 [41:59<4:04:05, 11.06s/it] 15%|█▍        | 231/1554 [42:10<4:04:46, 11.10s/it] 15%|█▍        | 232/1554 [42:21<4:03:36, 11.06s/it] 15%|█▍        | 233/1554 [42:32<4:02:43, 11.02s/it] 15%|█▌        | 234/1554 [42:42<4:00:33, 10.93s/it] 15%|█▌        | 235/1554 [42:53<3:59:36, 10.90s/it] 15%|█▌        | 236/1554 [43:04<4:01:01, 10.97s/it] 15%|█▌        | 237/1554 [43:15<4:02:04, 11.03s/it] 15%|█▌        | 238/1554 [43:26<4:00:05, 10.95s/it] 15%|█▌        | 239/1554 [43:37<3:59:55, 10.95s/it] 15%|█▌        | 240/1554 [43:48<3:59:10, 10.92s/it]                                                     15%|█▌        | 240/1554 [43:48<3:59:10, 10.92s/it] 16%|█▌        | 241/1554 [43:59<4:00:29, 10.99s/it] 16%|█▌        | 242/1554 [44:10<4:01:23, 11.04s/it] 16%|█▌        | 243/1554 [44:21<3:59:48, 10.98s/it] 16%|█▌        | 244/1554 [44:32<4:00:28, 11.01s/it] 16%|█▌        | 245/1554 [44:43<4:00:09, 11.01s/it] 16%|█▌        | 246/1554 [44:54<3:59:44, 11.00s/it] 16%|█▌        | 247/1554 [45:05<4:00:15, 11.03s/it] 16%|█▌        | 248/1554 [45:16<3:59:28, 11.00s/it] 16%|█▌        | 249/1554 [45:27<3:58:58, 10.99s/it] 16%|█▌        | 250/1554 [45:38<4:00:08, 11.05s/it]                                                     16%|█▌        | 250/1554 [45:38<4:00:08, 11.05s/it] 16%|█▌        | 251/1554 [45:50<4:00:39, 11.08s/it] 16%|█▌        | 252/1554 [46:00<3:58:25, 10.99s/it] 16%|█▋        | 253/1554 [46:11<3:56:46, 10.92s/it] 16%|█▋        | 254/1554 [46:22<3:56:43, 10.93s/it] 16%|█▋        | 255/1554 [46:33<3:55:33, 10.88s/it] 16%|█▋        | 256/1554 [46:44<3:56:10, 10.92s/it] 17%|█▋        | 257/1554 [46:55<3:56:11, 10.93s/it] 17%|█▋        | 258/1554 [47:06<3:56:07, 10.93s/it] 17%|█▋        | 259/1554 [47:17<3:56:18, 10.95s/it] 17%|█▋        | 260/1554 [47:28<3:55:07, 10.90s/it]                                                     17%|█▋        | 260/1554 [47:28<3:55:07, 10.90s/it] 17%|█▋        | 261/1554 [47:38<3:54:37, 10.89s/it] 17%|█▋        | 262/1554 [47:50<3:56:41, 10.99s/it] 17%|█▋        | 263/1554 [48:01<3:56:02, 10.97s/it] 17%|█▋        | 264/1554 [48:11<3:55:23, 10.95s/it] 17%|█▋        | 265/1554 [48:23<3:56:28, 11.01s/it] 17%|█▋        | 266/1554 [48:33<3:55:29, 10.97s/it] 17%|█▋        | 267/1554 [48:45<3:55:52, 11.00s/it] 17%|█▋        | 268/1554 [48:55<3:53:33, 10.90s/it] 17%|█▋        | 269/1554 [49:06<3:54:01, 10.93s/it] 17%|█▋        | 270/1554 [49:17<3:53:33, 10.91s/it]                                                     17%|█▋        | 270/1554 [49:17<3:53:33, 10.91s/it] 17%|█▋        | 271/1554 [49:28<3:53:47, 10.93s/it] 18%|█▊        | 272/1554 [49:39<3:53:28, 10.93s/it] 18%|█▊        | 273/1554 [49:50<3:52:38, 10.90s/it] 18%|█▊        | 274/1554 [50:01<3:52:25, 10.89s/it] 18%|█▊        | 275/1554 [50:12<3:52:29, 10.91s/it] 18%|█▊        | 276/1554 [50:23<3:53:00, 10.94s/it] 18%|█▊        | 277/1554 [50:34<3:53:18, 10.96s/it] 18%|█▊        | 278/1554 [50:45<3:53:24, 10.98s/it] 18%|█▊        | 279/1554 [50:56<3:52:35, 10.95s/it] 18%|█▊        | 280/1554 [51:07<3:52:57, 10.97s/it]                                                     18%|█▊        | 280/1554 [51:07<3:52:57, 10.97s/it] 18%|█▊        | 281/1554 [51:18<3:52:44, 10.97s/it] 18%|█▊        | 282/1554 [51:29<3:54:09, 11.05s/it] 18%|█▊        | 283/1554 [51:40<3:53:51, 11.04s/it] 18%|█▊        | 284/1554 [51:51<3:51:49, 10.95s/it] 18%|█▊        | 285/1554 [52:01<3:50:19, 10.89s/it] 18%|█▊        | 286/1554 [52:12<3:50:42, 10.92s/it] 18%|█▊        | 287/1554 [52:23<3:51:29, 10.96s/it] 19%|█▊        | 288/1554 [52:34<3:51:17, 10.96s/it] 19%|█▊        | 289/1554 [52:45<3:51:29, 10.98s/it] 19%|█▊        | 290/1554 [52:56<3:51:15, 10.98s/it]                                                     19%|█▊        | 290/1554 [52:56<3:51:15, 10.98s/it] 19%|█▊        | 291/1554 [53:07<3:50:58, 10.97s/it] 19%|█▉        | 292/1554 [53:18<3:50:56, 10.98s/it] 19%|█▉        | 293/1554 [53:29<3:50:32, 10.97s/it] 19%|█▉        | 294/1554 [53:40<3:51:08, 11.01s/it] 19%|█▉        | 295/1554 [53:51<3:51:23, 11.03s/it] 19%|█▉        | 296/1554 [54:02<3:51:58, 11.06s/it] 19%|█▉        | 297/1554 [54:13<3:49:42, 10.96s/it] 19%|█▉        | 298/1554 [54:24<3:50:34, 11.01s/it] 19%|█▉        | 299/1554 [54:35<3:49:57, 10.99s/it] 19%|█▉        | 300/1554 [54:46<3:48:48, 10.95s/it]                                                     19%|█▉        | 300/1554 [54:46<3:48:48, 10.95s/it] 19%|█▉        | 301/1554 [54:57<3:48:07, 10.92s/it] 19%|█▉        | 302/1554 [55:08<3:49:33, 11.00s/it] 19%|█▉        | 303/1554 [55:19<3:49:21, 11.00s/it] 20%|█▉        | 304/1554 [55:30<3:50:56, 11.09s/it] 20%|█▉        | 305/1554 [55:41<3:50:18, 11.06s/it] 20%|█▉        | 306/1554 [55:52<3:49:22, 11.03s/it] 20%|█▉        | 307/1554 [56:03<3:48:08, 10.98s/it] 20%|█▉        | 308/1554 [56:14<3:46:37, 10.91s/it] 20%|█▉        | 309/1554 [56:25<3:48:18, 11.00s/it] 20%|█▉        | 310/1554 [56:36<3:47:37, 10.98s/it]                                                     20%|█▉        | 310/1554 [56:36<3:47:37, 10.98s/it] 20%|██        | 311/1554 [56:47<3:46:09, 10.92s/it] 20%|██        | 312/1554 [56:58<3:47:23, 10.98s/it] 20%|██        | 313/1554 [57:09<3:47:17, 10.99s/it] 20%|██        | 314/1554 [57:20<3:46:34, 10.96s/it] 20%|██        | 315/1554 [57:31<3:47:07, 11.00s/it] 20%|██        | 316/1554 [57:42<3:46:14, 10.96s/it] 20%|██        | 317/1554 [57:53<3:46:34, 10.99s/it] 20%|██        | 318/1554 [58:04<3:45:50, 10.96s/it] 21%|██        | 319/1554 [58:15<3:45:27, 10.95s/it] 21%|██        | 320/1554 [58:26<3:44:18, 10.91s/it]                                                     21%|██        | 320/1554 [58:26<3:44:18, 10.91s/it] 21%|██        | 321/1554 [58:37<3:44:07, 10.91s/it] 21%|██        | 322/1554 [58:48<3:45:19, 10.97s/it] 21%|██        | 323/1554 [58:59<3:44:22, 10.94s/it] 21%|██        | 324/1554 [59:09<3:43:29, 10.90s/it] 21%|██        | 325/1554 [59:20<3:43:36, 10.92s/it] 21%|██        | 326/1554 [59:31<3:43:39, 10.93s/it] 21%|██        | 327/1554 [59:42<3:42:08, 10.86s/it] 21%|██        | 328/1554 [59:53<3:42:40, 10.90s/it] 21%|██        | 329/1554 [1:00:04<3:42:20, 10.89s/it] 21%|██        | 330/1554 [1:00:15<3:41:50, 10.87s/it]                                                       21%|██        | 330/1554 [1:00:15<3:41:50, 10.87s/it] 21%|██▏       | 331/1554 [1:00:26<3:41:40, 10.88s/it] 21%|██▏       | 332/1554 [1:00:36<3:40:51, 10.84s/it] 21%|██▏       | 333/1554 [1:00:47<3:40:57, 10.86s/it] 21%|██▏       | 334/1554 [1:00:58<3:39:39, 10.80s/it] 22%|██▏       | 335/1554 [1:01:09<3:41:45, 10.92s/it] 22%|██▏       | 336/1554 [1:01:20<3:42:48, 10.98s/it] 22%|██▏       | 337/1554 [1:01:31<3:42:36, 10.97s/it] 22%|██▏       | 338/1554 [1:01:42<3:41:04, 10.91s/it] 22%|██▏       | 339/1554 [1:01:53<3:40:53, 10.91s/it] 22%|██▏       | 340/1554 [1:02:04<3:41:05, 10.93s/it]                                                       22%|██▏       | 340/1554 [1:02:04<3:41:05, 10.93s/it] 22%|██▏       | 341/1554 [1:02:15<3:41:57, 10.98s/it] 22%|██▏       | 342/1554 [1:02:26<3:41:25, 10.96s/it] 22%|██▏       | 343/1554 [1:02:37<3:42:06, 11.00s/it] 22%|██▏       | 344/1554 [1:02:48<3:42:10, 11.02s/it] 22%|██▏       | 345/1554 [1:02:59<3:42:06, 11.02s/it] 22%|██▏       | 346/1554 [1:03:10<3:42:43, 11.06s/it] 22%|██▏       | 347/1554 [1:03:21<3:42:40, 11.07s/it] 22%|██▏       | 348/1554 [1:03:32<3:42:35, 11.07s/it] 22%|██▏       | 349/1554 [1:03:43<3:42:49, 11.09s/it] 23%|██▎       | 350/1554 [1:03:55<3:43:09, 11.12s/it]                                                       23%|██▎       | 350/1554 [1:03:55<3:43:09, 11.12s/it] 23%|██▎       | 351/1554 [1:04:06<3:42:42, 11.11s/it] 23%|██▎       | 352/1554 [1:04:17<3:42:07, 11.09s/it] 23%|██▎       | 353/1554 [1:04:28<3:41:08, 11.05s/it] 23%|██▎       | 354/1554 [1:04:39<3:40:21, 11.02s/it] 23%|██▎       | 355/1554 [1:04:50<3:40:18, 11.02s/it] 23%|██▎       | 356/1554 [1:05:01<3:40:41, 11.05s/it] 23%|██▎       | 357/1554 [1:05:12<3:42:37, 11.16s/it] 23%|██▎       | 358/1554 [1:05:23<3:41:53, 11.13s/it] 23%|██▎       | 359/1554 [1:05:34<3:41:47, 11.14s/it] 23%|██▎       | 360/1554 [1:05:46<3:41:49, 11.15s/it]                                                       23%|██▎       | 360/1554 [1:05:46<3:41:49, 11.15s/it] 23%|██▎       | 361/1554 [1:05:57<3:41:10, 11.12s/it] 23%|██▎       | 362/1554 [1:06:07<3:38:15, 10.99s/it] 23%|██▎       | 363/1554 [1:06:18<3:38:29, 11.01s/it] 23%|██▎       | 364/1554 [1:06:29<3:38:19, 11.01s/it] 23%|██▎       | 365/1554 [1:06:40<3:38:16, 11.01s/it] 24%|██▎       | 366/1554 [1:06:51<3:38:15, 11.02s/it] 24%|██▎       | 367/1554 [1:07:03<3:38:25, 11.04s/it] 24%|██▎       | 368/1554 [1:07:14<3:38:16, 11.04s/it] 24%|██▎       | 369/1554 [1:07:25<3:38:34, 11.07s/it] 24%|██▍       | 370/1554 [1:07:36<3:38:44, 11.08s/it]                                                       24%|██▍       | 370/1554 [1:07:36<3:38:44, 11.08s/it] 24%|██▍       | 371/1554 [1:07:47<3:38:57, 11.11s/it] 24%|██▍       | 372/1554 [1:07:58<3:39:03, 11.12s/it] 24%|██▍       | 373/1554 [1:08:09<3:39:35, 11.16s/it] 24%|██▍       | 374/1554 [1:08:20<3:39:02, 11.14s/it] 24%|██▍       | 375/1554 [1:08:32<3:38:28, 11.12s/it] 24%|██▍       | 376/1554 [1:08:43<3:38:06, 11.11s/it] 24%|██▍       | 377/1554 [1:08:54<3:38:30, 11.14s/it] 24%|██▍       | 378/1554 [1:09:05<3:38:09, 11.13s/it] 24%|██▍       | 379/1554 [1:09:16<3:36:52, 11.07s/it] 24%|██▍       | 380/1554 [1:09:27<3:36:36, 11.07s/it]                                                       24%|██▍       | 380/1554 [1:09:27<3:36:36, 11.07s/it] 25%|██▍       | 381/1554 [1:09:38<3:37:42, 11.14s/it] 25%|██▍       | 382/1554 [1:09:49<3:36:51, 11.10s/it] 25%|██▍       | 383/1554 [1:10:00<3:36:11, 11.08s/it] 25%|██▍       | 384/1554 [1:10:12<3:36:53, 11.12s/it] 25%|██▍       | 385/1554 [1:10:23<3:38:02, 11.19s/it] 25%|██▍       | 386/1554 [1:10:34<3:36:17, 11.11s/it] 25%|██▍       | 387/1554 [1:10:45<3:36:29, 11.13s/it] 25%|██▍       | 388/1554 [1:10:56<3:34:52, 11.06s/it] 25%|██▌       | 389/1554 [1:11:07<3:35:41, 11.11s/it] 25%|██▌       | 390/1554 [1:11:18<3:35:27, 11.11s/it]                                                       25%|██▌       | 390/1554 [1:11:18<3:35:27, 11.11s/it] 25%|██▌       | 391/1554 [1:11:29<3:34:53, 11.09s/it] 25%|██▌       | 392/1554 [1:11:40<3:35:23, 11.12s/it] 25%|██▌       | 393/1554 [1:11:52<3:35:15, 11.12s/it] 25%|██▌       | 394/1554 [1:12:03<3:35:00, 11.12s/it] 25%|██▌       | 395/1554 [1:12:14<3:33:24, 11.05s/it] 25%|██▌       | 396/1554 [1:12:24<3:32:01, 10.99s/it] 26%|██▌       | 397/1554 [1:12:35<3:32:07, 11.00s/it] 26%|██▌       | 398/1554 [1:12:46<3:31:21, 10.97s/it] 26%|██▌       | 399/1554 [1:12:57<3:31:43, 11.00s/it] 26%|██▌       | 400/1554 [1:13:09<3:32:24, 11.04s/it]                                                       26%|██▌       | 400/1554 [1:13:09<3:32:24, 11.04s/it] 26%|██▌       | 401/1554 [1:13:20<3:32:40, 11.07s/it] 26%|██▌       | 402/1554 [1:13:31<3:31:38, 11.02s/it] 26%|██▌       | 403/1554 [1:13:42<3:30:59, 11.00s/it] 26%|██▌       | 404/1554 [1:13:53<3:30:46, 11.00s/it] 26%|██▌       | 405/1554 [1:14:04<3:30:26, 10.99s/it] 26%|██▌       | 406/1554 [1:14:15<3:32:05, 11.09s/it] 26%|██▌       | 407/1554 [1:14:26<3:31:59, 11.09s/it] 26%|██▋       | 408/1554 [1:14:37<3:30:05, 11.00s/it] 26%|██▋       | 409/1554 [1:14:48<3:31:06, 11.06s/it] 26%|██▋       | 410/1554 [1:14:59<3:29:28, 10.99s/it]                                                       26%|██▋       | 410/1554 [1:14:59<3:29:28, 10.99s/it] 26%|██▋       | 411/1554 [1:15:10<3:30:27, 11.05s/it] 27%|██▋       | 412/1554 [1:15:21<3:31:31, 11.11s/it] 27%|██▋       | 413/1554 [1:15:32<3:31:44, 11.13s/it] 27%|██▋       | 414/1554 [1:15:43<3:30:42, 11.09s/it] 27%|██▋       | 415/1554 [1:15:54<3:30:43, 11.10s/it] 27%|██▋       | 416/1554 [1:16:06<3:30:18, 11.09s/it] 27%|██▋       | 417/1554 [1:16:16<3:29:23, 11.05s/it] 27%|██▋       | 418/1554 [1:16:27<3:28:56, 11.04s/it] 27%|██▋       | 419/1554 [1:16:39<3:29:20, 11.07s/it] 27%|██▋       | 420/1554 [1:16:50<3:28:36, 11.04s/it]                                                       27%|██▋       | 420/1554 [1:16:50<3:28:36, 11.04s/it] 27%|██▋       | 421/1554 [1:17:01<3:27:41, 11.00s/it] 27%|██▋       | 422/1554 [1:17:12<3:27:28, 11.00s/it] 27%|██▋       | 423/1554 [1:17:23<3:28:08, 11.04s/it] 27%|██▋       | 424/1554 [1:17:34<3:28:31, 11.07s/it] 27%|██▋       | 425/1554 [1:17:45<3:28:17, 11.07s/it] 27%|██▋       | 426/1554 [1:17:56<3:28:54, 11.11s/it] 27%|██▋       | 427/1554 [1:18:07<3:27:54, 11.07s/it] 28%|██▊       | 428/1554 [1:18:18<3:27:22, 11.05s/it] 28%|██▊       | 429/1554 [1:18:29<3:27:04, 11.04s/it] 28%|██▊       | 430/1554 [1:18:40<3:25:58, 10.99s/it]                                                       28%|██▊       | 430/1554 [1:18:40<3:25:58, 10.99s/it] 28%|██▊       | 431/1554 [1:18:51<3:26:17, 11.02s/it] 28%|██▊       | 432/1554 [1:19:02<3:24:41, 10.95s/it] 28%|██▊       | 433/1554 [1:19:13<3:23:36, 10.90s/it] 28%|██▊       | 434/1554 [1:19:24<3:25:00, 10.98s/it] 28%|██▊       | 435/1554 [1:19:35<3:23:38, 10.92s/it] 28%|██▊       | 436/1554 [1:19:45<3:23:36, 10.93s/it] 28%|██▊       | 437/1554 [1:19:57<3:23:56, 10.96s/it] 28%|██▊       | 438/1554 [1:20:08<3:24:21, 10.99s/it] 28%|██▊       | 439/1554 [1:20:19<3:25:25, 11.05s/it] 28%|██▊       | 440/1554 [1:20:30<3:24:45, 11.03s/it]                                                       28%|██▊       | 440/1554 [1:20:30<3:24:45, 11.03s/it] 28%|██▊       | 441/1554 [1:20:41<3:25:37, 11.08s/it] 28%|██▊       | 442/1554 [1:20:52<3:25:20, 11.08s/it] 29%|██▊       | 443/1554 [1:21:03<3:24:18, 11.03s/it] 29%|██▊       | 444/1554 [1:21:14<3:23:01, 10.97s/it] 29%|██▊       | 445/1554 [1:21:25<3:22:28, 10.95s/it] 29%|██▊       | 446/1554 [1:21:36<3:21:59, 10.94s/it] 29%|██▉       | 447/1554 [1:21:47<3:22:39, 10.98s/it] 29%|██▉       | 448/1554 [1:21:58<3:22:31, 10.99s/it] 29%|██▉       | 449/1554 [1:22:09<3:22:23, 10.99s/it] 29%|██▉       | 450/1554 [1:22:20<3:21:28, 10.95s/it]                                                       29%|██▉       | 450/1554 [1:22:20<3:21:28, 10.95s/it] 29%|██▉       | 451/1554 [1:22:31<3:22:28, 11.01s/it] 29%|██▉       | 452/1554 [1:22:42<3:21:50, 10.99s/it] 29%|██▉       | 453/1554 [1:22:53<3:22:17, 11.02s/it] 29%|██▉       | 454/1554 [1:23:04<3:22:52, 11.07s/it] 29%|██▉       | 455/1554 [1:23:15<3:23:27, 11.11s/it] 29%|██▉       | 456/1554 [1:23:26<3:22:55, 11.09s/it] 29%|██▉       | 457/1554 [1:23:37<3:21:25, 11.02s/it] 29%|██▉       | 458/1554 [1:23:48<3:22:06, 11.06s/it] 30%|██▉       | 459/1554 [1:23:59<3:21:21, 11.03s/it] 30%|██▉       | 460/1554 [1:24:10<3:21:08, 11.03s/it]                                                       30%|██▉       | 460/1554 [1:24:10<3:21:08, 11.03s/it] 30%|██▉       | 461/1554 [1:24:21<3:20:12, 10.99s/it] 30%|██▉       | 462/1554 [1:24:32<3:20:59, 11.04s/it] 30%|██▉       | 463/1554 [1:24:43<3:19:56, 11.00s/it] 30%|██▉       | 464/1554 [1:24:54<3:19:22, 10.97s/it] 30%|██▉       | 465/1554 [1:25:05<3:19:25, 10.99s/it] 30%|██▉       | 466/1554 [1:25:16<3:20:04, 11.03s/it] 30%|███       | 467/1554 [1:25:27<3:19:59, 11.04s/it] 30%|███       | 468/1554 [1:25:38<3:20:04, 11.05s/it] 30%|███       | 469/1554 [1:25:49<3:18:52, 11.00s/it] 30%|███       | 470/1554 [1:26:00<3:17:56, 10.96s/it]                                                       30%|███       | 470/1554 [1:26:00<3:17:56, 10.96s/it] 30%|███       | 471/1554 [1:26:11<3:19:14, 11.04s/it] 30%|███       | 472/1554 [1:26:22<3:19:19, 11.05s/it] 30%|███       | 473/1554 [1:26:33<3:17:20, 10.95s/it] 31%|███       | 474/1554 [1:26:44<3:16:55, 10.94s/it] 31%|███       | 475/1554 [1:26:55<3:16:46, 10.94s/it] 31%|███       | 476/1554 [1:27:06<3:16:51, 10.96s/it] 31%|███       | 477/1554 [1:27:17<3:17:16, 10.99s/it] 31%|███       | 478/1554 [1:27:28<3:16:35, 10.96s/it] 31%|███       | 479/1554 [1:27:39<3:16:31, 10.97s/it] 31%|███       | 480/1554 [1:27:50<3:17:30, 11.03s/it]                                                       31%|███       | 480/1554 [1:27:50<3:17:30, 11.03s/it] 31%|███       | 481/1554 [1:28:01<3:16:05, 10.96s/it] 31%|███       | 482/1554 [1:28:12<3:15:35, 10.95s/it] 31%|███       | 483/1554 [1:28:23<3:15:33, 10.96s/it] 31%|███       | 484/1554 [1:28:34<3:16:04, 10.99s/it] 31%|███       | 485/1554 [1:28:45<3:15:55, 11.00s/it] 31%|███▏      | 486/1554 [1:28:56<3:17:11, 11.08s/it] 31%|███▏      | 487/1554 [1:29:07<3:16:36, 11.06s/it] 31%|███▏      | 488/1554 [1:29:18<3:17:46, 11.13s/it] 31%|███▏      | 489/1554 [1:29:29<3:16:40, 11.08s/it] 32%|███▏      | 490/1554 [1:29:40<3:15:00, 11.00s/it]                                                       32%|███▏      | 490/1554 [1:29:40<3:15:00, 11.00s/it] 32%|███▏      | 491/1554 [1:29:51<3:14:44, 10.99s/it] 32%|███▏      | 492/1554 [1:30:02<3:15:54, 11.07s/it] 32%|███▏      | 493/1554 [1:30:13<3:15:00, 11.03s/it] 32%|███▏      | 494/1554 [1:30:24<3:14:30, 11.01s/it] 32%|███▏      | 495/1554 [1:30:35<3:14:22, 11.01s/it] 32%|███▏      | 496/1554 [1:30:46<3:14:02, 11.00s/it] 32%|███▏      | 497/1554 [1:30:57<3:14:01, 11.01s/it] 32%|███▏      | 498/1554 [1:31:08<3:13:44, 11.01s/it] 32%|███▏      | 499/1554 [1:31:20<3:14:35, 11.07s/it] 32%|███▏      | 500/1554 [1:31:31<3:14:22, 11.06s/it]                                                       32%|███▏      | 500/1554 [1:31:31<3:14:22, 11.06s/it][INFO|trainer.py:3819] 2024-09-29 13:03:54,981 >> 
192.168.0.25: ***** Running Evaluation *****
192.168.0.25: [INFO|trainer.py:3821] 2024-09-29 13:03:54,981 >>   Num examples = 14708
192.168.0.25: [INFO|trainer.py:3824] 2024-09-29 13:03:54,981 >>   Batch size = 1
192.168.0.89: [INFO|trainer.py:3819] 2024-09-29 13:03:39,328 >> 
192.168.0.89: ***** Running Evaluation *****
192.168.0.89: [INFO|trainer.py:3821] 2024-09-29 13:03:39,329 >>   Num examples = 14708
192.168.0.89: [INFO|trainer.py:3824] 2024-09-29 13:03:39,329 >>   Batch size = 1
192.168.0.149: [INFO|trainer.py:3819] 2024-09-29 13:03:58,010 >> 
192.168.0.149: ***** Running Evaluation *****
192.168.0.149: [INFO|trainer.py:3821] 2024-09-29 13:03:58,010 >>   Num examples = 14708
192.168.0.149: [INFO|trainer.py:3824] 2024-09-29 13:03:58,010 >>   Batch size = 1
192.168.0.25: 
192.168.0.25:   0%|          | 0/460 [00:00<?, ?it/s][A
192.168.0.25:   0%|          | 2/460 [00:02<08:47,  1.15s/it][A
192.168.0.25:   1%|          | 3/460 [00:04<13:00,  1.71s/it][A
192.168.0.25:   1%|          | 4/460 [00:07<15:14,  2.00s/it][A
192.168.0.25:   1%|          | 5/460 [00:09<16:30,  2.18s/it][A
192.168.0.25:   1%|▏         | 6/460 [00:12<17:17,  2.28s/it][A
192.168.0.25:   2%|▏         | 7/460 [00:14<17:46,  2.35s/it][A
192.168.0.25:   2%|▏         | 8/460 [00:17<18:05,  2.40s/it][A
192.168.0.25:   2%|▏         | 9/460 [00:19<18:17,  2.43s/it][A
192.168.0.25:   2%|▏         | 10/460 [00:22<18:24,  2.45s/it][A
192.168.0.25:   2%|▏         | 11/460 [00:24<18:28,  2.47s/it][A
192.168.0.25:   3%|▎         | 12/460 [00:27<18:30,  2.48s/it][A
192.168.0.25:   3%|▎         | 13/460 [00:29<18:30,  2.49s/it][A
192.168.0.25:   3%|▎         | 14/460 [00:32<18:30,  2.49s/it][A
192.168.0.25:   3%|▎         | 15/460 [00:34<18:30,  2.49s/it][A
192.168.0.25:   3%|▎         | 16/460 [00:37<18:28,  2.50s/it][A
192.168.0.25:   4%|▎         | 17/460 [00:39<18:26,  2.50s/it][A
192.168.0.25:   4%|▍         | 18/460 [00:42<18:24,  2.50s/it][A
192.168.0.25:   4%|▍         | 19/460 [00:44<18:22,  2.50s/it][A
192.168.0.25:   4%|▍         | 20/460 [00:47<18:20,  2.50s/it][A
192.168.0.25:   5%|▍         | 21/460 [00:49<18:17,  2.50s/it][A
192.168.0.25:   5%|▍         | 22/460 [00:52<18:15,  2.50s/it][A
192.168.0.25:   5%|▌         | 23/460 [00:54<18:12,  2.50s/it][A
192.168.0.25:   5%|▌         | 24/460 [00:57<18:10,  2.50s/it][A
192.168.0.25:   5%|▌         | 25/460 [00:59<18:07,  2.50s/it][A
192.168.0.25:   6%|▌         | 26/460 [01:02<18:05,  2.50s/it][A
192.168.0.25:   6%|▌         | 27/460 [01:04<18:02,  2.50s/it][A
192.168.0.25:   6%|▌         | 28/460 [01:07<18:00,  2.50s/it][A
192.168.0.25:   6%|▋         | 29/460 [01:09<17:57,  2.50s/it][A
192.168.0.25:   7%|▋         | 30/460 [01:12<17:55,  2.50s/it][A
192.168.0.25:   7%|▋         | 31/460 [01:14<17:53,  2.50s/it][A
192.168.0.25:   7%|▋         | 32/460 [01:17<17:50,  2.50s/it][A
192.168.0.25:   7%|▋         | 33/460 [01:19<17:48,  2.50s/it][A
192.168.0.25:   7%|▋         | 34/460 [01:22<17:45,  2.50s/it][A
192.168.0.25:   8%|▊         | 35/460 [01:24<17:42,  2.50s/it][A
192.168.0.25:   8%|▊         | 36/460 [01:27<17:40,  2.50s/it][A
192.168.0.25:   8%|▊         | 37/460 [01:29<17:37,  2.50s/it][A
192.168.0.25:   8%|▊         | 38/460 [01:32<17:35,  2.50s/it][A
192.168.0.25:   8%|▊         | 39/460 [01:34<17:32,  2.50s/it][A
192.168.0.25:   9%|▊         | 40/460 [01:37<17:30,  2.50s/it][A
192.168.0.25:   9%|▉         | 41/460 [01:39<17:27,  2.50s/it][A
192.168.0.25:   9%|▉         | 42/460 [01:42<17:25,  2.50s/it][A
192.168.0.25:   9%|▉         | 43/460 [01:44<17:23,  2.50s/it][A
192.168.0.25:  10%|▉         | 44/460 [01:47<17:20,  2.50s/it][A
192.168.0.25:  10%|▉         | 45/460 [01:49<17:17,  2.50s/it][A
192.168.0.25:  10%|█         | 46/460 [01:52<17:15,  2.50s/it][A
192.168.0.25:  10%|█         | 47/460 [01:54<17:12,  2.50s/it][A
192.168.0.25:  10%|█         | 48/460 [01:57<17:10,  2.50s/it][A
192.168.0.25:  11%|█         | 49/460 [01:59<17:07,  2.50s/it][A
192.168.0.25:  11%|█         | 50/460 [02:02<17:06,  2.50s/it][A
192.168.0.25:  11%|█         | 51/460 [02:04<17:03,  2.50s/it][A
192.168.0.25:  11%|█▏        | 52/460 [02:07<17:01,  2.50s/it][A
192.168.0.25:  12%|█▏        | 53/460 [02:09<16:58,  2.50s/it][A
192.168.0.25:  12%|█▏        | 54/460 [02:12<16:55,  2.50s/it][A
192.168.0.25:  12%|█▏        | 55/460 [02:14<16:53,  2.50s/it][A
192.168.0.25:  12%|█▏        | 56/460 [02:17<16:50,  2.50s/it][A
192.168.0.25:  12%|█▏        | 57/460 [02:19<16:48,  2.50s/it][A
192.168.0.25:  13%|█▎        | 58/460 [02:22<16:45,  2.50s/it][A
192.168.0.25:  13%|█▎        | 59/460 [02:24<16:43,  2.50s/it][A
192.168.0.25:  13%|█▎        | 60/460 [02:27<16:40,  2.50s/it][A
192.168.0.25:  13%|█▎        | 61/460 [02:29<16:38,  2.50s/it][A
192.168.0.25:  13%|█▎        | 62/460 [02:32<16:35,  2.50s/it][A
192.168.0.25:  14%|█▎        | 63/460 [02:34<16:33,  2.50s/it][A
192.168.0.25:  14%|█▍        | 64/460 [02:37<16:31,  2.50s/it][A
192.168.0.25:  14%|█▍        | 65/460 [02:39<16:28,  2.50s/it][A
192.168.0.25:  14%|█▍        | 66/460 [02:42<16:26,  2.50s/it][A
192.168.0.25:  15%|█▍        | 67/460 [02:44<16:23,  2.50s/it][A
192.168.0.25:  15%|█▍        | 68/460 [02:47<16:20,  2.50s/it][A
192.168.0.25:  15%|█▌        | 69/460 [02:49<16:18,  2.50s/it][A
192.168.0.25:  15%|█▌        | 70/460 [02:52<16:15,  2.50s/it][A
192.168.0.25:  15%|█▌        | 71/460 [02:54<16:13,  2.50s/it][A
192.168.0.25:  16%|█▌        | 72/460 [02:57<16:10,  2.50s/it][A
192.168.0.25:  16%|█▌        | 73/460 [02:59<16:08,  2.50s/it][A
192.168.0.25:  16%|█▌        | 74/460 [03:02<16:05,  2.50s/it][A
192.168.0.25:  16%|█▋        | 75/460 [03:04<16:03,  2.50s/it][A
192.168.0.25:  17%|█▋        | 76/460 [03:07<16:00,  2.50s/it][A
192.168.0.25:  17%|█▋        | 77/460 [03:09<15:58,  2.50s/it][A
192.168.0.25:  17%|█▋        | 78/460 [03:12<15:55,  2.50s/it][A
192.168.0.25:  17%|█▋        | 79/460 [03:14<15:53,  2.50s/it][A
192.168.0.25:  17%|█▋        | 80/460 [03:17<15:51,  2.50s/it][A
192.168.0.25:  18%|█▊        | 81/460 [03:19<15:48,  2.50s/it][A
192.168.0.25:  18%|█▊        | 82/460 [03:22<15:46,  2.50s/it][A
192.168.0.25:  18%|█▊        | 83/460 [03:24<15:43,  2.50s/it][A
192.168.0.25:  18%|█▊        | 84/460 [03:27<15:41,  2.50s/it][A
192.168.0.25:  18%|█▊        | 85/460 [03:29<15:38,  2.50s/it][A
192.168.0.25:  19%|█▊        | 86/460 [03:32<15:36,  2.50s/it][A
192.168.0.25:  19%|█▉        | 87/460 [03:34<15:33,  2.50s/it][A
192.168.0.25:  19%|█▉        | 88/460 [03:37<15:31,  2.50s/it][A
192.168.0.25:  19%|█▉        | 89/460 [03:39<15:28,  2.50s/it][A
192.168.0.25:  20%|█▉        | 90/460 [03:42<15:25,  2.50s/it][A
192.168.0.25:  20%|█▉        | 91/460 [03:44<15:23,  2.50s/it][A
192.168.0.25:  20%|██        | 92/460 [03:47<15:20,  2.50s/it][A
192.168.0.25:  20%|██        | 93/460 [03:49<15:18,  2.50s/it][A
192.168.0.25:  20%|██        | 94/460 [03:52<15:15,  2.50s/it][A
192.168.0.25:  21%|██        | 95/460 [03:54<15:13,  2.50s/it][A
192.168.0.25:  21%|██        | 96/460 [03:57<15:10,  2.50s/it][A
192.168.0.25:  21%|██        | 97/460 [03:59<15:08,  2.50s/it][A
192.168.0.25:  21%|██▏       | 98/460 [04:02<15:05,  2.50s/it][A
192.168.0.25:  22%|██▏       | 99/460 [04:04<15:03,  2.50s/it][A
192.168.0.25:  22%|██▏       | 100/460 [04:07<15:00,  2.50s/it][A
192.168.0.25:  22%|██▏       | 101/460 [04:09<14:58,  2.50s/it][A
192.168.0.25:  22%|██▏       | 102/460 [04:12<14:55,  2.50s/it][A
192.168.0.25:  22%|██▏       | 103/460 [04:14<14:53,  2.50s/it][A
192.168.0.25:  23%|██▎       | 104/460 [04:17<14:51,  2.50s/it][A
192.168.0.25:  23%|██▎       | 105/460 [04:19<14:48,  2.50s/it][A
192.168.0.25:  23%|██▎       | 106/460 [04:22<14:45,  2.50s/it][A
192.168.0.25:  23%|██▎       | 107/460 [04:24<14:43,  2.50s/it][A
192.168.0.25:  23%|██▎       | 108/460 [04:27<14:40,  2.50s/it][A
192.168.0.25:  24%|██▎       | 109/460 [04:29<14:37,  2.50s/it][A
192.168.0.25:  24%|██▍       | 110/460 [04:32<14:35,  2.50s/it][A
192.168.0.25:  24%|██▍       | 111/460 [04:34<14:32,  2.50s/it][A
192.168.0.25:  24%|██▍       | 112/460 [04:37<14:30,  2.50s/it][A
192.168.0.25:  25%|██▍       | 113/460 [04:39<14:27,  2.50s/it][A
192.168.0.25:  25%|██▍       | 114/460 [04:42<14:25,  2.50s/it][A
192.168.0.25:  25%|██▌       | 115/460 [04:45<14:23,  2.50s/it][A
192.168.0.25:  25%|██▌       | 116/460 [04:47<14:20,  2.50s/it][A
192.168.0.25:  25%|██▌       | 117/460 [04:50<14:18,  2.50s/it][A
192.168.0.25:  26%|██▌       | 118/460 [04:52<14:15,  2.50s/it][A
192.168.0.25:  26%|██▌       | 119/460 [04:55<14:12,  2.50s/it][A
192.168.0.25:  26%|██▌       | 120/460 [04:57<14:10,  2.50s/it][A
192.168.0.25:  26%|██▋       | 121/460 [05:00<14:08,  2.50s/it][A
192.168.0.25:  27%|██▋       | 122/460 [05:02<14:05,  2.50s/it][A
192.168.0.25:  27%|██▋       | 123/460 [05:05<14:02,  2.50s/it][A
192.168.0.25:  27%|██▋       | 124/460 [05:07<14:00,  2.50s/it][A
192.168.0.25:  27%|██▋       | 125/460 [05:10<13:57,  2.50s/it][A
192.168.0.25:  27%|██▋       | 126/460 [05:12<13:55,  2.50s/it][A
192.168.0.25:  28%|██▊       | 127/460 [05:15<13:53,  2.50s/it][A
192.168.0.25:  28%|██▊       | 128/460 [05:17<13:50,  2.50s/it][A
192.168.0.25:  28%|██▊       | 129/460 [05:20<13:47,  2.50s/it][A
192.168.0.25:  28%|██▊       | 130/460 [05:22<13:45,  2.50s/it][A
192.168.0.25:  28%|██▊       | 131/460 [05:25<13:43,  2.50s/it][A
192.168.0.25:  29%|██▊       | 132/460 [05:27<13:40,  2.50s/it][A
192.168.0.25:  29%|██▉       | 133/460 [05:30<13:38,  2.50s/it][A
192.168.0.25:  29%|██▉       | 134/460 [05:32<13:35,  2.50s/it][A
192.168.0.25:  29%|██▉       | 135/460 [05:35<13:32,  2.50s/it][A
192.168.0.25:  30%|██▉       | 136/460 [05:37<13:30,  2.50s/it][A
192.168.0.25:  30%|██▉       | 137/460 [05:40<13:27,  2.50s/it][A
192.168.0.25:  30%|███       | 138/460 [05:42<13:25,  2.50s/it][A
192.168.0.25:  30%|███       | 139/460 [05:45<13:22,  2.50s/it][A
192.168.0.25:  30%|███       | 140/460 [05:47<13:20,  2.50s/it][A
192.168.0.25:  31%|███       | 141/460 [05:50<13:18,  2.50s/it][A
192.168.0.25:  31%|███       | 142/460 [05:52<13:15,  2.50s/it][A
192.168.0.25:  31%|███       | 143/460 [05:55<13:12,  2.50s/it][A
192.168.0.25:  31%|███▏      | 144/460 [05:57<13:10,  2.50s/it][A
192.168.0.25:  32%|███▏      | 145/460 [06:00<13:07,  2.50s/it][A
192.168.0.25:  32%|███▏      | 146/460 [06:02<13:05,  2.50s/it][A
192.168.0.25:  32%|███▏      | 147/460 [06:05<13:02,  2.50s/it][A
192.168.0.25:  32%|███▏      | 148/460 [06:07<13:00,  2.50s/it][A
192.168.0.25:  32%|███▏      | 149/460 [06:10<12:57,  2.50s/it][A
192.168.0.25:  33%|███▎      | 150/460 [06:12<12:55,  2.50s/it][A
192.168.0.25:  33%|███▎      | 151/460 [06:15<12:52,  2.50s/it][A
192.168.0.25:  33%|███▎      | 152/460 [06:17<12:50,  2.50s/it][A
192.168.0.25:  33%|███▎      | 153/460 [06:20<12:48,  2.50s/it][A
192.168.0.25:  33%|███▎      | 154/460 [06:22<12:45,  2.50s/it][A
192.168.0.25:  34%|███▎      | 155/460 [06:25<12:43,  2.50s/it][A
192.168.0.25:  34%|███▍      | 156/460 [06:27<12:40,  2.50s/it][A
192.168.0.25:  34%|███▍      | 157/460 [06:30<12:37,  2.50s/it][A
192.168.0.25:  34%|███▍      | 158/460 [06:32<12:35,  2.50s/it][A
192.168.0.25:  35%|███▍      | 159/460 [06:35<12:32,  2.50s/it][A
192.168.0.25:  35%|███▍      | 160/460 [06:37<12:30,  2.50s/it][A
192.168.0.25:  35%|███▌      | 161/460 [06:40<12:27,  2.50s/it][A
192.168.0.25:  35%|███▌      | 162/460 [06:42<12:25,  2.50s/it][A
192.168.0.25:  35%|███▌      | 163/460 [06:45<12:22,  2.50s/it][A
192.168.0.25:  36%|███▌      | 164/460 [06:47<12:20,  2.50s/it][A
192.168.0.25:  36%|███▌      | 165/460 [06:50<12:17,  2.50s/it][A
192.168.0.25:  36%|███▌      | 166/460 [06:52<12:15,  2.50s/it][A
192.168.0.25:  36%|███▋      | 167/460 [06:55<12:12,  2.50s/it][A
192.168.0.25:  37%|███▋      | 168/460 [06:57<12:10,  2.50s/it][A
192.168.0.25:  37%|███▋      | 169/460 [07:00<12:08,  2.50s/it][A
192.168.0.25:  37%|███▋      | 170/460 [07:02<12:05,  2.50s/it][A
192.168.0.25:  37%|███▋      | 171/460 [07:05<12:03,  2.50s/it][A
192.168.0.25:  37%|███▋      | 172/460 [07:07<12:00,  2.50s/it][A
192.168.0.25:  38%|███▊      | 173/460 [07:10<11:57,  2.50s/it][A
192.168.0.25:  38%|███▊      | 174/460 [07:12<11:55,  2.50s/it][A
192.168.0.25:  38%|███▊      | 175/460 [07:15<11:52,  2.50s/it][A
192.168.0.25:  38%|███▊      | 176/460 [07:17<11:50,  2.50s/it][A
192.168.0.25:  38%|███▊      | 177/460 [07:20<11:48,  2.50s/it][A
192.168.0.25:  39%|███▊      | 178/460 [07:22<11:45,  2.50s/it][A
192.168.0.25:  39%|███▉      | 179/460 [07:25<11:42,  2.50s/it][A
192.168.0.25:  39%|███▉      | 180/460 [07:27<11:39,  2.50s/it][A
192.168.0.25:  39%|███▉      | 181/460 [07:30<11:37,  2.50s/it][A
192.168.0.25:  40%|███▉      | 182/460 [07:32<11:35,  2.50s/it][A
192.168.0.25:  40%|███▉      | 183/460 [07:35<11:32,  2.50s/it][A
192.168.0.25:  40%|████      | 184/460 [07:37<11:29,  2.50s/it][A
192.168.0.25:  40%|████      | 185/460 [07:40<11:27,  2.50s/it][A
192.168.0.25:  40%|████      | 186/460 [07:42<11:25,  2.50s/it][A
192.168.0.25:  41%|████      | 187/460 [07:45<11:23,  2.50s/it][A
192.168.0.25:  41%|████      | 188/460 [07:47<11:20,  2.50s/it][A
192.168.0.25:  41%|████      | 189/460 [07:50<11:18,  2.50s/it][A
192.168.0.25:  41%|████▏     | 190/460 [07:52<11:15,  2.50s/it][A
192.168.0.25:  42%|████▏     | 191/460 [07:55<11:13,  2.50s/it][A
192.168.0.25:  42%|████▏     | 192/460 [07:57<11:10,  2.50s/it][A
192.168.0.25:  42%|████▏     | 193/460 [08:00<11:08,  2.50s/it][A
192.168.0.25:  42%|████▏     | 194/460 [08:02<11:05,  2.50s/it][A
192.168.0.25:  42%|████▏     | 195/460 [08:05<11:02,  2.50s/it][A
192.168.0.25:  43%|████▎     | 196/460 [08:07<11:00,  2.50s/it][A
192.168.0.25:  43%|████▎     | 197/460 [08:10<10:57,  2.50s/it][A
192.168.0.25:  43%|████▎     | 198/460 [08:12<10:55,  2.50s/it][A
192.168.0.25:  43%|████▎     | 199/460 [08:15<10:52,  2.50s/it][A
192.168.0.25:  43%|████▎     | 200/460 [08:17<10:50,  2.50s/it][A
192.168.0.25:  44%|████▎     | 201/460 [08:20<10:48,  2.50s/it][A
192.168.0.25:  44%|████▍     | 202/460 [08:22<10:45,  2.50s/it][A
192.168.0.25:  44%|████▍     | 203/460 [08:25<10:43,  2.50s/it][A
192.168.0.25:  44%|████▍     | 204/460 [08:27<10:40,  2.50s/it][A
192.168.0.25:  45%|████▍     | 205/460 [08:30<10:38,  2.50s/it][A
192.168.0.25:  45%|████▍     | 206/460 [08:32<10:35,  2.50s/it][A
192.168.0.25:  45%|████▌     | 207/460 [08:35<10:33,  2.50s/it][A
192.168.0.25:  45%|████▌     | 208/460 [08:37<10:30,  2.50s/it][A
192.168.0.25:  45%|████▌     | 209/460 [08:40<10:28,  2.50s/it][A
192.168.0.25:  46%|████▌     | 210/460 [08:42<10:25,  2.50s/it][A
192.168.0.25:  46%|████▌     | 211/460 [08:45<10:23,  2.50s/it][A
192.168.0.25:  46%|████▌     | 212/460 [08:47<10:20,  2.50s/it][A
192.168.0.25:  46%|████▋     | 213/460 [08:50<10:18,  2.50s/it][A
192.168.0.25:  47%|████▋     | 214/460 [08:52<10:15,  2.50s/it][A
192.168.0.25:  47%|████▋     | 215/460 [08:55<10:13,  2.50s/it][A
192.168.0.25:  47%|████▋     | 216/460 [08:57<10:10,  2.50s/it][A
192.168.0.25:  47%|████▋     | 217/460 [09:00<10:07,  2.50s/it][A
192.168.0.25:  47%|████▋     | 218/460 [09:02<10:05,  2.50s/it][A
192.168.0.25:  48%|████▊     | 219/460 [09:05<10:02,  2.50s/it][A
192.168.0.25:  48%|████▊     | 220/460 [09:07<10:00,  2.50s/it][A
192.168.0.25:  48%|████▊     | 221/460 [09:10<09:57,  2.50s/it][A
192.168.0.25:  48%|████▊     | 222/460 [09:12<09:55,  2.50s/it][A
192.168.0.25:  48%|████▊     | 223/460 [09:15<09:52,  2.50s/it][A
192.168.0.25:  49%|████▊     | 224/460 [09:17<09:50,  2.50s/it][A
192.168.0.25:  49%|████▉     | 225/460 [09:20<09:47,  2.50s/it][A
192.168.0.25:  49%|████▉     | 226/460 [09:22<09:45,  2.50s/it][A
192.168.0.25:  49%|████▉     | 227/460 [09:25<09:43,  2.50s/it][A
192.168.0.25:  50%|████▉     | 228/460 [09:27<09:40,  2.50s/it][A
192.168.0.25:  50%|████▉     | 229/460 [09:30<09:38,  2.50s/it][A
192.168.0.25:  50%|█████     | 230/460 [09:32<09:36,  2.51s/it][A
192.168.0.25:  50%|█████     | 231/460 [09:35<09:33,  2.50s/it][A
192.168.0.25:  50%|█████     | 232/460 [09:37<09:31,  2.50s/it][A
192.168.0.25:  51%|█████     | 233/460 [09:40<09:28,  2.51s/it][A
192.168.0.25:  51%|█████     | 234/460 [09:42<09:26,  2.51s/it][A
192.168.0.25:  51%|█████     | 235/460 [09:45<09:24,  2.51s/it][A
192.168.0.25:  51%|█████▏    | 236/460 [09:47<09:21,  2.51s/it][A
192.168.0.25:  52%|█████▏    | 237/460 [09:50<09:18,  2.50s/it][A
192.168.0.25:  52%|█████▏    | 238/460 [09:52<09:15,  2.50s/it][A
192.168.0.25:  52%|█████▏    | 239/460 [09:55<09:13,  2.50s/it][A
192.168.0.25:  52%|█████▏    | 240/460 [09:57<09:10,  2.50s/it][A
192.168.0.25:  52%|█████▏    | 241/460 [10:00<09:07,  2.50s/it][A
192.168.0.25:  53%|█████▎    | 242/460 [10:02<09:05,  2.50s/it][A
192.168.0.25:  53%|█████▎    | 243/460 [10:05<09:02,  2.50s/it][A
192.168.0.25:  53%|█████▎    | 244/460 [10:07<09:00,  2.50s/it][A
192.168.0.25:  53%|█████▎    | 245/460 [10:10<08:57,  2.50s/it][A
192.168.0.25:  53%|█████▎    | 246/460 [10:12<08:54,  2.50s/it][A
192.168.0.25:  54%|█████▎    | 247/460 [10:15<08:52,  2.50s/it][A
192.168.0.25:  54%|█████▍    | 248/460 [10:17<08:50,  2.50s/it][A
192.168.0.25:  54%|█████▍    | 249/460 [10:20<08:47,  2.50s/it][A
192.168.0.25:  54%|█████▍    | 250/460 [10:22<08:45,  2.50s/it][A
192.168.0.25:  55%|█████▍    | 251/460 [10:25<08:42,  2.50s/it][A
192.168.0.25:  55%|█████▍    | 252/460 [10:27<08:39,  2.50s/it][A
192.168.0.25:  55%|█████▌    | 253/460 [10:30<08:37,  2.50s/it][A
192.168.0.25:  55%|█████▌    | 254/460 [10:32<08:34,  2.50s/it][A
192.168.0.25:  55%|█████▌    | 255/460 [10:35<08:32,  2.50s/it][A
192.168.0.25:  56%|█████▌    | 256/460 [10:37<08:30,  2.50s/it][A
192.168.0.25:  56%|█████▌    | 257/460 [10:40<08:27,  2.50s/it][A
192.168.0.25:  56%|█████▌    | 258/460 [10:42<08:25,  2.50s/it][A
192.168.0.25:  56%|█████▋    | 259/460 [10:45<08:22,  2.50s/it][A
192.168.0.25:  57%|█████▋    | 260/460 [10:47<08:20,  2.50s/it][A
192.168.0.25:  57%|█████▋    | 261/460 [10:50<08:17,  2.50s/it][A
192.168.0.25:  57%|█████▋    | 262/460 [10:52<08:15,  2.50s/it][A
192.168.0.25:  57%|█████▋    | 263/460 [10:55<08:12,  2.50s/it][A
192.168.0.25:  57%|█████▋    | 264/460 [10:57<08:10,  2.50s/it][A
192.168.0.25:  58%|█████▊    | 265/460 [11:00<08:07,  2.50s/it][A
192.168.0.25:  58%|█████▊    | 266/460 [11:02<08:05,  2.50s/it][A
192.168.0.25:  58%|█████▊    | 267/460 [11:05<08:02,  2.50s/it][A
192.168.0.25:  58%|█████▊    | 268/460 [11:07<08:00,  2.50s/it][A
192.168.0.25:  58%|█████▊    | 269/460 [11:10<07:57,  2.50s/it][A
192.168.0.25:  59%|█████▊    | 270/460 [11:12<07:55,  2.50s/it][A
192.168.0.25:  59%|█████▉    | 271/460 [11:15<07:52,  2.50s/it][A
192.168.0.25:  59%|█████▉    | 272/460 [11:17<07:50,  2.50s/it][A
192.168.0.25:  59%|█████▉    | 273/460 [11:20<07:47,  2.50s/it][A
192.168.0.25:  60%|█████▉    | 274/460 [11:22<07:45,  2.50s/it][A
192.168.0.25:  60%|█████▉    | 275/460 [11:25<07:42,  2.50s/it][A
192.168.0.25:  60%|██████    | 276/460 [11:27<07:40,  2.50s/it][A
192.168.0.25:  60%|██████    | 277/460 [11:30<07:38,  2.50s/it][A
192.168.0.25:  60%|██████    | 278/460 [11:32<07:35,  2.50s/it][A
192.168.0.25:  61%|██████    | 279/460 [11:35<07:32,  2.50s/it][A
192.168.0.25:  61%|██████    | 280/460 [11:37<07:30,  2.50s/it][A
192.168.0.25:  61%|██████    | 281/460 [11:40<07:27,  2.50s/it][A
192.168.0.25:  61%|██████▏   | 282/460 [11:42<07:25,  2.50s/it][A
192.168.0.25:  62%|██████▏   | 283/460 [11:45<07:22,  2.50s/it][A
192.168.0.25:  62%|██████▏   | 284/460 [11:47<07:20,  2.50s/it][A
192.168.0.25:  62%|██████▏   | 285/460 [11:50<07:17,  2.50s/it][A
192.168.0.25:  62%|██████▏   | 286/460 [11:52<07:14,  2.50s/it][A
192.168.0.25:  62%|██████▏   | 287/460 [11:55<07:12,  2.50s/it][A
192.168.0.25:  63%|██████▎   | 288/460 [11:57<07:10,  2.50s/it][A
192.168.0.25:  63%|██████▎   | 289/460 [12:00<07:07,  2.50s/it][A
192.168.0.25:  63%|██████▎   | 290/460 [12:02<07:05,  2.50s/it][A
192.168.0.25:  63%|██████▎   | 291/460 [12:05<07:02,  2.50s/it][A
192.168.0.25:  63%|██████▎   | 292/460 [12:07<07:00,  2.50s/it][A
192.168.0.25:  64%|██████▎   | 293/460 [12:10<06:57,  2.50s/it][A
192.168.0.25:  64%|██████▍   | 294/460 [12:12<06:55,  2.50s/it][A
192.168.0.25:  64%|██████▍   | 295/460 [12:15<06:52,  2.50s/it][A
192.168.0.25:  64%|██████▍   | 296/460 [12:17<06:50,  2.50s/it][A
192.168.0.25:  65%|██████▍   | 297/460 [12:20<06:47,  2.50s/it][A
192.168.0.25:  65%|██████▍   | 298/460 [12:22<06:45,  2.50s/it][A
192.168.0.25:  65%|██████▌   | 299/460 [12:25<06:42,  2.50s/it][A
192.168.0.25:  65%|██████▌   | 300/460 [12:27<06:40,  2.50s/it][A
192.168.0.25:  65%|██████▌   | 301/460 [12:30<06:37,  2.50s/it][A
192.168.0.25:  66%|██████▌   | 302/460 [12:32<06:35,  2.50s/it][A
192.168.0.25:  66%|██████▌   | 303/460 [12:35<06:32,  2.50s/it][A
192.168.0.25:  66%|██████▌   | 304/460 [12:37<06:30,  2.50s/it][A
192.168.0.25:  66%|██████▋   | 305/460 [12:40<06:27,  2.50s/it][A
192.168.0.25:  67%|██████▋   | 306/460 [12:42<06:25,  2.50s/it][A
192.168.0.25:  67%|██████▋   | 307/460 [12:45<06:22,  2.50s/it][A
192.168.0.25:  67%|██████▋   | 308/460 [12:47<06:20,  2.50s/it][A
192.168.0.25:  67%|██████▋   | 309/460 [12:50<06:17,  2.50s/it][A
192.168.0.25:  67%|██████▋   | 310/460 [12:52<06:15,  2.50s/it][A
192.168.0.25:  68%|██████▊   | 311/460 [12:55<06:12,  2.50s/it][A
192.168.0.25:  68%|██████▊   | 312/460 [12:57<06:10,  2.50s/it][A
192.168.0.25:  68%|██████▊   | 313/460 [13:00<06:07,  2.50s/it][A
192.168.0.25:  68%|██████▊   | 314/460 [13:02<06:05,  2.50s/it][A
192.168.0.25:  68%|██████▊   | 315/460 [13:05<06:02,  2.50s/it][A
192.168.0.25:  69%|██████▊   | 316/460 [13:07<06:00,  2.50s/it][A
192.168.0.25:  69%|██████▉   | 317/460 [13:10<05:57,  2.50s/it][A
192.168.0.25:  69%|██████▉   | 318/460 [13:12<05:55,  2.50s/it][A
192.168.0.25:  69%|██████▉   | 319/460 [13:15<05:52,  2.50s/it][A
192.168.0.25:  70%|██████▉   | 320/460 [13:17<05:50,  2.50s/it][A
192.168.0.25:  70%|██████▉   | 321/460 [13:20<05:47,  2.50s/it][A
192.168.0.25:  70%|███████   | 322/460 [13:22<05:45,  2.50s/it][A
192.168.0.25:  70%|███████   | 323/460 [13:25<05:42,  2.50s/it][A
192.168.0.25:  70%|███████   | 324/460 [13:27<05:40,  2.50s/it][A
192.168.0.25:  71%|███████   | 325/460 [13:30<05:37,  2.50s/it][A
192.168.0.25:  71%|███████   | 326/460 [13:32<05:35,  2.50s/it][A
192.168.0.25:  71%|███████   | 327/460 [13:35<05:32,  2.50s/it][A
192.168.0.25:  71%|███████▏  | 328/460 [13:37<05:30,  2.50s/it][A
192.168.0.25:  72%|███████▏  | 329/460 [13:40<05:27,  2.50s/it][A
192.168.0.25:  72%|███████▏  | 330/460 [13:42<05:25,  2.50s/it][A
192.168.0.25:  72%|███████▏  | 331/460 [13:45<05:22,  2.50s/it][A
192.168.0.25:  72%|███████▏  | 332/460 [13:47<05:20,  2.50s/it][A
192.168.0.25:  72%|███████▏  | 333/460 [13:50<05:17,  2.50s/it][A
192.168.0.25:  73%|███████▎  | 334/460 [13:52<05:15,  2.50s/it][A
192.168.0.25:  73%|███████▎  | 335/460 [13:55<05:12,  2.50s/it][A
192.168.0.25:  73%|███████▎  | 336/460 [13:57<05:10,  2.50s/it][A
192.168.0.25:  73%|███████▎  | 337/460 [14:00<05:07,  2.50s/it][A
192.168.0.25:  73%|███████▎  | 338/460 [14:02<05:05,  2.50s/it][A
192.168.0.25:  74%|███████▎  | 339/460 [14:05<05:02,  2.50s/it][A
192.168.0.25:  74%|███████▍  | 340/460 [14:07<05:00,  2.50s/it][A
192.168.0.25:  74%|███████▍  | 341/460 [14:10<04:57,  2.50s/it][A
192.168.0.25:  74%|███████▍  | 342/460 [14:12<04:55,  2.50s/it][A
192.168.0.25:  75%|███████▍  | 343/460 [14:15<04:52,  2.50s/it][A
192.168.0.25:  75%|███████▍  | 344/460 [14:17<04:50,  2.50s/it][A
192.168.0.25:  75%|███████▌  | 345/460 [14:20<04:47,  2.50s/it][A
192.168.0.25:  75%|███████▌  | 346/460 [14:22<04:45,  2.50s/it][A
192.168.0.25:  75%|███████▌  | 347/460 [14:25<04:42,  2.50s/it][A
192.168.0.25:  76%|███████▌  | 348/460 [14:27<04:40,  2.50s/it][A
192.168.0.25:  76%|███████▌  | 349/460 [14:30<04:37,  2.50s/it][A
192.168.0.25:  76%|███████▌  | 350/460 [14:32<04:35,  2.50s/it][A
192.168.0.25:  76%|███████▋  | 351/460 [14:35<04:32,  2.50s/it][A
192.168.0.25:  77%|███████▋  | 352/460 [14:37<04:30,  2.50s/it][A
192.168.0.25:  77%|███████▋  | 353/460 [14:40<04:27,  2.50s/it][A
192.168.0.25:  77%|███████▋  | 354/460 [14:42<04:25,  2.50s/it][A
192.168.0.25:  77%|███████▋  | 355/460 [14:45<04:22,  2.50s/it][A
192.168.0.25:  77%|███████▋  | 356/460 [14:47<04:20,  2.50s/it][A
192.168.0.25:  78%|███████▊  | 357/460 [14:50<04:17,  2.50s/it][A
192.168.0.25:  78%|███████▊  | 358/460 [14:52<04:15,  2.50s/it][A
192.168.0.25:  78%|███████▊  | 359/460 [14:55<04:12,  2.50s/it][A
192.168.0.25:  78%|███████▊  | 360/460 [14:57<04:10,  2.50s/it][A
192.168.0.25:  78%|███████▊  | 361/460 [15:00<04:07,  2.50s/it][A
192.168.0.25:  79%|███████▊  | 362/460 [15:02<04:05,  2.50s/it][A
192.168.0.25:  79%|███████▉  | 363/460 [15:05<04:02,  2.50s/it][A
192.168.0.25:  79%|███████▉  | 364/460 [15:07<04:00,  2.50s/it][A
192.168.0.25:  79%|███████▉  | 365/460 [15:10<03:57,  2.50s/it][A
192.168.0.25:  80%|███████▉  | 366/460 [15:12<03:55,  2.50s/it][A
192.168.0.25:  80%|███████▉  | 367/460 [15:15<03:52,  2.50s/it][A
192.168.0.25:  80%|████████  | 368/460 [15:17<03:50,  2.50s/it][A
192.168.0.25:  80%|████████  | 369/460 [15:20<03:47,  2.50s/it][A
192.168.0.25:  80%|████████  | 370/460 [15:22<03:45,  2.50s/it][A
192.168.0.25:  81%|████████  | 371/460 [15:25<03:42,  2.50s/it][A
192.168.0.25:  81%|████████  | 372/460 [15:27<03:40,  2.50s/it][A
192.168.0.25:  81%|████████  | 373/460 [15:30<03:37,  2.50s/it][A
192.168.0.25:  81%|████████▏ | 374/460 [15:32<03:35,  2.50s/it][A
192.168.0.25:  82%|████████▏ | 375/460 [15:35<03:32,  2.50s/it][A
192.168.0.25:  82%|████████▏ | 376/460 [15:37<03:30,  2.50s/it][A
192.168.0.25:  82%|████████▏ | 377/460 [15:40<03:27,  2.50s/it][A
192.168.0.25:  82%|████████▏ | 378/460 [15:42<03:25,  2.50s/it][A
192.168.0.25:  82%|████████▏ | 379/460 [15:45<03:22,  2.50s/it][A
192.168.0.25:  83%|████████▎ | 380/460 [15:47<03:20,  2.50s/it][A
192.168.0.25:  83%|████████▎ | 381/460 [15:50<03:17,  2.50s/it][A
192.168.0.25:  83%|████████▎ | 382/460 [15:52<03:15,  2.50s/it][A
192.168.0.25:  83%|████████▎ | 383/460 [15:55<03:12,  2.50s/it][A
192.168.0.25:  83%|████████▎ | 384/460 [15:57<03:10,  2.50s/it][A
192.168.0.25:  84%|████████▎ | 385/460 [16:00<03:07,  2.50s/it][A
192.168.0.25:  84%|████████▍ | 386/460 [16:02<03:05,  2.50s/it][A
192.168.0.25:  84%|████████▍ | 387/460 [16:05<03:02,  2.50s/it][A
192.168.0.25:  84%|████████▍ | 388/460 [16:07<03:00,  2.50s/it][A
192.168.0.25:  85%|████████▍ | 389/460 [16:10<02:57,  2.50s/it][A
192.168.0.25:  85%|████████▍ | 390/460 [16:12<02:55,  2.50s/it][A
192.168.0.25:  85%|████████▌ | 391/460 [16:15<02:52,  2.50s/it][A
192.168.0.25:  85%|████████▌ | 392/460 [16:17<02:50,  2.50s/it][A
192.168.0.25:  85%|████████▌ | 393/460 [16:20<02:47,  2.50s/it][A
192.168.0.25:  86%|████████▌ | 394/460 [16:22<02:45,  2.50s/it][A
192.168.0.25:  86%|████████▌ | 395/460 [16:25<02:42,  2.50s/it][A
192.168.0.25:  86%|████████▌ | 396/460 [16:27<02:40,  2.50s/it][A
192.168.0.25:  86%|████████▋ | 397/460 [16:30<02:37,  2.50s/it][A
192.168.0.25:  87%|████████▋ | 398/460 [16:32<02:35,  2.50s/it][A
192.168.0.25:  87%|████████▋ | 399/460 [16:35<02:32,  2.50s/it][A
192.168.0.25:  87%|████████▋ | 400/460 [16:37<02:30,  2.50s/it][A
192.168.0.25:  87%|████████▋ | 401/460 [16:40<02:27,  2.50s/it][A
192.168.0.25:  87%|████████▋ | 402/460 [16:42<02:25,  2.50s/it][A
192.168.0.25:  88%|████████▊ | 403/460 [16:45<02:22,  2.50s/it][A
192.168.0.25:  88%|████████▊ | 404/460 [16:47<02:20,  2.50s/it][A
192.168.0.25:  88%|████████▊ | 405/460 [16:50<02:17,  2.50s/it][A
192.168.0.25:  88%|████████▊ | 406/460 [16:52<02:15,  2.50s/it][A
192.168.0.25:  88%|████████▊ | 407/460 [16:55<02:12,  2.50s/it][A
192.168.0.25:  89%|████████▊ | 408/460 [16:57<02:10,  2.50s/it][A
192.168.0.25:  89%|████████▉ | 409/460 [17:00<02:07,  2.50s/it][A
192.168.0.25:  89%|████████▉ | 410/460 [17:02<02:05,  2.50s/it][A
192.168.0.25:  89%|████████▉ | 411/460 [17:05<02:02,  2.50s/it][A
192.168.0.25:  90%|████████▉ | 412/460 [17:07<02:00,  2.50s/it][A
192.168.0.25:  90%|████████▉ | 413/460 [17:10<01:57,  2.50s/it][A
192.168.0.25:  90%|█████████ | 414/460 [17:12<01:55,  2.50s/it][A
192.168.0.25:  90%|█████████ | 415/460 [17:15<01:52,  2.50s/it][A
192.168.0.25:  90%|█████████ | 416/460 [17:17<01:50,  2.50s/it][A
192.168.0.25:  91%|█████████ | 417/460 [17:20<01:47,  2.50s/it][A
192.168.0.25:  91%|█████████ | 418/460 [17:22<01:45,  2.50s/it][A
192.168.0.25:  91%|█████████ | 419/460 [17:25<01:42,  2.50s/it][A
192.168.0.25:  91%|█████████▏| 420/460 [17:27<01:40,  2.50s/it][A
192.168.0.25:  92%|█████████▏| 421/460 [17:30<01:37,  2.50s/it][A
192.168.0.25:  92%|█████████▏| 422/460 [17:32<01:35,  2.50s/it][A
192.168.0.25:  92%|█████████▏| 423/460 [17:35<01:32,  2.50s/it][A
192.168.0.25:  92%|█████████▏| 424/460 [17:37<01:30,  2.50s/it][A
192.168.0.25:  92%|█████████▏| 425/460 [17:40<01:27,  2.50s/it][A
192.168.0.25:  93%|█████████▎| 426/460 [17:42<01:25,  2.50s/it][A
192.168.0.25:  93%|█████████▎| 427/460 [17:45<01:22,  2.50s/it][A
192.168.0.25:  93%|█████████▎| 428/460 [17:47<01:20,  2.50s/it][A
192.168.0.25:  93%|█████████▎| 429/460 [17:50<01:17,  2.50s/it][A
192.168.0.25:  93%|█████████▎| 430/460 [17:52<01:15,  2.50s/it][A
192.168.0.25:  94%|█████████▎| 431/460 [17:55<01:12,  2.50s/it][A
192.168.0.25:  94%|█████████▍| 432/460 [17:57<01:10,  2.50s/it][A
192.168.0.25:  94%|█████████▍| 433/460 [18:00<01:07,  2.50s/it][A
192.168.0.25:  94%|█████████▍| 434/460 [18:02<01:05,  2.50s/it][A
192.168.0.25:  95%|█████████▍| 435/460 [18:05<01:02,  2.50s/it][A
192.168.0.25:  95%|█████████▍| 436/460 [18:07<01:00,  2.50s/it][A
192.168.0.25:  95%|█████████▌| 437/460 [18:10<00:57,  2.50s/it][A
192.168.0.25:  95%|█████████▌| 438/460 [18:12<00:55,  2.50s/it][A
192.168.0.25:  95%|█████████▌| 439/460 [18:15<00:52,  2.50s/it][A
192.168.0.25:  96%|█████████▌| 440/460 [18:17<00:50,  2.50s/it][A
192.168.0.25:  96%|█████████▌| 441/460 [18:20<00:47,  2.50s/it][A
192.168.0.25:  96%|█████████▌| 442/460 [18:22<00:45,  2.50s/it][A
192.168.0.25:  96%|█████████▋| 443/460 [18:25<00:42,  2.50s/it][A
192.168.0.25:  97%|█████████▋| 444/460 [18:27<00:40,  2.50s/it][A
192.168.0.25:  97%|█████████▋| 445/460 [18:30<00:37,  2.50s/it][A
192.168.0.25:  97%|█████████▋| 446/460 [18:32<00:35,  2.50s/it][A
192.168.0.25:  97%|█████████▋| 447/460 [18:35<00:32,  2.50s/it][A
192.168.0.25:  97%|█████████▋| 448/460 [18:37<00:30,  2.50s/it][A
192.168.0.25:  98%|█████████▊| 449/460 [18:40<00:27,  2.50s/it][A
192.168.0.25:  98%|█████████▊| 450/460 [18:43<00:25,  2.50s/it][A
192.168.0.25:  98%|█████████▊| 451/460 [18:45<00:22,  2.50s/it][A
192.168.0.25:  98%|█████████▊| 452/460 [18:48<00:20,  2.50s/it][A
192.168.0.25:  98%|█████████▊| 453/460 [18:50<00:17,  2.50s/it][A
192.168.0.25:  99%|█████████▊| 454/460 [18:53<00:15,  2.50s/it][A
192.168.0.25:  99%|█████████▉| 455/460 [18:55<00:12,  2.50s/it][A
192.168.0.25:  99%|█████████▉| 456/460 [18:58<00:10,  2.50s/it][A
192.168.0.25:  99%|█████████▉| 457/460 [19:00<00:07,  2.50s/it][A
192.168.0.25: 100%|█████████▉| 458/460 [19:03<00:05,  2.50s/it][A
192.168.0.25: 100%|█████████▉| 459/460 [19:05<00:02,  2.50s/it][A
192.168.0.25: 100%|██████████| 460/460 [19:08<00:00,  2.50s/it][A                                                      
192.168.0.25: {'eval_loss': 3.5650503635406494, 'eval_runtime': 1150.8054, 'eval_samples_per_second': 12.781, 'eval_steps_per_second': 0.4, 'epoch': 0.97}
192.168.0.25:                                                  [A 32%|███▏      | 500/1554 [1:50:41<3:14:22, 11.06s/it]
192.168.0.25: 100%|██████████| 460/460 [19:08<00:00,  2.50s/it][A
192.168.0.25: {'loss': 3.5569, 'grad_norm': 5.665999803386217, 'learning_rate': 7.686737932393606e-06, 'epoch': 0.98}
192.168.0.25: {'loss': 3.5604, 'grad_norm': 6.616923753690849, 'learning_rate': 7.599821480713571e-06, 'epoch': 1.0}
192.168.0.25: {'loss': 3.5149, 'grad_norm': 4.060962796985584, 'learning_rate': 7.511814650294994e-06, 'epoch': 1.02}
192.168.0.25: {'loss': 3.5057, 'grad_norm': 5.170921399116866, 'learning_rate': 7.422754351663252e-06, 'epoch': 1.04}
192.168.0.25: {'loss': 3.4982, 'grad_norm': 6.378168851859602, 'learning_rate': 7.3326779371738e-06, 'epoch': 1.06}
192.168.0.25: {'loss': 3.4903, 'grad_norm': 5.166246007386591, 'learning_rate': 7.241623185346409e-06, 'epoch': 1.08}
192.168.0.25: {'loss': 3.4808, 'grad_norm': 4.163213417844972, 'learning_rate': 7.149628285020647e-06, 'epoch': 1.1}
192.168.0.25: {'loss': 3.4828, 'grad_norm': 4.9257645996199555, 'learning_rate': 7.056731819339287e-06, 'epoch': 1.12}
192.168.0.25: {'loss': 3.4708, 'grad_norm': 4.982049381533574, 'learning_rate': 6.9629727495663265e-06, 'epoch': 1.14}
192.168.0.25: {'loss': 3.4755, 'grad_norm': 7.414659321341912, 'learning_rate': 6.86839039874644e-06, 'epoch': 1.16}
192.168.0.25: {'loss': 3.4837, 'grad_norm': 5.247482786254855, 'learning_rate': 6.773024435212678e-06, 'epoch': 1.18}
192.168.0.25: {'loss': 3.4698, 'grad_norm': 4.127977394855321, 'learning_rate': 6.676914855949372e-06, 'epoch': 1.2}
192.168.0.25: {'loss': 3.4546, 'grad_norm': 4.996309062502727, 'learning_rate': 6.580101969817176e-06, 'epoch': 1.22}
192.168.0.25: {'loss': 3.4652, 'grad_norm': 4.386955927401136, 'learning_rate': 6.48262638064733e-06, 'epoch': 1.24}
192.168.0.25: {'loss': 3.4575, 'grad_norm': 4.390716660381087, 'learning_rate': 6.384528970212196e-06, 'epoch': 1.25}
192.168.0.25: {'loss': 3.4322, 'grad_norm': 7.348089226676764, 'learning_rate': 6.285850881079229e-06, 'epoch': 1.27}
192.168.0.25: {'loss': 3.4327, 'grad_norm': 5.771203792354817, 'learning_rate': 6.186633499355576e-06, 'epoch': 1.29}
192.168.0.25: {'loss': 3.4367, 'grad_norm': 5.623743891868488, 'learning_rate': 6.086918437330508e-06, 'epoch': 1.31}
192.168.0.25: {'loss': 3.4105, 'grad_norm': 5.15103625640225, 'learning_rate': 5.986747516023031e-06, 'epoch': 1.33}
192.168.0.25: {'loss': 3.4085, 'grad_norm': 3.9146924149665177, 'learning_rate': 5.886162747641912e-06, 'epoch': 1.35}
192.168.0.25: {'loss': 3.4234, 'grad_norm': 4.345643433578811, 'learning_rate': 5.785206317965554e-06, 'epoch': 1.37}
192.168.0.25: {'loss': 3.3878, 'grad_norm': 7.347585642271059, 'learning_rate': 5.6839205686490474e-06, 'epoch': 1.39}
192.168.0.25: {'loss': 3.4023, 'grad_norm': 4.626240950428425, 'learning_rate': 5.582347979465864e-06, 'epoch': 1.41}
192.168.0.25: {'loss': 3.391, 'grad_norm': 4.282501928681999, 'learning_rate': 5.480531150491622e-06, 'epoch': 1.43}
192.168.0.25: {'loss': 3.3707, 'grad_norm': 3.2221963450244107, 'learning_rate': 5.378512784237382e-06, 'epoch': 1.45}
192.168.0.25: {'loss': 3.388, 'grad_norm': 3.5963166732362124, 'learning_rate': 5.276335667739998e-06, 'epoch': 1.47}
192.168.0.25: {'loss': 3.3613, 'grad_norm': 3.8305922673302457, 'learning_rate': 5.174042654617001e-06, 'epoch': 1.49}
192.168.0.25: {'loss': 3.3734, 'grad_norm': 5.0115813595499645, 'learning_rate': 5.071676647093581e-06, 'epoch': 1.51}
192.168.0.25: {'loss': 3.3759, 'grad_norm': 3.0133422750436005, 'learning_rate': 4.969280578009157e-06, 'epoch': 1.53}
192.168.0.25: {'loss': 3.388, 'grad_norm': 4.457946950741451, 'learning_rate': 4.866897392811127e-06, 'epoch': 1.54}
192.168.0.25: {'loss': 3.373, 'grad_norm': 4.954336698077748, 'learning_rate': 4.764570031543316e-06, 'epoch': 1.56}
192.168.0.25: {'loss': 3.3688, 'grad_norm': 6.779232445764855, 'learning_rate': 4.662341410836703e-06, 'epoch': 1.58}
192.168.0.25: {'loss': 3.3393, 'grad_norm': 4.301161303172458, 'learning_rate': 4.560254405909959e-06, 'epoch': 1.6}
192.168.0.25: {'loss': 3.3566, 'grad_norm': 2.967417046304206, 'learning_rate': 4.458351832587354e-06, 'epoch': 1.62}
192.168.0.25: {'loss': 3.352, 'grad_norm': 4.773088857652033, 'learning_rate': 4.356676429341577e-06, 'epoch': 1.64}
192.168.0.25: {'loss': 3.3499, 'grad_norm': 4.091886838957308, 'learning_rate': 4.2552708393690035e-06, 'epoch': 1.66}
192.168.0.25: {'loss': 3.3326, 'grad_norm': 3.508179371431317, 'learning_rate': 4.154177592704902e-06, 'epoch': 1.68}
192.168.0.25: {'loss': 3.3267, 'grad_norm': 2.6589898732692987, 'learning_rate': 4.053439088386124e-06, 'epoch': 1.7}
192.168.0.25: {'loss': 3.3189, 'grad_norm': 3.66900223704842, 'learning_rate': 3.95309757666873e-06, 'epoch': 1.72}
192.168.0.25: {'loss': 3.3359, 'grad_norm': 6.70132215748857, 'learning_rate': 3.853195141308001e-06, 'epoch': 1.74}
192.168.0.25: {'loss': 3.312, 'grad_norm': 4.320889480996063, 'learning_rate': 3.7537736819082926e-06, 'epoch': 1.76}
192.168.0.25: {'loss': 3.3327, 'grad_norm': 5.07621245534953, 'learning_rate': 3.6548748963501324e-06, 'epoch': 1.78}
192.168.0.25: {'loss': 3.3321, 'grad_norm': 3.661176183183694, 'learning_rate': 3.5565402633018963e-06, 'epoch': 1.8}
192.168.0.25: {'loss': 3.3073, 'grad_norm': 5.431792146338123, 'learning_rate': 3.458811024823444e-06, 'epoch': 1.81}
192.168.0.25: {'loss': 3.2919, 'grad_norm': 5.062735469663709, 'learning_rate': 3.3617281690689895e-06, 'epoch': 1.83}
192.168.0.25: {'loss': 3.2891, 'grad_norm': 5.873042977842072, 'learning_rate': 3.265332413096445e-06, 'epoch': 1.85}
192.168.0.25: {'loss': 3.3109, 'grad_norm': 4.195077445550907, 'learning_rate': 3.1696641857904743e-06, 'epoch': 1.87}
192.168.0.25: {'loss': 3.2767, 'grad_norm': 2.849369240278249, 'learning_rate': 3.0747636109064126e-06, 'epoch': 1.89}
192.168.0.25: {'loss': 3.2843, 'grad_norm': 3.2549431695386435, 'learning_rate': 2.9806704902421557e-06, 'epoch': 1.91}
192.168.0.25: {'loss': 3.3006, 'grad_norm': 2.1530495817266475, 'learning_rate': 2.8874242869450655e-06, 'epoch': 1.93}
192.168.0.149: [INFO|trainer.py:3819] 2024-09-29 14:51:32,743 >> 
192.168.0.149: ***** Running Evaluation *****
192.168.0.149: [INFO|trainer.py:3821] 2024-09-29 14:51:32,744 >>   Num examples = 14708
192.168.0.149: [INFO|trainer.py:3824] 2024-09-29 14:51:32,744 >>   Batch size = 1
192.168.0.89: [INFO|trainer.py:3819] 2024-09-29 14:51:14,035 >> 
192.168.0.89: ***** Running Evaluation *****
192.168.0.89: [INFO|trainer.py:3821] 2024-09-29 14:51:14,035 >>   Num examples = 14708
192.168.0.89: [INFO|trainer.py:3824] 2024-09-29 14:51:14,035 >>   Batch size = 1
192.168.0.25:                                                  [A 32%|███▏      | 501/1554 [1:50:51<104:05:52, 355.89s/it] 32%|███▏      | 502/1554 [1:51:01<73:38:39, 252.01s/it]  32%|███▏      | 503/1554 [1:51:10<52:20:29, 179.29s/it] 32%|███▏      | 504/1554 [1:51:20<37:26:44, 128.39s/it] 32%|███▏      | 505/1554 [1:51:30<27:01:42, 92.76s/it]  33%|███▎      | 506/1554 [1:51:39<19:44:41, 67.83s/it] 33%|███▎      | 507/1554 [1:51:49<14:39:27, 50.40s/it] 33%|███▎      | 508/1554 [1:51:59<11:05:41, 38.19s/it] 33%|███▎      | 509/1554 [1:52:08<8:35:42, 29.61s/it]  33%|███▎      | 510/1554 [1:52:18<6:50:36, 23.60s/it]                                                       33%|███▎      | 510/1554 [1:52:18<6:50:36, 23.60s/it] 33%|███▎      | 511/1554 [1:52:27<5:37:12, 19.40s/it] 33%|███▎      | 512/1554 [1:52:37<4:45:56, 16.47s/it] 33%|███▎      | 513/1554 [1:52:47<4:10:16, 14.42s/it] 33%|███▎      | 514/1554 [1:52:56<3:45:13, 12.99s/it] 33%|███▎      | 515/1554 [1:53:06<3:27:58, 12.01s/it] 33%|███▎      | 516/1554 [1:53:16<3:16:17, 11.35s/it] 33%|███▎      | 517/1554 [1:53:26<3:08:50, 10.93s/it] 33%|███▎      | 518/1554 [1:53:36<3:03:30, 10.63s/it] 33%|███▎      | 519/1554 [1:53:46<3:00:27, 10.46s/it] 33%|███▎      | 520/1554 [1:53:56<2:58:39, 10.37s/it]                                                       33%|███▎      | 520/1554 [1:53:56<2:58:39, 10.37s/it] 34%|███▎      | 521/1554 [1:54:06<2:57:15, 10.30s/it] 34%|███▎      | 522/1554 [1:54:16<2:56:33, 10.26s/it] 34%|███▎      | 523/1554 [1:54:27<2:56:44, 10.29s/it] 34%|███▎      | 524/1554 [1:54:37<2:56:47, 10.30s/it] 34%|███▍      | 525/1554 [1:54:47<2:56:58, 10.32s/it] 34%|███▍      | 526/1554 [1:54:58<2:57:06, 10.34s/it] 34%|███▍      | 527/1554 [1:55:08<2:57:17, 10.36s/it] 34%|███▍      | 528/1554 [1:55:19<2:57:31, 10.38s/it] 34%|███▍      | 529/1554 [1:55:29<2:57:46, 10.41s/it] 34%|███▍      | 530/1554 [1:55:40<2:58:41, 10.47s/it]                                                       34%|███▍      | 530/1554 [1:55:40<2:58:41, 10.47s/it] 34%|███▍      | 531/1554 [1:55:50<2:57:14, 10.40s/it] 34%|███▍      | 532/1554 [1:56:01<2:58:30, 10.48s/it] 34%|███▍      | 533/1554 [1:56:11<2:57:09, 10.41s/it] 34%|███▍      | 534/1554 [1:56:21<2:56:40, 10.39s/it] 34%|███▍      | 535/1554 [1:56:31<2:55:58, 10.36s/it] 34%|███▍      | 536/1554 [1:56:42<2:55:40, 10.35s/it] 35%|███▍      | 537/1554 [1:56:52<2:54:59, 10.32s/it] 35%|███▍      | 538/1554 [1:57:02<2:54:08, 10.28s/it] 35%|███▍      | 539/1554 [1:57:12<2:53:12, 10.24s/it] 35%|███▍      | 540/1554 [1:57:23<2:54:06, 10.30s/it]                                                       35%|███▍      | 540/1554 [1:57:23<2:54:06, 10.30s/it] 35%|███▍      | 541/1554 [1:57:33<2:55:50, 10.42s/it] 35%|███▍      | 542/1554 [1:57:44<2:54:52, 10.37s/it] 35%|███▍      | 543/1554 [1:57:54<2:53:58, 10.33s/it] 35%|███▌      | 544/1554 [1:58:04<2:54:31, 10.37s/it] 35%|███▌      | 545/1554 [1:58:15<2:54:01, 10.35s/it] 35%|███▌      | 546/1554 [1:58:25<2:54:21, 10.38s/it] 35%|███▌      | 547/1554 [1:58:36<2:54:12, 10.38s/it] 35%|███▌      | 548/1554 [1:58:46<2:53:18, 10.34s/it] 35%|███▌      | 549/1554 [1:58:56<2:52:57, 10.33s/it] 35%|███▌      | 550/1554 [1:59:07<2:55:42, 10.50s/it]                                                       35%|███▌      | 550/1554 [1:59:07<2:55:42, 10.50s/it] 35%|███▌      | 551/1554 [1:59:17<2:55:03, 10.47s/it] 36%|███▌      | 552/1554 [1:59:28<2:53:24, 10.38s/it] 36%|███▌      | 553/1554 [1:59:38<2:53:52, 10.42s/it] 36%|███▌      | 554/1554 [1:59:49<2:54:43, 10.48s/it] 36%|███▌      | 555/1554 [2:00:00<2:56:14, 10.59s/it] 36%|███▌      | 556/1554 [2:00:10<2:55:47, 10.57s/it] 36%|███▌      | 557/1554 [2:00:21<2:55:05, 10.54s/it] 36%|███▌      | 558/1554 [2:00:31<2:54:33, 10.52s/it] 36%|███▌      | 559/1554 [2:00:41<2:54:19, 10.51s/it] 36%|███▌      | 560/1554 [2:00:52<2:53:27, 10.47s/it]                                                       36%|███▌      | 560/1554 [2:00:52<2:53:27, 10.47s/it] 36%|███▌      | 561/1554 [2:01:02<2:53:29, 10.48s/it] 36%|███▌      | 562/1554 [2:01:13<2:54:21, 10.55s/it] 36%|███▌      | 563/1554 [2:01:24<2:54:02, 10.54s/it] 36%|███▋      | 564/1554 [2:01:34<2:55:32, 10.64s/it] 36%|███▋      | 565/1554 [2:01:45<2:54:51, 10.61s/it] 36%|███▋      | 566/1554 [2:01:56<2:54:36, 10.60s/it] 36%|███▋      | 567/1554 [2:02:06<2:53:14, 10.53s/it] 37%|███▋      | 568/1554 [2:02:17<2:53:14, 10.54s/it] 37%|███▋      | 569/1554 [2:02:27<2:54:36, 10.64s/it] 37%|███▋      | 570/1554 [2:02:38<2:54:29, 10.64s/it]                                                       37%|███▋      | 570/1554 [2:02:38<2:54:29, 10.64s/it] 37%|███▋      | 571/1554 [2:02:49<2:54:56, 10.68s/it] 37%|███▋      | 572/1554 [2:03:00<2:54:55, 10.69s/it] 37%|███▋      | 573/1554 [2:03:10<2:54:57, 10.70s/it] 37%|███▋      | 574/1554 [2:03:21<2:55:34, 10.75s/it] 37%|███▋      | 575/1554 [2:03:32<2:55:11, 10.74s/it] 37%|███▋      | 576/1554 [2:03:43<2:55:36, 10.77s/it] 37%|███▋      | 577/1554 [2:03:53<2:54:45, 10.73s/it] 37%|███▋      | 578/1554 [2:04:04<2:53:59, 10.70s/it] 37%|███▋      | 579/1554 [2:04:15<2:53:33, 10.68s/it] 37%|███▋      | 580/1554 [2:04:25<2:52:29, 10.63s/it]                                                       37%|███▋      | 580/1554 [2:04:25<2:52:29, 10.63s/it] 37%|███▋      | 581/1554 [2:04:36<2:53:13, 10.68s/it] 37%|███▋      | 582/1554 [2:04:47<2:52:53, 10.67s/it] 38%|███▊      | 583/1554 [2:04:57<2:52:01, 10.63s/it] 38%|███▊      | 584/1554 [2:05:08<2:52:13, 10.65s/it] 38%|███▊      | 585/1554 [2:05:19<2:52:32, 10.68s/it] 38%|███▊      | 586/1554 [2:05:29<2:52:39, 10.70s/it] 38%|███▊      | 587/1554 [2:05:40<2:51:55, 10.67s/it] 38%|███▊      | 588/1554 [2:05:51<2:51:42, 10.66s/it] 38%|███▊      | 589/1554 [2:06:01<2:51:43, 10.68s/it] 38%|███▊      | 590/1554 [2:06:12<2:52:11, 10.72s/it]                                                       38%|███▊      | 590/1554 [2:06:12<2:52:11, 10.72s/it] 38%|███▊      | 591/1554 [2:06:23<2:51:52, 10.71s/it] 38%|███▊      | 592/1554 [2:06:33<2:50:23, 10.63s/it] 38%|███▊      | 593/1554 [2:06:44<2:50:11, 10.63s/it] 38%|███▊      | 594/1554 [2:06:55<2:50:30, 10.66s/it] 38%|███▊      | 595/1554 [2:07:05<2:51:21, 10.72s/it] 38%|███▊      | 596/1554 [2:07:16<2:51:26, 10.74s/it] 38%|███▊      | 597/1554 [2:07:27<2:51:26, 10.75s/it] 38%|███▊      | 598/1554 [2:07:38<2:50:50, 10.72s/it] 39%|███▊      | 599/1554 [2:07:48<2:51:20, 10.77s/it] 39%|███▊      | 600/1554 [2:07:59<2:51:55, 10.81s/it]                                                       39%|███▊      | 600/1554 [2:07:59<2:51:55, 10.81s/it] 39%|███▊      | 601/1554 [2:08:10<2:50:30, 10.74s/it] 39%|███▊      | 602/1554 [2:08:20<2:49:15, 10.67s/it] 39%|███▉      | 603/1554 [2:08:31<2:50:00, 10.73s/it] 39%|███▉      | 604/1554 [2:08:42<2:49:49, 10.73s/it] 39%|███▉      | 605/1554 [2:08:53<2:50:29, 10.78s/it] 39%|███▉      | 606/1554 [2:09:03<2:49:17, 10.71s/it] 39%|███▉      | 607/1554 [2:09:14<2:49:13, 10.72s/it] 39%|███▉      | 608/1554 [2:09:25<2:49:28, 10.75s/it] 39%|███▉      | 609/1554 [2:09:36<2:49:22, 10.75s/it] 39%|███▉      | 610/1554 [2:09:47<2:50:11, 10.82s/it]                                                       39%|███▉      | 610/1554 [2:09:47<2:50:11, 10.82s/it] 39%|███▉      | 611/1554 [2:09:57<2:48:50, 10.74s/it] 39%|███▉      | 612/1554 [2:10:08<2:49:15, 10.78s/it] 39%|███▉      | 613/1554 [2:10:19<2:49:25, 10.80s/it] 40%|███▉      | 614/1554 [2:10:30<2:48:49, 10.78s/it] 40%|███▉      | 615/1554 [2:10:40<2:47:59, 10.73s/it] 40%|███▉      | 616/1554 [2:10:51<2:48:14, 10.76s/it] 40%|███▉      | 617/1554 [2:11:02<2:48:46, 10.81s/it] 40%|███▉      | 618/1554 [2:11:13<2:47:52, 10.76s/it] 40%|███▉      | 619/1554 [2:11:23<2:46:51, 10.71s/it] 40%|███▉      | 620/1554 [2:11:34<2:46:24, 10.69s/it]                                                       40%|███▉      | 620/1554 [2:11:34<2:46:24, 10.69s/it] 40%|███▉      | 621/1554 [2:11:45<2:46:13, 10.69s/it] 40%|████      | 622/1554 [2:11:55<2:46:09, 10.70s/it] 40%|████      | 623/1554 [2:12:06<2:45:29, 10.67s/it] 40%|████      | 624/1554 [2:12:17<2:45:18, 10.67s/it] 40%|████      | 625/1554 [2:12:27<2:45:19, 10.68s/it] 40%|████      | 626/1554 [2:12:38<2:45:31, 10.70s/it] 40%|████      | 627/1554 [2:12:49<2:44:53, 10.67s/it] 40%|████      | 628/1554 [2:12:59<2:44:39, 10.67s/it] 40%|████      | 629/1554 [2:13:10<2:44:30, 10.67s/it] 41%|████      | 630/1554 [2:13:21<2:45:16, 10.73s/it]                                                       41%|████      | 630/1554 [2:13:21<2:45:16, 10.73s/it] 41%|████      | 631/1554 [2:13:32<2:44:45, 10.71s/it] 41%|████      | 632/1554 [2:13:42<2:44:50, 10.73s/it] 41%|████      | 633/1554 [2:13:53<2:44:21, 10.71s/it] 41%|████      | 634/1554 [2:14:04<2:44:06, 10.70s/it] 41%|████      | 635/1554 [2:14:15<2:44:11, 10.72s/it] 41%|████      | 636/1554 [2:14:25<2:43:23, 10.68s/it] 41%|████      | 637/1554 [2:14:36<2:43:26, 10.69s/it] 41%|████      | 638/1554 [2:14:47<2:43:22, 10.70s/it] 41%|████      | 639/1554 [2:14:57<2:43:22, 10.71s/it] 41%|████      | 640/1554 [2:15:08<2:43:52, 10.76s/it]                                                       41%|████      | 640/1554 [2:15:08<2:43:52, 10.76s/it] 41%|████      | 641/1554 [2:15:19<2:42:54, 10.71s/it] 41%|████▏     | 642/1554 [2:15:30<2:43:25, 10.75s/it] 41%|████▏     | 643/1554 [2:15:40<2:42:38, 10.71s/it] 41%|████▏     | 644/1554 [2:15:51<2:43:14, 10.76s/it] 42%|████▏     | 645/1554 [2:16:02<2:43:49, 10.81s/it] 42%|████▏     | 646/1554 [2:16:13<2:43:04, 10.78s/it] 42%|████▏     | 647/1554 [2:16:24<2:43:09, 10.79s/it] 42%|████▏     | 648/1554 [2:16:34<2:43:32, 10.83s/it] 42%|████▏     | 649/1554 [2:16:45<2:43:51, 10.86s/it] 42%|████▏     | 650/1554 [2:16:56<2:43:07, 10.83s/it]                                                       42%|████▏     | 650/1554 [2:16:56<2:43:07, 10.83s/it] 42%|████▏     | 651/1554 [2:17:07<2:42:25, 10.79s/it] 42%|████▏     | 652/1554 [2:17:17<2:41:00, 10.71s/it] 42%|████▏     | 653/1554 [2:17:28<2:40:43, 10.70s/it] 42%|████▏     | 654/1554 [2:17:39<2:41:12, 10.75s/it] 42%|████▏     | 655/1554 [2:17:50<2:41:01, 10.75s/it] 42%|████▏     | 656/1554 [2:18:00<2:40:34, 10.73s/it] 42%|████▏     | 657/1554 [2:18:11<2:40:19, 10.72s/it] 42%|████▏     | 658/1554 [2:18:22<2:40:04, 10.72s/it] 42%|████▏     | 659/1554 [2:18:32<2:39:45, 10.71s/it] 42%|████▏     | 660/1554 [2:18:43<2:39:09, 10.68s/it]                                                       42%|████▏     | 660/1554 [2:18:43<2:39:09, 10.68s/it] 43%|████▎     | 661/1554 [2:18:54<2:39:05, 10.69s/it] 43%|████▎     | 662/1554 [2:19:05<2:39:21, 10.72s/it] 43%|████▎     | 663/1554 [2:19:15<2:39:41, 10.75s/it] 43%|████▎     | 664/1554 [2:19:26<2:39:44, 10.77s/it] 43%|████▎     | 665/1554 [2:19:37<2:39:14, 10.75s/it] 43%|████▎     | 666/1554 [2:19:48<2:39:24, 10.77s/it] 43%|████▎     | 667/1554 [2:19:58<2:38:51, 10.75s/it] 43%|████▎     | 668/1554 [2:20:09<2:38:06, 10.71s/it] 43%|████▎     | 669/1554 [2:20:19<2:36:39, 10.62s/it] 43%|████▎     | 670/1554 [2:20:30<2:36:35, 10.63s/it]                                                       43%|████▎     | 670/1554 [2:20:30<2:36:35, 10.63s/it] 43%|████▎     | 671/1554 [2:20:41<2:37:10, 10.68s/it] 43%|████▎     | 672/1554 [2:20:52<2:37:43, 10.73s/it] 43%|████▎     | 673/1554 [2:21:02<2:37:07, 10.70s/it] 43%|████▎     | 674/1554 [2:21:13<2:36:57, 10.70s/it] 43%|████▎     | 675/1554 [2:21:24<2:37:12, 10.73s/it] 44%|████▎     | 676/1554 [2:21:35<2:37:11, 10.74s/it] 44%|████▎     | 677/1554 [2:21:45<2:36:29, 10.71s/it] 44%|████▎     | 678/1554 [2:21:56<2:35:43, 10.67s/it] 44%|████▎     | 679/1554 [2:22:07<2:36:41, 10.74s/it] 44%|████▍     | 680/1554 [2:22:18<2:36:38, 10.75s/it]                                                       44%|████▍     | 680/1554 [2:22:18<2:36:38, 10.75s/it] 44%|████▍     | 681/1554 [2:22:28<2:36:50, 10.78s/it] 44%|████▍     | 682/1554 [2:22:39<2:37:50, 10.86s/it] 44%|████▍     | 683/1554 [2:22:50<2:36:30, 10.78s/it] 44%|████▍     | 684/1554 [2:23:01<2:35:51, 10.75s/it] 44%|████▍     | 685/1554 [2:23:11<2:35:42, 10.75s/it] 44%|████▍     | 686/1554 [2:23:22<2:35:03, 10.72s/it] 44%|████▍     | 687/1554 [2:23:33<2:34:42, 10.71s/it] 44%|████▍     | 688/1554 [2:23:43<2:33:57, 10.67s/it] 44%|████▍     | 689/1554 [2:23:54<2:33:46, 10.67s/it] 44%|████▍     | 690/1554 [2:24:05<2:33:47, 10.68s/it]                                                       44%|████▍     | 690/1554 [2:24:05<2:33:47, 10.68s/it] 44%|████▍     | 691/1554 [2:24:15<2:33:09, 10.65s/it] 45%|████▍     | 692/1554 [2:24:26<2:33:22, 10.68s/it] 45%|████▍     | 693/1554 [2:24:37<2:32:42, 10.64s/it] 45%|████▍     | 694/1554 [2:24:47<2:32:58, 10.67s/it] 45%|████▍     | 695/1554 [2:24:58<2:32:52, 10.68s/it] 45%|████▍     | 696/1554 [2:25:09<2:33:48, 10.76s/it] 45%|████▍     | 697/1554 [2:25:20<2:33:49, 10.77s/it] 45%|████▍     | 698/1554 [2:25:30<2:33:01, 10.73s/it] 45%|████▍     | 699/1554 [2:25:41<2:33:28, 10.77s/it] 45%|████▌     | 700/1554 [2:25:52<2:32:08, 10.69s/it]                                                       45%|████▌     | 700/1554 [2:25:52<2:32:08, 10.69s/it] 45%|████▌     | 701/1554 [2:26:02<2:31:19, 10.64s/it] 45%|████▌     | 702/1554 [2:26:13<2:30:56, 10.63s/it] 45%|████▌     | 703/1554 [2:26:24<2:30:42, 10.63s/it] 45%|████▌     | 704/1554 [2:26:34<2:30:49, 10.65s/it] 45%|████▌     | 705/1554 [2:26:45<2:30:58, 10.67s/it] 45%|████▌     | 706/1554 [2:26:56<2:30:49, 10.67s/it] 45%|████▌     | 707/1554 [2:27:06<2:31:13, 10.71s/it] 46%|████▌     | 708/1554 [2:27:17<2:31:20, 10.73s/it] 46%|████▌     | 709/1554 [2:27:28<2:30:46, 10.71s/it] 46%|████▌     | 710/1554 [2:27:39<2:30:31, 10.70s/it]                                                       46%|████▌     | 710/1554 [2:27:39<2:30:31, 10.70s/it] 46%|████▌     | 711/1554 [2:27:49<2:30:25, 10.71s/it] 46%|████▌     | 712/1554 [2:28:00<2:30:59, 10.76s/it] 46%|████▌     | 713/1554 [2:28:11<2:29:58, 10.70s/it] 46%|████▌     | 714/1554 [2:28:21<2:29:13, 10.66s/it] 46%|████▌     | 715/1554 [2:28:32<2:28:03, 10.59s/it] 46%|████▌     | 716/1554 [2:28:42<2:28:11, 10.61s/it] 46%|████▌     | 717/1554 [2:28:53<2:27:30, 10.57s/it] 46%|████▌     | 718/1554 [2:29:03<2:27:33, 10.59s/it] 46%|████▋     | 719/1554 [2:29:14<2:28:14, 10.65s/it] 46%|████▋     | 720/1554 [2:29:25<2:27:58, 10.65s/it]                                                       46%|████▋     | 720/1554 [2:29:25<2:27:58, 10.65s/it] 46%|████▋     | 721/1554 [2:29:36<2:28:04, 10.67s/it] 46%|████▋     | 722/1554 [2:29:46<2:28:02, 10.68s/it] 47%|████▋     | 723/1554 [2:29:57<2:27:24, 10.64s/it] 47%|████▋     | 724/1554 [2:30:08<2:27:22, 10.65s/it] 47%|████▋     | 725/1554 [2:30:18<2:28:01, 10.71s/it] 47%|████▋     | 726/1554 [2:30:29<2:27:49, 10.71s/it] 47%|████▋     | 727/1554 [2:30:40<2:27:34, 10.71s/it] 47%|████▋     | 728/1554 [2:30:51<2:27:23, 10.71s/it] 47%|████▋     | 729/1554 [2:31:01<2:26:49, 10.68s/it] 47%|████▋     | 730/1554 [2:31:12<2:26:28, 10.67s/it]                                                       47%|████▋     | 730/1554 [2:31:12<2:26:28, 10.67s/it] 47%|████▋     | 731/1554 [2:31:22<2:26:06, 10.65s/it] 47%|████▋     | 732/1554 [2:31:33<2:26:16, 10.68s/it] 47%|████▋     | 733/1554 [2:31:44<2:25:39, 10.65s/it] 47%|████▋     | 734/1554 [2:31:54<2:25:44, 10.66s/it] 47%|████▋     | 735/1554 [2:32:05<2:25:38, 10.67s/it] 47%|████▋     | 736/1554 [2:32:16<2:25:23, 10.66s/it] 47%|████▋     | 737/1554 [2:32:26<2:25:08, 10.66s/it] 47%|████▋     | 738/1554 [2:32:37<2:24:36, 10.63s/it] 48%|████▊     | 739/1554 [2:32:48<2:24:35, 10.65s/it] 48%|████▊     | 740/1554 [2:32:58<2:24:53, 10.68s/it]                                                       48%|████▊     | 740/1554 [2:32:58<2:24:53, 10.68s/it] 48%|████▊     | 741/1554 [2:33:09<2:24:36, 10.67s/it] 48%|████▊     | 742/1554 [2:33:20<2:24:54, 10.71s/it] 48%|████▊     | 743/1554 [2:33:30<2:23:38, 10.63s/it] 48%|████▊     | 744/1554 [2:33:41<2:23:29, 10.63s/it] 48%|████▊     | 745/1554 [2:33:52<2:23:19, 10.63s/it] 48%|████▊     | 746/1554 [2:34:02<2:22:47, 10.60s/it] 48%|████▊     | 747/1554 [2:34:12<2:21:47, 10.54s/it] 48%|████▊     | 748/1554 [2:34:23<2:22:11, 10.59s/it] 48%|████▊     | 749/1554 [2:34:34<2:22:19, 10.61s/it] 48%|████▊     | 750/1554 [2:34:45<2:22:38, 10.64s/it]                                                       48%|████▊     | 750/1554 [2:34:45<2:22:38, 10.64s/it] 48%|████▊     | 751/1554 [2:34:55<2:23:08, 10.70s/it] 48%|████▊     | 752/1554 [2:35:06<2:23:28, 10.73s/it] 48%|████▊     | 753/1554 [2:35:17<2:22:35, 10.68s/it] 49%|████▊     | 754/1554 [2:35:27<2:21:55, 10.64s/it] 49%|████▊     | 755/1554 [2:35:38<2:21:25, 10.62s/it] 49%|████▊     | 756/1554 [2:35:48<2:20:55, 10.60s/it] 49%|████▊     | 757/1554 [2:35:59<2:22:32, 10.73s/it] 49%|████▉     | 758/1554 [2:36:10<2:22:16, 10.72s/it] 49%|████▉     | 759/1554 [2:36:21<2:21:41, 10.69s/it] 49%|████▉     | 760/1554 [2:36:32<2:21:57, 10.73s/it]                                                       49%|████▉     | 760/1554 [2:36:32<2:21:57, 10.73s/it] 49%|████▉     | 761/1554 [2:36:42<2:20:57, 10.67s/it] 49%|████▉     | 762/1554 [2:36:53<2:20:10, 10.62s/it] 49%|████▉     | 763/1554 [2:37:03<2:19:53, 10.61s/it] 49%|████▉     | 764/1554 [2:37:14<2:19:24, 10.59s/it] 49%|████▉     | 765/1554 [2:37:24<2:18:46, 10.55s/it] 49%|████▉     | 766/1554 [2:37:35<2:18:59, 10.58s/it] 49%|████▉     | 767/1554 [2:37:46<2:19:16, 10.62s/it] 49%|████▉     | 768/1554 [2:37:56<2:19:47, 10.67s/it] 49%|████▉     | 769/1554 [2:38:07<2:19:40, 10.68s/it] 50%|████▉     | 770/1554 [2:38:18<2:19:26, 10.67s/it]                                                       50%|████▉     | 770/1554 [2:38:18<2:19:26, 10.67s/it] 50%|████▉     | 771/1554 [2:38:28<2:19:07, 10.66s/it] 50%|████▉     | 772/1554 [2:38:39<2:18:26, 10.62s/it] 50%|████▉     | 773/1554 [2:38:50<2:18:17, 10.62s/it] 50%|████▉     | 774/1554 [2:39:00<2:18:37, 10.66s/it] 50%|████▉     | 775/1554 [2:39:11<2:17:26, 10.59s/it] 50%|████▉     | 776/1554 [2:39:21<2:17:32, 10.61s/it] 50%|█████     | 777/1554 [2:39:32<2:18:24, 10.69s/it] 50%|█████     | 778/1554 [2:39:43<2:18:32, 10.71s/it] 50%|█████     | 779/1554 [2:39:54<2:18:01, 10.69s/it] 50%|█████     | 780/1554 [2:40:04<2:17:05, 10.63s/it]                                                       50%|█████     | 780/1554 [2:40:04<2:17:05, 10.63s/it] 50%|█████     | 781/1554 [2:40:15<2:17:23, 10.66s/it] 50%|█████     | 782/1554 [2:40:25<2:17:01, 10.65s/it] 50%|█████     | 783/1554 [2:40:36<2:17:45, 10.72s/it] 50%|█████     | 784/1554 [2:40:47<2:16:45, 10.66s/it] 51%|█████     | 785/1554 [2:40:58<2:17:48, 10.75s/it] 51%|█████     | 786/1554 [2:41:08<2:17:01, 10.71s/it] 51%|█████     | 787/1554 [2:41:19<2:16:35, 10.68s/it] 51%|█████     | 788/1554 [2:41:30<2:16:08, 10.66s/it] 51%|█████     | 789/1554 [2:41:40<2:16:06, 10.68s/it] 51%|█████     | 790/1554 [2:41:51<2:15:29, 10.64s/it]                                                       51%|█████     | 790/1554 [2:41:51<2:15:29, 10.64s/it] 51%|█████     | 791/1554 [2:42:02<2:15:21, 10.64s/it] 51%|█████     | 792/1554 [2:42:12<2:15:44, 10.69s/it] 51%|█████     | 793/1554 [2:42:23<2:14:38, 10.62s/it] 51%|█████     | 794/1554 [2:42:34<2:14:49, 10.64s/it] 51%|█████     | 795/1554 [2:42:44<2:14:35, 10.64s/it] 51%|█████     | 796/1554 [2:42:55<2:13:52, 10.60s/it] 51%|█████▏    | 797/1554 [2:43:05<2:14:19, 10.65s/it] 51%|█████▏    | 798/1554 [2:43:16<2:13:55, 10.63s/it] 51%|█████▏    | 799/1554 [2:43:27<2:15:05, 10.74s/it] 51%|█████▏    | 800/1554 [2:43:38<2:14:17, 10.69s/it]                                                       51%|█████▏    | 800/1554 [2:43:38<2:14:17, 10.69s/it] 52%|█████▏    | 801/1554 [2:43:48<2:13:32, 10.64s/it] 52%|█████▏    | 802/1554 [2:43:59<2:13:39, 10.66s/it] 52%|█████▏    | 803/1554 [2:44:09<2:13:19, 10.65s/it] 52%|█████▏    | 804/1554 [2:44:20<2:13:24, 10.67s/it] 52%|█████▏    | 805/1554 [2:44:31<2:12:38, 10.63s/it] 52%|█████▏    | 806/1554 [2:44:42<2:13:27, 10.71s/it] 52%|█████▏    | 807/1554 [2:44:53<2:14:10, 10.78s/it] 52%|█████▏    | 808/1554 [2:45:03<2:13:11, 10.71s/it] 52%|█████▏    | 809/1554 [2:45:14<2:13:12, 10.73s/it] 52%|█████▏    | 810/1554 [2:45:24<2:12:02, 10.65s/it]                                                       52%|█████▏    | 810/1554 [2:45:24<2:12:02, 10.65s/it] 52%|█████▏    | 811/1554 [2:45:35<2:12:14, 10.68s/it] 52%|█████▏    | 812/1554 [2:45:46<2:12:34, 10.72s/it] 52%|█████▏    | 813/1554 [2:45:57<2:12:23, 10.72s/it] 52%|█████▏    | 814/1554 [2:46:07<2:12:38, 10.76s/it] 52%|█████▏    | 815/1554 [2:46:18<2:11:34, 10.68s/it] 53%|█████▎    | 816/1554 [2:46:29<2:11:17, 10.67s/it] 53%|█████▎    | 817/1554 [2:46:39<2:11:15, 10.69s/it] 53%|█████▎    | 818/1554 [2:46:50<2:10:39, 10.65s/it] 53%|█████▎    | 819/1554 [2:47:01<2:11:00, 10.69s/it] 53%|█████▎    | 820/1554 [2:47:11<2:10:22, 10.66s/it]                                                       53%|█████▎    | 820/1554 [2:47:11<2:10:22, 10.66s/it] 53%|█████▎    | 821/1554 [2:47:22<2:10:10, 10.66s/it] 53%|█████▎    | 822/1554 [2:47:33<2:10:05, 10.66s/it] 53%|█████▎    | 823/1554 [2:47:43<2:10:30, 10.71s/it] 53%|█████▎    | 824/1554 [2:47:54<2:09:33, 10.65s/it] 53%|█████▎    | 825/1554 [2:48:04<2:09:04, 10.62s/it] 53%|█████▎    | 826/1554 [2:48:15<2:09:30, 10.67s/it] 53%|█████▎    | 827/1554 [2:48:26<2:09:41, 10.70s/it] 53%|█████▎    | 828/1554 [2:48:37<2:09:02, 10.66s/it] 53%|█████▎    | 829/1554 [2:48:47<2:09:13, 10.69s/it] 53%|█████▎    | 830/1554 [2:48:58<2:08:29, 10.65s/it]                                                       53%|█████▎    | 830/1554 [2:48:58<2:08:29, 10.65s/it] 53%|█████▎    | 831/1554 [2:49:09<2:10:05, 10.80s/it] 54%|█████▎    | 832/1554 [2:49:20<2:09:27, 10.76s/it] 54%|█████▎    | 833/1554 [2:49:30<2:09:03, 10.74s/it] 54%|█████▎    | 834/1554 [2:49:41<2:09:31, 10.79s/it] 54%|█████▎    | 835/1554 [2:49:52<2:08:35, 10.73s/it] 54%|█████▍    | 836/1554 [2:50:02<2:07:20, 10.64s/it] 54%|█████▍    | 837/1554 [2:50:13<2:07:49, 10.70s/it] 54%|█████▍    | 838/1554 [2:50:24<2:07:38, 10.70s/it] 54%|█████▍    | 839/1554 [2:50:35<2:07:52, 10.73s/it] 54%|█████▍    | 840/1554 [2:50:45<2:07:02, 10.68s/it]                                                       54%|█████▍    | 840/1554 [2:50:45<2:07:02, 10.68s/it] 54%|█████▍    | 841/1554 [2:50:56<2:06:25, 10.64s/it] 54%|█████▍    | 842/1554 [2:51:06<2:05:29, 10.58s/it] 54%|█████▍    | 843/1554 [2:51:17<2:05:18, 10.57s/it] 54%|█████▍    | 844/1554 [2:51:27<2:05:24, 10.60s/it] 54%|█████▍    | 845/1554 [2:51:38<2:05:13, 10.60s/it] 54%|█████▍    | 846/1554 [2:51:49<2:05:12, 10.61s/it] 55%|█████▍    | 847/1554 [2:51:59<2:04:29, 10.56s/it] 55%|█████▍    | 848/1554 [2:52:10<2:04:38, 10.59s/it] 55%|█████▍    | 849/1554 [2:52:21<2:05:05, 10.65s/it] 55%|█████▍    | 850/1554 [2:52:31<2:04:44, 10.63s/it]                                                       55%|█████▍    | 850/1554 [2:52:31<2:04:44, 10.63s/it] 55%|█████▍    | 851/1554 [2:52:42<2:05:24, 10.70s/it] 55%|█████▍    | 852/1554 [2:52:53<2:04:29, 10.64s/it] 55%|█████▍    | 853/1554 [2:53:03<2:03:36, 10.58s/it] 55%|█████▍    | 854/1554 [2:53:13<2:03:01, 10.55s/it] 55%|█████▌    | 855/1554 [2:53:24<2:03:10, 10.57s/it] 55%|█████▌    | 856/1554 [2:53:35<2:03:17, 10.60s/it] 55%|█████▌    | 857/1554 [2:53:45<2:02:47, 10.57s/it] 55%|█████▌    | 858/1554 [2:53:56<2:03:16, 10.63s/it] 55%|█████▌    | 859/1554 [2:54:07<2:03:32, 10.67s/it] 55%|█████▌    | 860/1554 [2:54:17<2:02:51, 10.62s/it]                                                       55%|█████▌    | 860/1554 [2:54:17<2:02:51, 10.62s/it] 55%|█████▌    | 861/1554 [2:54:28<2:03:13, 10.67s/it] 55%|█████▌    | 862/1554 [2:54:38<2:02:07, 10.59s/it] 56%|█████▌    | 863/1554 [2:54:49<2:02:20, 10.62s/it] 56%|█████▌    | 864/1554 [2:55:00<2:02:24, 10.64s/it] 56%|█████▌    | 865/1554 [2:55:10<2:01:59, 10.62s/it] 56%|█████▌    | 866/1554 [2:55:21<2:01:16, 10.58s/it] 56%|█████▌    | 867/1554 [2:55:31<2:00:13, 10.50s/it] 56%|█████▌    | 868/1554 [2:55:42<1:59:46, 10.48s/it] 56%|█████▌    | 869/1554 [2:55:52<2:00:28, 10.55s/it] 56%|█████▌    | 870/1554 [2:56:03<2:01:17, 10.64s/it]                                                       56%|█████▌    | 870/1554 [2:56:03<2:01:17, 10.64s/it] 56%|█████▌    | 871/1554 [2:56:14<2:00:54, 10.62s/it] 56%|█████▌    | 872/1554 [2:56:24<2:00:09, 10.57s/it] 56%|█████▌    | 873/1554 [2:56:35<2:00:09, 10.59s/it] 56%|█████▌    | 874/1554 [2:56:45<2:00:03, 10.59s/it] 56%|█████▋    | 875/1554 [2:56:56<2:00:59, 10.69s/it] 56%|█████▋    | 876/1554 [2:57:07<2:00:58, 10.71s/it] 56%|█████▋    | 877/1554 [2:57:18<1:59:43, 10.61s/it] 56%|█████▋    | 878/1554 [2:57:28<1:59:44, 10.63s/it] 57%|█████▋    | 879/1554 [2:57:39<1:59:22, 10.61s/it] 57%|█████▋    | 880/1554 [2:57:50<1:59:54, 10.67s/it]                                                       57%|█████▋    | 880/1554 [2:57:50<1:59:54, 10.67s/it] 57%|█████▋    | 881/1554 [2:58:00<2:00:17, 10.72s/it] 57%|█████▋    | 882/1554 [2:58:11<1:59:52, 10.70s/it] 57%|█████▋    | 883/1554 [2:58:22<1:59:47, 10.71s/it] 57%|█████▋    | 884/1554 [2:58:32<1:58:55, 10.65s/it] 57%|█████▋    | 885/1554 [2:58:43<1:57:49, 10.57s/it] 57%|█████▋    | 886/1554 [2:58:53<1:57:58, 10.60s/it] 57%|█████▋    | 887/1554 [2:59:04<1:57:36, 10.58s/it] 57%|█████▋    | 888/1554 [2:59:15<1:57:34, 10.59s/it] 57%|█████▋    | 889/1554 [2:59:25<1:58:34, 10.70s/it] 57%|█████▋    | 890/1554 [2:59:36<1:58:33, 10.71s/it]                                                       57%|█████▋    | 890/1554 [2:59:36<1:58:33, 10.71s/it] 57%|█████▋    | 891/1554 [2:59:47<1:58:25, 10.72s/it] 57%|█████▋    | 892/1554 [2:59:58<1:57:59, 10.69s/it] 57%|█████▋    | 893/1554 [3:00:08<1:56:50, 10.61s/it] 58%|█████▊    | 894/1554 [3:00:19<1:56:29, 10.59s/it] 58%|█████▊    | 895/1554 [3:00:29<1:56:35, 10.62s/it] 58%|█████▊    | 896/1554 [3:00:40<1:56:16, 10.60s/it] 58%|█████▊    | 897/1554 [3:00:50<1:56:16, 10.62s/it] 58%|█████▊    | 898/1554 [3:01:01<1:55:11, 10.54s/it] 58%|█████▊    | 899/1554 [3:01:11<1:54:59, 10.53s/it] 58%|█████▊    | 900/1554 [3:01:22<1:54:50, 10.54s/it]                                                       58%|█████▊    | 900/1554 [3:01:22<1:54:50, 10.54s/it] 58%|█████▊    | 901/1554 [3:01:33<1:55:02, 10.57s/it] 58%|█████▊    | 902/1554 [3:01:43<1:55:33, 10.63s/it] 58%|█████▊    | 903/1554 [3:01:54<1:55:13, 10.62s/it] 58%|█████▊    | 904/1554 [3:02:04<1:55:02, 10.62s/it] 58%|█████▊    | 905/1554 [3:02:15<1:54:32, 10.59s/it] 58%|█████▊    | 906/1554 [3:02:26<1:55:01, 10.65s/it] 58%|█████▊    | 907/1554 [3:02:36<1:54:22, 10.61s/it] 58%|█████▊    | 908/1554 [3:02:47<1:53:45, 10.57s/it] 58%|█████▊    | 909/1554 [3:02:58<1:54:24, 10.64s/it] 59%|█████▊    | 910/1554 [3:03:08<1:54:56, 10.71s/it]                                                       59%|█████▊    | 910/1554 [3:03:09<1:54:56, 10.71s/it] 59%|█████▊    | 911/1554 [3:03:20<1:56:05, 10.83s/it] 59%|█████▊    | 912/1554 [3:03:30<1:55:28, 10.79s/it] 59%|█████▉    | 913/1554 [3:03:41<1:54:46, 10.74s/it] 59%|█████▉    | 914/1554 [3:03:52<1:54:10, 10.70s/it] 59%|█████▉    | 915/1554 [3:04:02<1:53:48, 10.69s/it] 59%|█████▉    | 916/1554 [3:04:13<1:54:28, 10.77s/it] 59%|█████▉    | 917/1554 [3:04:24<1:54:01, 10.74s/it] 59%|█████▉    | 918/1554 [3:04:34<1:52:47, 10.64s/it] 59%|█████▉    | 919/1554 [3:04:45<1:53:50, 10.76s/it] 59%|█████▉    | 920/1554 [3:04:56<1:53:42, 10.76s/it]                                                       59%|█████▉    | 920/1554 [3:04:56<1:53:42, 10.76s/it] 59%|█████▉    | 921/1554 [3:05:07<1:53:03, 10.72s/it] 59%|█████▉    | 922/1554 [3:05:17<1:52:54, 10.72s/it] 59%|█████▉    | 923/1554 [3:05:28<1:51:54, 10.64s/it] 59%|█████▉    | 924/1554 [3:05:39<1:52:29, 10.71s/it] 60%|█████▉    | 925/1554 [3:05:49<1:51:37, 10.65s/it] 60%|█████▉    | 926/1554 [3:06:00<1:52:55, 10.79s/it] 60%|█████▉    | 927/1554 [3:06:11<1:53:14, 10.84s/it] 60%|█████▉    | 928/1554 [3:06:22<1:53:21, 10.87s/it] 60%|█████▉    | 929/1554 [3:06:33<1:52:09, 10.77s/it] 60%|█████▉    | 930/1554 [3:06:44<1:52:26, 10.81s/it]                                                       60%|█████▉    | 930/1554 [3:06:44<1:52:26, 10.81s/it] 60%|█████▉    | 931/1554 [3:06:54<1:51:56, 10.78s/it] 60%|█████▉    | 932/1554 [3:07:05<1:51:58, 10.80s/it] 60%|██████    | 933/1554 [3:07:16<1:51:40, 10.79s/it] 60%|██████    | 934/1554 [3:07:27<1:51:07, 10.75s/it] 60%|██████    | 935/1554 [3:07:37<1:49:39, 10.63s/it] 60%|██████    | 936/1554 [3:07:47<1:49:02, 10.59s/it] 60%|██████    | 937/1554 [3:07:58<1:49:07, 10.61s/it] 60%|██████    | 938/1554 [3:08:09<1:48:47, 10.60s/it] 60%|██████    | 939/1554 [3:08:19<1:48:02, 10.54s/it] 60%|██████    | 940/1554 [3:08:30<1:47:54, 10.54s/it]                                                       60%|██████    | 940/1554 [3:08:30<1:47:54, 10.54s/it] 61%|██████    | 941/1554 [3:08:41<1:48:52, 10.66s/it] 61%|██████    | 942/1554 [3:08:51<1:49:14, 10.71s/it] 61%|██████    | 943/1554 [3:09:02<1:48:23, 10.64s/it] 61%|██████    | 944/1554 [3:09:12<1:47:49, 10.61s/it] 61%|██████    | 945/1554 [3:09:23<1:47:06, 10.55s/it] 61%|██████    | 946/1554 [3:09:33<1:46:26, 10.50s/it] 61%|██████    | 947/1554 [3:09:44<1:47:01, 10.58s/it] 61%|██████    | 948/1554 [3:09:55<1:47:32, 10.65s/it] 61%|██████    | 949/1554 [3:10:05<1:47:19, 10.64s/it] 61%|██████    | 950/1554 [3:10:16<1:47:11, 10.65s/it]                                                       61%|██████    | 950/1554 [3:10:16<1:47:11, 10.65s/it] 61%|██████    | 951/1554 [3:10:26<1:46:06, 10.56s/it] 61%|██████▏   | 952/1554 [3:10:37<1:47:00, 10.67s/it] 61%|██████▏   | 953/1554 [3:10:48<1:47:04, 10.69s/it] 61%|██████▏   | 954/1554 [3:10:59<1:47:19, 10.73s/it] 61%|██████▏   | 955/1554 [3:11:10<1:46:49, 10.70s/it] 62%|██████▏   | 956/1554 [3:11:20<1:46:32, 10.69s/it] 62%|██████▏   | 957/1554 [3:11:31<1:46:12, 10.67s/it] 62%|██████▏   | 958/1554 [3:11:41<1:45:17, 10.60s/it] 62%|██████▏   | 959/1554 [3:11:53<1:47:06, 10.80s/it] 62%|██████▏   | 960/1554 [3:12:03<1:45:43, 10.68s/it]                                                       62%|██████▏   | 960/1554 [3:12:03<1:45:43, 10.68s/it] 62%|██████▏   | 961/1554 [3:12:13<1:44:52, 10.61s/it] 62%|██████▏   | 962/1554 [3:12:24<1:44:57, 10.64s/it] 62%|██████▏   | 963/1554 [3:12:35<1:44:46, 10.64s/it] 62%|██████▏   | 964/1554 [3:12:45<1:44:06, 10.59s/it] 62%|██████▏   | 965/1554 [3:12:56<1:44:00, 10.60s/it] 62%|██████▏   | 966/1554 [3:13:06<1:43:50, 10.60s/it] 62%|██████▏   | 967/1554 [3:13:17<1:42:51, 10.51s/it] 62%|██████▏   | 968/1554 [3:13:28<1:43:26, 10.59s/it] 62%|██████▏   | 969/1554 [3:13:38<1:43:31, 10.62s/it] 62%|██████▏   | 970/1554 [3:13:49<1:43:32, 10.64s/it]                                                       62%|██████▏   | 970/1554 [3:13:49<1:43:32, 10.64s/it] 62%|██████▏   | 971/1554 [3:13:59<1:43:01, 10.60s/it] 63%|██████▎   | 972/1554 [3:14:10<1:43:38, 10.69s/it] 63%|██████▎   | 973/1554 [3:14:21<1:42:27, 10.58s/it] 63%|██████▎   | 974/1554 [3:14:31<1:42:15, 10.58s/it] 63%|██████▎   | 975/1554 [3:14:42<1:41:37, 10.53s/it] 63%|██████▎   | 976/1554 [3:14:52<1:41:50, 10.57s/it] 63%|██████▎   | 977/1554 [3:15:03<1:40:49, 10.48s/it] 63%|██████▎   | 978/1554 [3:15:13<1:41:08, 10.54s/it] 63%|██████▎   | 979/1554 [3:15:24<1:41:12, 10.56s/it] 63%|██████▎   | 980/1554 [3:15:34<1:40:23, 10.49s/it]                                                       63%|██████▎   | 980/1554 [3:15:34<1:40:23, 10.49s/it] 63%|██████▎   | 981/1554 [3:15:45<1:40:54, 10.57s/it] 63%|██████▎   | 982/1554 [3:15:55<1:40:26, 10.54s/it] 63%|██████▎   | 983/1554 [3:16:06<1:40:26, 10.56s/it] 63%|██████▎   | 984/1554 [3:16:17<1:40:16, 10.55s/it] 63%|██████▎   | 985/1554 [3:16:27<1:40:14, 10.57s/it] 63%|██████▎   | 986/1554 [3:16:38<1:39:52, 10.55s/it] 64%|██████▎   | 987/1554 [3:16:48<1:39:23, 10.52s/it] 64%|██████▎   | 988/1554 [3:16:59<1:39:07, 10.51s/it] 64%|██████▎   | 989/1554 [3:17:09<1:39:24, 10.56s/it] 64%|██████▎   | 990/1554 [3:17:20<1:39:29, 10.58s/it]                                                       64%|██████▎   | 990/1554 [3:17:20<1:39:29, 10.58s/it] 64%|██████▍   | 991/1554 [3:17:31<1:40:43, 10.73s/it] 64%|██████▍   | 992/1554 [3:17:41<1:39:37, 10.64s/it] 64%|██████▍   | 993/1554 [3:17:52<1:38:26, 10.53s/it] 64%|██████▍   | 994/1554 [3:18:02<1:38:20, 10.54s/it] 64%|██████▍   | 995/1554 [3:18:13<1:38:07, 10.53s/it] 64%|██████▍   | 996/1554 [3:18:23<1:37:54, 10.53s/it] 64%|██████▍   | 997/1554 [3:18:34<1:37:54, 10.55s/it] 64%|██████▍   | 998/1554 [3:18:44<1:37:33, 10.53s/it] 64%|██████▍   | 999/1554 [3:18:55<1:37:24, 10.53s/it] 64%|██████▍   | 1000/1554 [3:19:05<1:37:06, 10.52s/it]                                                        64%|██████▍   | 1000/1554 [3:19:05<1:37:06, 10.52s/it][INFO|trainer.py:3819] 2024-09-29 14:51:29,710 >> 
192.168.0.25: ***** Running Evaluation *****
192.168.0.25: [INFO|trainer.py:3821] 2024-09-29 14:51:29,710 >>   Num examples = 14708
192.168.0.25: [INFO|trainer.py:3824] 2024-09-29 14:51:29,710 >>   Batch size = 1
192.168.0.13: [INFO|trainer.py:3819] 2024-09-29 14:51:30,154 >> 
192.168.0.13: ***** Running Evaluation *****
192.168.0.13: [INFO|trainer.py:3821] 2024-09-29 14:51:30,154 >>   Num examples = 14708
192.168.0.13: [INFO|trainer.py:3824] 2024-09-29 14:51:30,154 >>   Batch size = 1
192.168.0.25: 
192.168.0.25:   0%|          | 0/460 [00:00<?, ?it/s][A
192.168.0.25:   0%|          | 2/460 [00:02<09:32,  1.25s/it][A
192.168.0.25:   1%|          | 3/460 [00:05<13:29,  1.77s/it][A
192.168.0.25:   1%|          | 4/460 [00:07<15:31,  2.04s/it][A
192.168.0.25:   1%|          | 5/460 [00:10<16:42,  2.20s/it][A
192.168.0.25:   1%|▏         | 6/460 [00:12<17:25,  2.30s/it][A
192.168.0.25:   2%|▏         | 7/460 [00:15<17:51,  2.37s/it][A
192.168.0.25:   2%|▏         | 8/460 [00:17<18:08,  2.41s/it][A
192.168.0.25:   2%|▏         | 9/460 [00:20<18:19,  2.44s/it][A
192.168.0.25:   2%|▏         | 10/460 [00:22<18:25,  2.46s/it][A
192.168.0.25:   2%|▏         | 11/460 [00:25<18:29,  2.47s/it][A
192.168.0.25:   3%|▎         | 12/460 [00:27<18:31,  2.48s/it][A
192.168.0.25:   3%|▎         | 13/460 [00:30<18:31,  2.49s/it][A
192.168.0.25:   3%|▎         | 14/460 [00:32<18:31,  2.49s/it][A
192.168.0.25:   3%|▎         | 15/460 [00:35<18:30,  2.49s/it][A
192.168.0.25:   3%|▎         | 16/460 [00:37<18:28,  2.50s/it][A
192.168.0.25:   4%|▎         | 17/460 [00:40<18:26,  2.50s/it][A
192.168.0.25:   4%|▍         | 18/460 [00:42<18:24,  2.50s/it][A
192.168.0.25:   4%|▍         | 19/460 [00:45<18:22,  2.50s/it][A
192.168.0.25:   4%|▍         | 20/460 [00:47<18:20,  2.50s/it][A
192.168.0.25:   5%|▍         | 21/460 [00:50<18:18,  2.50s/it][A
192.168.0.25:   5%|▍         | 22/460 [00:52<18:15,  2.50s/it][A
192.168.0.25:   5%|▌         | 23/460 [00:55<18:13,  2.50s/it][A
192.168.0.25:   5%|▌         | 24/460 [00:57<18:10,  2.50s/it][A
192.168.0.25:   5%|▌         | 25/460 [01:00<18:07,  2.50s/it][A
192.168.0.25:   6%|▌         | 26/460 [01:02<18:05,  2.50s/it][A
192.168.0.25:   6%|▌         | 27/460 [01:05<18:03,  2.50s/it][A
192.168.0.25:   6%|▌         | 28/460 [01:07<18:00,  2.50s/it][A
192.168.0.25:   6%|▋         | 29/460 [01:10<17:58,  2.50s/it][A
192.168.0.25:   7%|▋         | 30/460 [01:12<17:55,  2.50s/it][A
192.168.0.25:   7%|▋         | 31/460 [01:15<17:53,  2.50s/it][A
192.168.0.25:   7%|▋         | 32/460 [01:17<17:50,  2.50s/it][A
192.168.0.25:   7%|▋         | 33/460 [01:20<17:48,  2.50s/it][A
192.168.0.25:   7%|▋         | 34/460 [01:22<17:45,  2.50s/it][A
192.168.0.25:   8%|▊         | 35/460 [01:25<17:42,  2.50s/it][A
192.168.0.25:   8%|▊         | 36/460 [01:27<17:40,  2.50s/it][A
192.168.0.25:   8%|▊         | 37/460 [01:30<17:37,  2.50s/it][A
192.168.0.25:   8%|▊         | 38/460 [01:32<17:35,  2.50s/it][A
192.168.0.25:   8%|▊         | 39/460 [01:35<17:32,  2.50s/it][A
192.168.0.25:   9%|▊         | 40/460 [01:37<17:30,  2.50s/it][A
192.168.0.25:   9%|▉         | 41/460 [01:40<17:27,  2.50s/it][A
192.168.0.25:   9%|▉         | 42/460 [01:42<17:25,  2.50s/it][A
192.168.0.25:   9%|▉         | 43/460 [01:45<17:22,  2.50s/it][A
192.168.0.25:  10%|▉         | 44/460 [01:47<17:20,  2.50s/it][A
192.168.0.25:  10%|▉         | 45/460 [01:50<17:17,  2.50s/it][A
192.168.0.25:  10%|█         | 46/460 [01:52<17:15,  2.50s/it][A
192.168.0.25:  10%|█         | 47/460 [01:55<17:13,  2.50s/it][A
192.168.0.25:  10%|█         | 48/460 [01:57<17:10,  2.50s/it][A
192.168.0.25:  11%|█         | 49/460 [02:00<17:07,  2.50s/it][A
192.168.0.25:  11%|█         | 50/460 [02:02<17:05,  2.50s/it][A
192.168.0.25:  11%|█         | 51/460 [02:05<17:02,  2.50s/it][A
192.168.0.25:  11%|█▏        | 52/460 [02:07<17:00,  2.50s/it][A
192.168.0.25:  12%|█▏        | 53/460 [02:10<16:57,  2.50s/it][A
192.168.0.25:  12%|█▏        | 54/460 [02:12<16:55,  2.50s/it][A
192.168.0.25:  12%|█▏        | 55/460 [02:15<16:52,  2.50s/it][A
192.168.0.25:  12%|█▏        | 56/460 [02:17<16:50,  2.50s/it][A
192.168.0.25:  12%|█▏        | 57/460 [02:20<16:47,  2.50s/it][A
192.168.0.25:  13%|█▎        | 58/460 [02:22<16:45,  2.50s/it][A
192.168.0.25:  13%|█▎        | 59/460 [02:25<16:42,  2.50s/it][A
192.168.0.25:  13%|█▎        | 60/460 [02:27<16:40,  2.50s/it][A
192.168.0.25:  13%|█▎        | 61/460 [02:30<16:38,  2.50s/it][A
192.168.0.25:  13%|█▎        | 62/460 [02:32<16:35,  2.50s/it][A
192.168.0.25:  14%|█▎        | 63/460 [02:35<16:33,  2.50s/it][A
192.168.0.25:  14%|█▍        | 64/460 [02:37<16:30,  2.50s/it][A
192.168.0.25:  14%|█▍        | 65/460 [02:40<16:28,  2.50s/it][A
192.168.0.25:  14%|█▍        | 66/460 [02:42<16:25,  2.50s/it][A
192.168.0.25:  15%|█▍        | 67/460 [02:45<16:23,  2.50s/it][A
192.168.0.25:  15%|█▍        | 68/460 [02:47<16:20,  2.50s/it][A
192.168.0.25:  15%|█▌        | 69/460 [02:50<16:17,  2.50s/it][A
192.168.0.25:  15%|█▌        | 70/460 [02:52<16:15,  2.50s/it][A
192.168.0.25:  15%|█▌        | 71/460 [02:55<16:12,  2.50s/it][A
192.168.0.25:  16%|█▌        | 72/460 [02:57<16:10,  2.50s/it][A
192.168.0.25:  16%|█▌        | 73/460 [03:00<16:07,  2.50s/it][A
192.168.0.25:  16%|█▌        | 74/460 [03:02<16:05,  2.50s/it][A
192.168.0.25:  16%|█▋        | 75/460 [03:05<16:03,  2.50s/it][A
192.168.0.25:  17%|█▋        | 76/460 [03:07<16:00,  2.50s/it][A
192.168.0.25:  17%|█▋        | 77/460 [03:10<15:57,  2.50s/it][A
192.168.0.25:  17%|█▋        | 78/460 [03:12<15:55,  2.50s/it][A
192.168.0.25:  17%|█▋        | 79/460 [03:15<15:53,  2.50s/it][A
192.168.0.25:  17%|█▋        | 80/460 [03:17<15:50,  2.50s/it][A
192.168.0.25:  18%|█▊        | 81/460 [03:20<15:48,  2.50s/it][A
192.168.0.25:  18%|█▊        | 82/460 [03:22<15:45,  2.50s/it][A
192.168.0.25:  18%|█▊        | 83/460 [03:25<15:43,  2.50s/it][A
192.168.0.25:  18%|█▊        | 84/460 [03:27<15:40,  2.50s/it][A
192.168.0.25:  18%|█▊        | 85/460 [03:30<15:38,  2.50s/it][A
192.168.0.25:  19%|█▊        | 86/460 [03:32<15:35,  2.50s/it][A
192.168.0.25:  19%|█▉        | 87/460 [03:35<15:33,  2.50s/it][A
192.168.0.25:  19%|█▉        | 88/460 [03:37<15:30,  2.50s/it][A
192.168.0.25:  19%|█▉        | 89/460 [03:40<15:28,  2.50s/it][A
192.168.0.25:  20%|█▉        | 90/460 [03:42<15:25,  2.50s/it][A
192.168.0.25:  20%|█▉        | 91/460 [03:45<15:23,  2.50s/it][A
192.168.0.25:  20%|██        | 92/460 [03:47<15:21,  2.50s/it][A
192.168.0.25:  20%|██        | 93/460 [03:50<15:18,  2.50s/it][A
192.168.0.25:  20%|██        | 94/460 [03:52<15:16,  2.50s/it][A
192.168.0.25:  21%|██        | 95/460 [03:55<15:13,  2.50s/it][A
192.168.0.25:  21%|██        | 96/460 [03:57<15:11,  2.50s/it][A
192.168.0.25:  21%|██        | 97/460 [04:00<15:08,  2.50s/it][A
192.168.0.25:  21%|██▏       | 98/460 [04:02<15:06,  2.50s/it][A
192.168.0.25:  22%|██▏       | 99/460 [04:05<15:03,  2.50s/it][A
192.168.0.25:  22%|██▏       | 100/460 [04:07<15:00,  2.50s/it][A
192.168.0.25:  22%|██▏       | 101/460 [04:10<14:58,  2.50s/it][A
192.168.0.25:  22%|██▏       | 102/460 [04:12<14:55,  2.50s/it][A
192.168.0.25:  22%|██▏       | 103/460 [04:15<14:52,  2.50s/it][A
192.168.0.25:  23%|██▎       | 104/460 [04:17<14:50,  2.50s/it][A
192.168.0.25:  23%|██▎       | 105/460 [04:20<14:47,  2.50s/it][A
192.168.0.25:  23%|██▎       | 106/460 [04:22<14:45,  2.50s/it][A
192.168.0.25:  23%|██▎       | 107/460 [04:25<14:43,  2.50s/it][A
192.168.0.25:  23%|██▎       | 108/460 [04:27<14:40,  2.50s/it][A
192.168.0.25:  24%|██▎       | 109/460 [04:30<14:38,  2.50s/it][A
192.168.0.25:  24%|██▍       | 110/460 [04:32<14:35,  2.50s/it][A
192.168.0.25:  24%|██▍       | 111/460 [04:35<14:33,  2.50s/it][A
192.168.0.25:  24%|██▍       | 112/460 [04:37<14:31,  2.50s/it][A
192.168.0.25:  25%|██▍       | 113/460 [04:40<14:28,  2.50s/it][A
192.168.0.25:  25%|██▍       | 114/460 [04:42<14:25,  2.50s/it][A
192.168.0.25:  25%|██▌       | 115/460 [04:45<14:23,  2.50s/it][A
192.168.0.25:  25%|██▌       | 116/460 [04:47<14:20,  2.50s/it][A
192.168.0.25:  25%|██▌       | 117/460 [04:50<14:17,  2.50s/it][A
192.168.0.25:  26%|██▌       | 118/460 [04:52<14:15,  2.50s/it][A
192.168.0.25:  26%|██▌       | 119/460 [04:55<14:12,  2.50s/it][A
192.168.0.25:  26%|██▌       | 120/460 [04:57<14:10,  2.50s/it][A
192.168.0.25:  26%|██▋       | 121/460 [05:00<14:07,  2.50s/it][A
192.168.0.25:  27%|██▋       | 122/460 [05:02<14:05,  2.50s/it][A
192.168.0.25:  27%|██▋       | 123/460 [05:05<14:02,  2.50s/it][A
192.168.0.25:  27%|██▋       | 124/460 [05:07<14:00,  2.50s/it][A
192.168.0.25:  27%|██▋       | 125/460 [05:10<13:57,  2.50s/it][A
192.168.0.25:  27%|██▋       | 126/460 [05:12<13:55,  2.50s/it][A
192.168.0.25:  28%|██▊       | 127/460 [05:15<13:52,  2.50s/it][A
192.168.0.25:  28%|██▊       | 128/460 [05:17<13:50,  2.50s/it][A
192.168.0.25:  28%|██▊       | 129/460 [05:20<13:48,  2.50s/it][A
192.168.0.25:  28%|██▊       | 130/460 [05:22<13:45,  2.50s/it][A
192.168.0.25:  28%|██▊       | 131/460 [05:25<13:43,  2.50s/it][A
192.168.0.25:  29%|██▊       | 132/460 [05:27<13:41,  2.51s/it][A
192.168.0.25:  29%|██▉       | 133/460 [05:30<13:38,  2.50s/it][A
192.168.0.25:  29%|██▉       | 134/460 [05:32<13:36,  2.50s/it][A
192.168.0.25:  29%|██▉       | 135/460 [05:35<13:33,  2.50s/it][A
192.168.0.25:  30%|██▉       | 136/460 [05:37<13:31,  2.50s/it][A
192.168.0.25:  30%|██▉       | 137/460 [05:40<13:28,  2.50s/it][A
192.168.0.25:  30%|███       | 138/460 [05:42<13:25,  2.50s/it][A
192.168.0.25:  30%|███       | 139/460 [05:45<13:23,  2.50s/it][A
192.168.0.25:  30%|███       | 140/460 [05:47<13:20,  2.50s/it][A
192.168.0.25:  31%|███       | 141/460 [05:50<13:18,  2.50s/it][A
192.168.0.25:  31%|███       | 142/460 [05:52<13:15,  2.50s/it][A
192.168.0.25:  31%|███       | 143/460 [05:55<13:13,  2.50s/it][A
192.168.0.25:  31%|███▏      | 144/460 [05:57<13:11,  2.50s/it][A
192.168.0.25:  32%|███▏      | 145/460 [06:00<13:08,  2.50s/it][A
192.168.0.25:  32%|███▏      | 146/460 [06:02<13:06,  2.50s/it][A
192.168.0.25:  32%|███▏      | 147/460 [06:05<13:03,  2.50s/it][A
192.168.0.25:  32%|███▏      | 148/460 [06:07<13:01,  2.50s/it][A
192.168.0.25:  32%|███▏      | 149/460 [06:10<12:58,  2.50s/it][A
192.168.0.25:  33%|███▎      | 150/460 [06:12<12:56,  2.50s/it][A
192.168.0.25:  33%|███▎      | 151/460 [06:15<12:53,  2.50s/it][A
192.168.0.25:  33%|███▎      | 152/460 [06:17<12:50,  2.50s/it][A
192.168.0.25:  33%|███▎      | 153/460 [06:20<12:48,  2.50s/it][A
192.168.0.25:  33%|███▎      | 154/460 [06:22<12:45,  2.50s/it][A
192.168.0.25:  34%|███▎      | 155/460 [06:25<12:42,  2.50s/it][A
192.168.0.25:  34%|███▍      | 156/460 [06:27<12:40,  2.50s/it][A
192.168.0.25:  34%|███▍      | 157/460 [06:30<12:38,  2.50s/it][A
192.168.0.25:  34%|███▍      | 158/460 [06:32<12:35,  2.50s/it][A
192.168.0.25:  35%|███▍      | 159/460 [06:35<12:33,  2.50s/it][A
192.168.0.25:  35%|███▍      | 160/460 [06:37<12:30,  2.50s/it][A
192.168.0.25:  35%|███▌      | 161/460 [06:40<12:28,  2.50s/it][A
192.168.0.25:  35%|███▌      | 162/460 [06:42<12:25,  2.50s/it][A
192.168.0.25:  35%|███▌      | 163/460 [06:45<12:23,  2.50s/it][A
192.168.0.25:  36%|███▌      | 164/460 [06:47<12:20,  2.50s/it][A
192.168.0.25:  36%|███▌      | 165/460 [06:50<12:18,  2.50s/it][A
192.168.0.25:  36%|███▌      | 166/460 [06:52<12:15,  2.50s/it][A
192.168.0.25:  36%|███▋      | 167/460 [06:55<12:13,  2.50s/it][A
192.168.0.25:  37%|███▋      | 168/460 [06:57<12:10,  2.50s/it][A
192.168.0.25:  37%|███▋      | 169/460 [07:00<12:08,  2.50s/it][A
192.168.0.25:  37%|███▋      | 170/460 [07:02<12:05,  2.50s/it][A
192.168.0.25:  37%|███▋      | 171/460 [07:05<12:03,  2.50s/it][A
192.168.0.25:  37%|███▋      | 172/460 [07:07<12:00,  2.50s/it][A
192.168.0.25:  38%|███▊      | 173/460 [07:10<11:58,  2.50s/it][A
192.168.0.25:  38%|███▊      | 174/460 [07:12<11:55,  2.50s/it][A
192.168.0.25:  38%|███▊      | 175/460 [07:15<11:53,  2.50s/it][A
192.168.0.25:  38%|███▊      | 176/460 [07:17<11:50,  2.50s/it][A
192.168.0.25:  38%|███▊      | 177/460 [07:20<11:48,  2.50s/it][A
192.168.0.25:  39%|███▊      | 178/460 [07:22<11:45,  2.50s/it][A
192.168.0.25:  39%|███▉      | 179/460 [07:25<11:43,  2.50s/it][A
192.168.0.25:  39%|███▉      | 180/460 [07:27<11:41,  2.50s/it][A
192.168.0.25:  39%|███▉      | 181/460 [07:30<11:38,  2.50s/it][A
192.168.0.25:  40%|███▉      | 182/460 [07:32<11:36,  2.50s/it][A
192.168.0.25:  40%|███▉      | 183/460 [07:35<11:33,  2.50s/it][A
192.168.0.25:  40%|████      | 184/460 [07:37<11:30,  2.50s/it][A
192.168.0.25:  40%|████      | 185/460 [07:40<11:28,  2.50s/it][A
192.168.0.25:  40%|████      | 186/460 [07:42<11:25,  2.50s/it][A
192.168.0.25:  41%|████      | 187/460 [07:45<11:23,  2.50s/it][A
192.168.0.25:  41%|████      | 188/460 [07:47<11:20,  2.50s/it][A
192.168.0.25:  41%|████      | 189/460 [07:50<11:17,  2.50s/it][A
192.168.0.25:  41%|████▏     | 190/460 [07:52<11:15,  2.50s/it][A
192.168.0.25:  42%|████▏     | 191/460 [07:55<11:12,  2.50s/it][A
192.168.0.25:  42%|████▏     | 192/460 [07:57<11:10,  2.50s/it][A
192.168.0.25:  42%|████▏     | 193/460 [08:00<11:07,  2.50s/it][A
192.168.0.25:  42%|████▏     | 194/460 [08:02<11:05,  2.50s/it][A
192.168.0.25:  42%|████▏     | 195/460 [08:05<11:02,  2.50s/it][A
192.168.0.25:  43%|████▎     | 196/460 [08:07<11:00,  2.50s/it][A
192.168.0.25:  43%|████▎     | 197/460 [08:10<10:57,  2.50s/it][A
192.168.0.25:  43%|████▎     | 198/460 [08:12<10:55,  2.50s/it][A
192.168.0.25:  43%|████▎     | 199/460 [08:15<10:52,  2.50s/it][A
192.168.0.25:  43%|████▎     | 200/460 [08:17<10:50,  2.50s/it][A
192.168.0.25:  44%|████▎     | 201/460 [08:20<10:48,  2.50s/it][A
192.168.0.25:  44%|████▍     | 202/460 [08:22<10:45,  2.50s/it][A
192.168.0.25:  44%|████▍     | 203/460 [08:25<10:43,  2.50s/it][A
192.168.0.25:  44%|████▍     | 204/460 [08:27<10:40,  2.50s/it][A
192.168.0.25:  45%|████▍     | 205/460 [08:30<10:38,  2.50s/it][A
192.168.0.25:  45%|████▍     | 206/460 [08:32<10:35,  2.50s/it][A
192.168.0.25:  45%|████▌     | 207/460 [08:35<10:33,  2.50s/it][A
192.168.0.25:  45%|████▌     | 208/460 [08:37<10:30,  2.50s/it][A
192.168.0.25:  45%|████▌     | 209/460 [08:40<10:28,  2.50s/it][A
192.168.0.25:  46%|████▌     | 210/460 [08:42<10:25,  2.50s/it][A
192.168.0.25:  46%|████▌     | 211/460 [08:45<10:23,  2.50s/it][A
192.168.0.25:  46%|████▌     | 212/460 [08:47<10:21,  2.50s/it][A
192.168.0.25:  46%|████▋     | 213/460 [08:50<10:18,  2.50s/it][A
192.168.0.25:  47%|████▋     | 214/460 [08:52<10:16,  2.50s/it][A
192.168.0.25:  47%|████▋     | 215/460 [08:55<10:13,  2.50s/it][A
192.168.0.25:  47%|████▋     | 216/460 [08:57<10:11,  2.50s/it][A
192.168.0.25:  47%|████▋     | 217/460 [09:00<10:08,  2.50s/it][A
192.168.0.25:  47%|████▋     | 218/460 [09:02<10:06,  2.50s/it][A
192.168.0.25:  48%|████▊     | 219/460 [09:05<10:03,  2.50s/it][A
192.168.0.25:  48%|████▊     | 220/460 [09:07<10:00,  2.50s/it][A
192.168.0.25:  48%|████▊     | 221/460 [09:10<09:58,  2.50s/it][A
192.168.0.25:  48%|████▊     | 222/460 [09:12<09:55,  2.50s/it][A
192.168.0.25:  48%|████▊     | 223/460 [09:15<09:53,  2.50s/it][A
192.168.0.25:  49%|████▊     | 224/460 [09:17<09:50,  2.50s/it][A
192.168.0.25:  49%|████▉     | 225/460 [09:20<09:47,  2.50s/it][A
192.168.0.25:  49%|████▉     | 226/460 [09:22<09:45,  2.50s/it][A
192.168.0.25:  49%|████▉     | 227/460 [09:25<09:43,  2.50s/it][A
192.168.0.25:  50%|████▉     | 228/460 [09:27<09:40,  2.50s/it][A
192.168.0.25:  50%|████▉     | 229/460 [09:30<09:38,  2.50s/it][A
192.168.0.25:  50%|█████     | 230/460 [09:32<09:35,  2.50s/it][A
192.168.0.25:  50%|█████     | 231/460 [09:35<09:33,  2.50s/it][A
192.168.0.25:  50%|█████     | 232/460 [09:37<09:30,  2.50s/it][A
192.168.0.25:  51%|█████     | 233/460 [09:40<09:27,  2.50s/it][A
192.168.0.25:  51%|█████     | 234/460 [09:42<09:25,  2.50s/it][A
192.168.0.25:  51%|█████     | 235/460 [09:45<09:22,  2.50s/it][A
192.168.0.25:  51%|█████▏    | 236/460 [09:47<09:20,  2.50s/it][A
192.168.0.25:  52%|█████▏    | 237/460 [09:50<09:17,  2.50s/it][A
192.168.0.25:  52%|█████▏    | 238/460 [09:52<09:15,  2.50s/it][A
192.168.0.25:  52%|█████▏    | 239/460 [09:55<09:12,  2.50s/it][A
192.168.0.25:  52%|█████▏    | 240/460 [09:58<09:10,  2.50s/it][A
192.168.0.25:  52%|█████▏    | 241/460 [10:00<09:07,  2.50s/it][A
192.168.0.25:  53%|█████▎    | 242/460 [10:03<09:05,  2.50s/it][A
192.168.0.25:  53%|█████▎    | 243/460 [10:05<09:02,  2.50s/it][A
192.168.0.25:  53%|█████▎    | 244/460 [10:08<09:00,  2.50s/it][A
192.168.0.25:  53%|█████▎    | 245/460 [10:10<08:58,  2.50s/it][A
192.168.0.25:  53%|█████▎    | 246/460 [10:13<08:55,  2.50s/it][A
192.168.0.25:  54%|█████▎    | 247/460 [10:15<08:53,  2.50s/it][A
192.168.0.25:  54%|█████▍    | 248/460 [10:18<08:50,  2.50s/it][A
192.168.0.25:  54%|█████▍    | 249/460 [10:20<08:47,  2.50s/it][A
192.168.0.25:  54%|█████▍    | 250/460 [10:23<08:45,  2.50s/it][A
192.168.0.25:  55%|█████▍    | 251/460 [10:25<08:42,  2.50s/it][A
192.168.0.25:  55%|█████▍    | 252/460 [10:28<08:40,  2.50s/it][A
192.168.0.25:  55%|█████▌    | 253/460 [10:30<08:37,  2.50s/it][A
192.168.0.25:  55%|█████▌    | 254/460 [10:33<08:35,  2.50s/it][A
192.168.0.25:  55%|█████▌    | 255/460 [10:35<08:33,  2.50s/it][A
192.168.0.25:  56%|█████▌    | 256/460 [10:38<08:30,  2.50s/it][A
192.168.0.25:  56%|█████▌    | 257/460 [10:40<08:28,  2.50s/it][A
192.168.0.25:  56%|█████▌    | 258/460 [10:43<08:25,  2.50s/it][A
192.168.0.25:  56%|█████▋    | 259/460 [10:45<08:23,  2.50s/it][A
192.168.0.25:  57%|█████▋    | 260/460 [10:48<08:20,  2.50s/it][A
192.168.0.25:  57%|█████▋    | 261/460 [10:50<08:17,  2.50s/it][A
192.168.0.25:  57%|█████▋    | 262/460 [10:53<08:15,  2.50s/it][A
192.168.0.25:  57%|█████▋    | 263/460 [10:55<08:13,  2.50s/it][A
192.168.0.25:  57%|█████▋    | 264/460 [10:58<08:10,  2.50s/it][A
192.168.0.25:  58%|█████▊    | 265/460 [11:00<08:08,  2.50s/it][A
192.168.0.25:  58%|█████▊    | 266/460 [11:03<08:05,  2.50s/it][A
192.168.0.25:  58%|█████▊    | 267/460 [11:05<08:03,  2.50s/it][A
192.168.0.25:  58%|█████▊    | 268/460 [11:08<08:00,  2.50s/it][A
192.168.0.25:  58%|█████▊    | 269/460 [11:10<07:58,  2.50s/it][A
192.168.0.25:  59%|█████▊    | 270/460 [11:13<07:55,  2.50s/it][A
192.168.0.25:  59%|█████▉    | 271/460 [11:15<07:53,  2.50s/it][A
192.168.0.25:  59%|█████▉    | 272/460 [11:18<07:50,  2.51s/it][A
192.168.0.25:  59%|█████▉    | 273/460 [11:20<07:48,  2.50s/it][A
192.168.0.25:  60%|█████▉    | 274/460 [11:23<07:45,  2.50s/it][A
192.168.0.25:  60%|█████▉    | 275/460 [11:25<07:43,  2.50s/it][A
192.168.0.25:  60%|██████    | 276/460 [11:28<07:40,  2.50s/it][A
192.168.0.25:  60%|██████    | 277/460 [11:30<07:37,  2.50s/it][A
192.168.0.25:  60%|██████    | 278/460 [11:33<07:35,  2.50s/it][A
192.168.0.25:  61%|██████    | 279/460 [11:35<07:32,  2.50s/it][A
192.168.0.25:  61%|██████    | 280/460 [11:38<07:30,  2.50s/it][A
192.168.0.25:  61%|██████    | 281/460 [11:40<07:28,  2.50s/it][A
192.168.0.25:  61%|██████▏   | 282/460 [11:43<07:26,  2.51s/it][A
192.168.0.25:  62%|██████▏   | 283/460 [11:45<07:23,  2.51s/it][A
192.168.0.25:  62%|██████▏   | 284/460 [11:48<07:20,  2.51s/it][A
192.168.0.25:  62%|██████▏   | 285/460 [11:50<07:18,  2.51s/it][A
192.168.0.25:  62%|██████▏   | 286/460 [11:53<07:15,  2.51s/it][A
192.168.0.25:  62%|██████▏   | 287/460 [11:55<07:13,  2.51s/it][A
192.168.0.25:  63%|██████▎   | 288/460 [11:58<07:10,  2.51s/it][A
192.168.0.25:  63%|██████▎   | 289/460 [12:00<07:08,  2.50s/it][A
192.168.0.25:  63%|██████▎   | 290/460 [12:03<07:05,  2.50s/it][A
192.168.0.25:  63%|██████▎   | 291/460 [12:05<07:03,  2.50s/it][A
192.168.0.25:  63%|██████▎   | 292/460 [12:08<07:00,  2.50s/it][A
192.168.0.25:  64%|██████▎   | 293/460 [12:10<06:58,  2.50s/it][A
192.168.0.25:  64%|██████▍   | 294/460 [12:13<06:55,  2.50s/it][A
192.168.0.25:  64%|██████▍   | 295/460 [12:15<06:53,  2.50s/it][A
192.168.0.25:  64%|██████▍   | 296/460 [12:18<06:50,  2.50s/it][A
192.168.0.25:  65%|██████▍   | 297/460 [12:20<06:48,  2.50s/it][A
192.168.0.25:  65%|██████▍   | 298/460 [12:23<06:45,  2.50s/it][A
192.168.0.25:  65%|██████▌   | 299/460 [12:25<06:42,  2.50s/it][A
192.168.0.25:  65%|██████▌   | 300/460 [12:28<06:40,  2.50s/it][A
192.168.0.25:  65%|██████▌   | 301/460 [12:30<06:38,  2.50s/it][A
192.168.0.25:  66%|██████▌   | 302/460 [12:33<06:35,  2.50s/it][A
192.168.0.25:  66%|██████▌   | 303/460 [12:35<06:33,  2.50s/it][A
192.168.0.25:  66%|██████▌   | 304/460 [12:38<06:30,  2.50s/it][A
192.168.0.25:  66%|██████▋   | 305/460 [12:40<06:27,  2.50s/it][A
192.168.0.25:  67%|██████▋   | 306/460 [12:43<06:25,  2.50s/it][A
192.168.0.25:  67%|██████▋   | 307/460 [12:45<06:23,  2.50s/it][A
192.168.0.25:  67%|██████▋   | 308/460 [12:48<06:20,  2.50s/it][A
192.168.0.25:  67%|██████▋   | 309/460 [12:50<06:18,  2.50s/it][A
192.168.0.25:  67%|██████▋   | 310/460 [12:53<06:15,  2.50s/it][A
192.168.0.25:  68%|██████▊   | 311/460 [12:55<06:13,  2.50s/it][A
192.168.0.25:  68%|██████▊   | 312/460 [12:58<06:10,  2.50s/it][A
192.168.0.25:  68%|██████▊   | 313/460 [13:00<06:07,  2.50s/it][A
192.168.0.25:  68%|██████▊   | 314/460 [13:03<06:05,  2.50s/it][A
192.168.0.25:  68%|██████▊   | 315/460 [13:05<06:03,  2.50s/it][A
192.168.0.25:  69%|██████▊   | 316/460 [13:08<06:00,  2.51s/it][A
192.168.0.25:  69%|██████▉   | 317/460 [13:10<05:58,  2.51s/it][A
192.168.0.25:  69%|██████▉   | 318/460 [13:13<05:55,  2.51s/it][A
192.168.0.25:  69%|██████▉   | 319/460 [13:15<05:53,  2.51s/it][A
192.168.0.25:  70%|██████▉   | 320/460 [13:18<05:50,  2.50s/it][A
192.168.0.25:  70%|██████▉   | 321/460 [13:20<05:48,  2.50s/it][A
192.168.0.25:  70%|███████   | 322/460 [13:23<05:45,  2.50s/it][A
192.168.0.25:  70%|███████   | 323/460 [13:25<05:43,  2.50s/it][A
192.168.0.25:  70%|███████   | 324/460 [13:28<05:40,  2.50s/it][A
192.168.0.25:  71%|███████   | 325/460 [13:30<05:38,  2.50s/it][A
192.168.0.25:  71%|███████   | 326/460 [13:33<05:35,  2.50s/it][A
192.168.0.25:  71%|███████   | 327/460 [13:35<05:32,  2.50s/it][A
192.168.0.25:  71%|███████▏  | 328/460 [13:38<05:30,  2.50s/it][A
192.168.0.25:  72%|███████▏  | 329/460 [13:40<05:28,  2.50s/it][A
192.168.0.25:  72%|███████▏  | 330/460 [13:43<05:25,  2.50s/it][A
192.168.0.25:  72%|███████▏  | 331/460 [13:45<05:23,  2.50s/it][A
192.168.0.25:  72%|███████▏  | 332/460 [13:48<05:20,  2.50s/it][A
192.168.0.25:  72%|███████▏  | 333/460 [13:50<05:18,  2.50s/it][A
192.168.0.25:  73%|███████▎  | 334/460 [13:53<05:15,  2.50s/it][A
192.168.0.25:  73%|███████▎  | 335/460 [13:55<05:12,  2.50s/it][A
192.168.0.25:  73%|███████▎  | 336/460 [13:58<05:10,  2.50s/it][A
192.168.0.25:  73%|███████▎  | 337/460 [14:00<05:08,  2.50s/it][A
192.168.0.25:  73%|███████▎  | 338/460 [14:03<05:05,  2.50s/it][A
192.168.0.25:  74%|███████▎  | 339/460 [14:05<05:02,  2.50s/it][A
192.168.0.25:  74%|███████▍  | 340/460 [14:08<05:00,  2.50s/it][A
192.168.0.25:  74%|███████▍  | 341/460 [14:10<04:57,  2.50s/it][A
192.168.0.25:  74%|███████▍  | 342/460 [14:13<04:55,  2.50s/it][A
192.168.0.25:  75%|███████▍  | 343/460 [14:15<04:52,  2.50s/it][A
192.168.0.25:  75%|███████▍  | 344/460 [14:18<04:50,  2.50s/it][A
192.168.0.25:  75%|███████▌  | 345/460 [14:20<04:47,  2.50s/it][A
192.168.0.25:  75%|███████▌  | 346/460 [14:23<04:45,  2.50s/it][A
192.168.0.25:  75%|███████▌  | 347/460 [14:25<04:42,  2.50s/it][A
192.168.0.25:  76%|███████▌  | 348/460 [14:28<04:40,  2.50s/it][A
192.168.0.25:  76%|███████▌  | 349/460 [14:30<04:37,  2.50s/it][A
192.168.0.25:  76%|███████▌  | 350/460 [14:33<04:35,  2.50s/it][A
192.168.0.25:  76%|███████▋  | 351/460 [14:35<04:32,  2.50s/it][A
192.168.0.25:  77%|███████▋  | 352/460 [14:38<04:30,  2.50s/it][A
192.168.0.25:  77%|███████▋  | 353/460 [14:40<04:27,  2.50s/it][A
192.168.0.25:  77%|███████▋  | 354/460 [14:43<04:25,  2.50s/it][A
192.168.0.25:  77%|███████▋  | 355/460 [14:45<04:22,  2.50s/it][A
192.168.0.25:  77%|███████▋  | 356/460 [14:48<04:20,  2.50s/it][A
192.168.0.25:  78%|███████▊  | 357/460 [14:50<04:17,  2.50s/it][A
192.168.0.25:  78%|███████▊  | 358/460 [14:53<04:15,  2.50s/it][A
192.168.0.25:  78%|███████▊  | 359/460 [14:55<04:12,  2.50s/it][A
192.168.0.25:  78%|███████▊  | 360/460 [14:58<04:10,  2.50s/it][A
192.168.0.25:  78%|███████▊  | 361/460 [15:00<04:07,  2.50s/it][A
192.168.0.25:  79%|███████▊  | 362/460 [15:03<04:05,  2.50s/it][A
192.168.0.25:  79%|███████▉  | 363/460 [15:05<04:02,  2.50s/it][A
192.168.0.25:  79%|███████▉  | 364/460 [15:08<04:00,  2.50s/it][A
192.168.0.25:  79%|███████▉  | 365/460 [15:10<03:58,  2.51s/it][A
192.168.0.25:  80%|███████▉  | 366/460 [15:13<03:55,  2.51s/it][A
192.168.0.25:  80%|███████▉  | 367/460 [15:15<03:53,  2.51s/it][A
192.168.0.25:  80%|████████  | 368/460 [15:18<03:50,  2.51s/it][A
192.168.0.25:  80%|████████  | 369/460 [15:20<03:47,  2.50s/it][A
192.168.0.25:  80%|████████  | 370/460 [15:23<03:45,  2.50s/it][A
192.168.0.25:  81%|████████  | 371/460 [15:25<03:42,  2.50s/it][A
192.168.0.25:  81%|████████  | 372/460 [15:28<03:40,  2.50s/it][A
192.168.0.25:  81%|████████  | 373/460 [15:30<03:37,  2.50s/it][A
192.168.0.25:  81%|████████▏ | 374/460 [15:33<03:35,  2.50s/it][A
192.168.0.25:  82%|████████▏ | 375/460 [15:36<03:32,  2.50s/it][A
192.168.0.25:  82%|████████▏ | 376/460 [15:38<03:30,  2.51s/it][A
192.168.0.25:  82%|████████▏ | 377/460 [15:41<03:27,  2.50s/it][A
192.168.0.25:  82%|████████▏ | 378/460 [15:43<03:25,  2.50s/it][A
192.168.0.25:  82%|████████▏ | 379/460 [15:46<03:22,  2.50s/it][A
192.168.0.25:  83%|████████▎ | 380/460 [15:48<03:20,  2.50s/it][A
192.168.0.25:  83%|████████▎ | 381/460 [15:51<03:17,  2.50s/it][A
192.168.0.25:  83%|████████▎ | 382/460 [15:53<03:15,  2.50s/it][A
192.168.0.25:  83%|████████▎ | 383/460 [15:56<03:12,  2.50s/it][A
192.168.0.25:  83%|████████▎ | 384/460 [15:58<03:10,  2.50s/it][A
192.168.0.25:  84%|████████▎ | 385/460 [16:01<03:07,  2.50s/it][A
192.168.0.25:  84%|████████▍ | 386/460 [16:03<03:05,  2.50s/it][A
192.168.0.25:  84%|████████▍ | 387/460 [16:06<03:02,  2.50s/it][A
192.168.0.25:  84%|████████▍ | 388/460 [16:08<03:00,  2.50s/it][A
192.168.0.25:  85%|████████▍ | 389/460 [16:11<02:57,  2.50s/it][A
192.168.0.25:  85%|████████▍ | 390/460 [16:13<02:55,  2.50s/it][A
192.168.0.25:  85%|████████▌ | 391/460 [16:16<02:52,  2.50s/it][A
192.168.0.25:  85%|████████▌ | 392/460 [16:18<02:50,  2.50s/it][A
192.168.0.25:  85%|████████▌ | 393/460 [16:21<02:47,  2.50s/it][A
192.168.0.25:  86%|████████▌ | 394/460 [16:23<02:45,  2.50s/it][A
192.168.0.25:  86%|████████▌ | 395/460 [16:26<02:42,  2.50s/it][A
192.168.0.25:  86%|████████▌ | 396/460 [16:28<02:40,  2.50s/it][A
192.168.0.25:  86%|████████▋ | 397/460 [16:31<02:37,  2.50s/it][A
192.168.0.25:  87%|████████▋ | 398/460 [16:33<02:35,  2.50s/it][A
192.168.0.25:  87%|████████▋ | 399/460 [16:36<02:32,  2.50s/it][A
192.168.0.25:  87%|████████▋ | 400/460 [16:38<02:30,  2.50s/it][A
192.168.0.25:  87%|████████▋ | 401/460 [16:41<02:27,  2.50s/it][A
192.168.0.25:  87%|████████▋ | 402/460 [16:43<02:25,  2.50s/it][A
192.168.0.25:  88%|████████▊ | 403/460 [16:46<02:22,  2.50s/it][A
192.168.0.25:  88%|████████▊ | 404/460 [16:48<02:20,  2.50s/it][A
192.168.0.25:  88%|████████▊ | 405/460 [16:51<02:17,  2.50s/it][A
192.168.0.25:  88%|████████▊ | 406/460 [16:53<02:15,  2.50s/it][A
192.168.0.25:  88%|████████▊ | 407/460 [16:56<02:12,  2.50s/it][A
192.168.0.25:  89%|████████▊ | 408/460 [16:58<02:10,  2.51s/it][A
192.168.0.25:  89%|████████▉ | 409/460 [17:01<02:07,  2.51s/it][A
192.168.0.25:  89%|████████▉ | 410/460 [17:03<02:05,  2.50s/it][A
192.168.0.25:  89%|████████▉ | 411/460 [17:06<02:02,  2.50s/it][A
192.168.0.25:  90%|████████▉ | 412/460 [17:08<02:00,  2.50s/it][A
192.168.0.25:  90%|████████▉ | 413/460 [17:11<01:57,  2.50s/it][A
192.168.0.25:  90%|█████████ | 414/460 [17:13<01:55,  2.50s/it][A
192.168.0.25:  90%|█████████ | 415/460 [17:16<01:52,  2.50s/it][A
192.168.0.25:  90%|█████████ | 416/460 [17:18<01:50,  2.50s/it][A
192.168.0.25:  91%|█████████ | 417/460 [17:21<01:47,  2.50s/it][A
192.168.0.25:  91%|█████████ | 418/460 [17:23<01:45,  2.50s/it][A
192.168.0.25:  91%|█████████ | 419/460 [17:26<01:42,  2.50s/it][A
192.168.0.25:  91%|█████████▏| 420/460 [17:28<01:40,  2.50s/it][A
192.168.0.25:  92%|█████████▏| 421/460 [17:31<01:37,  2.50s/it][A
192.168.0.25:  92%|█████████▏| 422/460 [17:33<01:35,  2.50s/it][A
192.168.0.25:  92%|█████████▏| 423/460 [17:36<01:32,  2.50s/it][A
192.168.0.25:  92%|█████████▏| 424/460 [17:38<01:30,  2.50s/it][A
192.168.0.25:  92%|█████████▏| 425/460 [17:41<01:27,  2.50s/it][A
192.168.0.25:  93%|█████████▎| 426/460 [17:43<01:25,  2.50s/it][A
192.168.0.25:  93%|█████████▎| 427/460 [17:46<01:22,  2.50s/it][A
192.168.0.25:  93%|█████████▎| 428/460 [17:48<01:20,  2.50s/it][A
192.168.0.25:  93%|█████████▎| 429/460 [17:51<01:17,  2.50s/it][A
192.168.0.25:  93%|█████████▎| 430/460 [17:53<01:15,  2.50s/it][A
192.168.0.25:  94%|█████████▎| 431/460 [17:56<01:12,  2.50s/it][A
192.168.0.25:  94%|█████████▍| 432/460 [17:58<01:10,  2.50s/it][A
192.168.0.25:  94%|█████████▍| 433/460 [18:01<01:07,  2.50s/it][A
192.168.0.25:  94%|█████████▍| 434/460 [18:03<01:05,  2.51s/it][A
192.168.0.25:  95%|█████████▍| 435/460 [18:06<01:02,  2.51s/it][A
192.168.0.25:  95%|█████████▍| 436/460 [18:08<01:00,  2.51s/it][A
192.168.0.25:  95%|█████████▌| 437/460 [18:11<00:57,  2.51s/it][A
192.168.0.25:  95%|█████████▌| 438/460 [18:13<00:55,  2.51s/it][A
192.168.0.25:  95%|█████████▌| 439/460 [18:16<00:52,  2.51s/it][A
192.168.0.25:  96%|█████████▌| 440/460 [18:18<00:50,  2.51s/it][A
192.168.0.25:  96%|█████████▌| 441/460 [18:21<00:47,  2.51s/it][A
192.168.0.25:  96%|█████████▌| 442/460 [18:23<00:45,  2.51s/it][A
192.168.0.25:  96%|█████████▋| 443/460 [18:26<00:42,  2.50s/it][A
192.168.0.25:  97%|█████████▋| 444/460 [18:28<00:40,  2.50s/it][A
192.168.0.25:  97%|█████████▋| 445/460 [18:31<00:37,  2.50s/it][A
192.168.0.25:  97%|█████████▋| 446/460 [18:33<00:35,  2.50s/it][A
192.168.0.25:  97%|█████████▋| 447/460 [18:36<00:32,  2.50s/it][A
192.168.0.25:  97%|█████████▋| 448/460 [18:38<00:30,  2.50s/it][A
192.168.0.25:  98%|█████████▊| 449/460 [18:41<00:27,  2.50s/it][A
192.168.0.25:  98%|█████████▊| 450/460 [18:43<00:25,  2.50s/it][A
192.168.0.25:  98%|█████████▊| 451/460 [18:46<00:22,  2.50s/it][A
192.168.0.25:  98%|█████████▊| 452/460 [18:48<00:20,  2.50s/it][A
192.168.0.25:  98%|█████████▊| 453/460 [18:51<00:17,  2.50s/it][A
192.168.0.25:  99%|█████████▊| 454/460 [18:53<00:15,  2.50s/it][A
192.168.0.25:  99%|█████████▉| 455/460 [18:56<00:12,  2.50s/it][A
192.168.0.25:  99%|█████████▉| 456/460 [18:58<00:10,  2.50s/it][A
192.168.0.25:  99%|█████████▉| 457/460 [19:01<00:07,  2.50s/it][A
192.168.0.25: 100%|█████████▉| 458/460 [19:03<00:05,  2.50s/it][A
192.168.0.25: 100%|█████████▉| 459/460 [19:06<00:02,  2.50s/it][A
192.168.0.25: 100%|██████████| 460/460 [19:08<00:00,  2.50s/it][A                                                       
192.168.0.25: {'eval_loss': 3.309122323989868, 'eval_runtime': 1151.3575, 'eval_samples_per_second': 12.774, 'eval_steps_per_second': 0.4, 'epoch': 1.93}
192.168.0.25:                                                  [A 64%|██████▍   | 1000/1554 [3:38:17<1:37:06, 10.52s/it]
192.168.0.25: 100%|██████████| 460/460 [19:09<00:00,  2.50s/it][A
192.168.0.25:                                                  [A[INFO|trainer.py:3503] 2024-09-29 15:10:49,804 >> Saving model checkpoint to /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1000
192.168.0.25: [INFO|configuration_utils.py:472] 2024-09-29 15:10:49,810 >> Configuration saved in /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1000/config.json
192.168.0.25: [INFO|configuration_utils.py:807] 2024-09-29 15:10:49,811 >> Configuration saved in /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1000/generation_config.json
192.168.0.25: [INFO|modeling_utils.py:2807] 2024-09-29 15:11:19,663 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1000/model.safetensors.index.json.
192.168.0.25: [INFO|tokenization_utils_base.py:2684] 2024-09-29 15:11:19,684 >> tokenizer config file saved in /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1000/tokenizer_config.json
192.168.0.25: [INFO|tokenization_utils_base.py:2693] 2024-09-29 15:11:19,685 >> Special tokens file saved in /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1000/special_tokens_map.json
192.168.0.25: [2024-09-29 15:11:20,032] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1000 is about to be saved!
192.168.0.13: [2024-09-29 15:11:20,534] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1000/global_step1000/zero_pp_rank_24_mp_rank_00_model_states.pt...
192.168.0.149: [2024-09-29 15:11:23,133] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1000/global_step1000/zero_pp_rank_8_mp_rank_00_model_states.pt...
192.168.0.89: [2024-09-29 15:11:04,419] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1000/global_step1000/zero_pp_rank_16_mp_rank_00_model_states.pt...
192.168.0.25: [2024-09-29 15:11:20,101] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1000/global_step1000/zero_pp_rank_0_mp_rank_00_model_states.pt
192.168.0.25: [2024-09-29 15:11:20,101] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1000/global_step1000/zero_pp_rank_0_mp_rank_00_model_states.pt...
192.168.0.13: [2024-09-29 15:11:20,581] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1000/global_step1000/zero_pp_rank_24_mp_rank_00_model_states.pt.
192.168.0.149: [2024-09-29 15:11:23,183] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1000/global_step1000/zero_pp_rank_8_mp_rank_00_model_states.pt.
192.168.0.25: [2024-09-29 15:11:20,147] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1000/global_step1000/zero_pp_rank_0_mp_rank_00_model_states.pt.
192.168.0.89: [2024-09-29 15:11:04,470] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1000/global_step1000/zero_pp_rank_16_mp_rank_00_model_states.pt.
192.168.0.89: [2024-09-29 15:11:04,573] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1000/global_step1000/zero_pp_rank_16_mp_rank_00_optim_states.pt...
192.168.0.25: [2024-09-29 15:11:20,252] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1000/global_step1000/zero_pp_rank_0_mp_rank_00_optim_states.pt...
192.168.0.13: [2024-09-29 15:11:20,695] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1000/global_step1000/zero_pp_rank_24_mp_rank_00_optim_states.pt...
192.168.0.149: [2024-09-29 15:11:23,289] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1000/global_step1000/zero_pp_rank_8_mp_rank_00_optim_states.pt...
192.168.0.89: [2024-09-29 15:11:14,894] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1000/global_step1000/zero_pp_rank_16_mp_rank_00_optim_states.pt.
192.168.0.89: [2024-09-29 15:11:14,894] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1000/global_step1000/zero_pp_rank_16_mp_rank_00_optim_states.pt
192.168.0.149: [2024-09-29 15:11:33,999] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1000/global_step1000/zero_pp_rank_8_mp_rank_00_optim_states.pt.
192.168.0.149: [2024-09-29 15:11:34,000] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1000/global_step1000/zero_pp_rank_8_mp_rank_00_optim_states.pt
192.168.0.13: [2024-09-29 15:11:32,239] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1000/global_step1000/zero_pp_rank_24_mp_rank_00_optim_states.pt.
192.168.0.13: [2024-09-29 15:11:32,239] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1000/global_step1000/zero_pp_rank_24_mp_rank_00_optim_states.pt
192.168.0.25: [2024-09-29 15:11:32,447] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1000/global_step1000/zero_pp_rank_0_mp_rank_00_optim_states.pt.
192.168.0.25: [2024-09-29 15:11:32,448] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1000/global_step1000/zero_pp_rank_0_mp_rank_00_optim_states.pt
192.168.0.13: [2024-09-29 15:11:34,535] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
192.168.0.89: [2024-09-29 15:11:18,413] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
192.168.0.149: [2024-09-29 15:11:37,129] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
192.168.0.25: [2024-09-29 15:11:34,093] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
192.168.0.25: {'loss': 3.278, 'grad_norm': 4.508566861030327, 'learning_rate': 2.7950641089609275e-06, 'epoch': 1.95}
192.168.0.25: {'loss': 3.2911, 'grad_norm': 4.380779046229093, 'learning_rate': 2.70362869263188e-06, 'epoch': 1.97}
192.168.0.25: {'loss': 3.2568, 'grad_norm': 5.676637441873876, 'learning_rate': 2.613156386450174e-06, 'epoch': 1.99}
192.168.0.25: {'loss': 3.2504, 'grad_norm': 4.191066305092493, 'learning_rate': 2.5236851349746242e-06, 'epoch': 2.01}
192.168.0.25: {'loss': 3.2118, 'grad_norm': 4.396137076395368, 'learning_rate': 2.435252462916467e-06, 'epoch': 2.03}
192.168.0.25: {'loss': 3.1947, 'grad_norm': 4.141186715657775, 'learning_rate': 2.3478954594012884e-06, 'epoch': 2.05}
192.168.0.25: {'loss': 3.228, 'grad_norm': 2.9948097471714177, 'learning_rate': 2.2616507624136564e-06, 'epoch': 2.07}
192.168.0.25: {'loss': 3.2238, 'grad_norm': 2.619676914823949, 'learning_rate': 2.176554543430965e-06, 'epoch': 2.08}
192.168.0.25: {'loss': 3.2073, 'grad_norm': 3.9519193114873734, 'learning_rate': 2.092642492252915e-06, 'epoch': 2.1}
192.168.0.25: {'loss': 3.2179, 'grad_norm': 3.1552855273870826, 'learning_rate': 2.0099498020330305e-06, 'epoch': 2.12}
192.168.0.25: {'loss': 3.2135, 'grad_norm': 3.25721965856999, 'learning_rate': 1.928511154518473e-06, 'epoch': 2.14}
192.168.0.25: {'loss': 3.221, 'grad_norm': 3.613370591167445, 'learning_rate': 1.8483607055043234e-06, 'epoch': 2.16}
192.168.0.25: {'loss': 3.2041, 'grad_norm': 3.8829418137813274, 'learning_rate': 1.7695320705084678e-06, 'epoch': 2.18}
192.168.0.25: {'loss': 3.1853, 'grad_norm': 2.217951133795124, 'learning_rate': 1.6920583106730749e-06, 'epoch': 2.2}
192.168.0.25: {'loss': 3.2086, 'grad_norm': 4.779894268829741, 'learning_rate': 1.615971918898581e-06, 'epoch': 2.22}
192.168.0.25: {'loss': 3.2097, 'grad_norm': 2.474779612429445, 'learning_rate': 1.541304806215993e-06, 'epoch': 2.24}
192.168.0.25: {'loss': 3.1952, 'grad_norm': 2.3609701663066787, 'learning_rate': 1.4680882884032333e-06, 'epoch': 2.26}
192.168.0.25: {'loss': 3.1683, 'grad_norm': 2.409071420031279, 'learning_rate': 1.396353072851151e-06, 'epoch': 2.28}
192.168.0.25: {'loss': 3.2082, 'grad_norm': 2.861228633882501, 'learning_rate': 1.3261292456846648e-06, 'epoch': 2.3}
192.168.0.25: {'loss': 3.2256, 'grad_norm': 2.3342130796354814, 'learning_rate': 1.257446259144494e-06, 'epoch': 2.32}
192.168.0.25: {'loss': 3.183, 'grad_norm': 2.148935267265617, 'learning_rate': 1.1903329192347397e-06, 'epoch': 2.34}
192.168.0.25: {'loss': 3.1957, 'grad_norm': 2.42264844787975, 'learning_rate': 1.1248173736414807e-06, 'epoch': 2.36}
192.168.0.25: {'loss': 3.1786, 'grad_norm': 2.60249124164555, 'learning_rate': 1.0609270999275e-06, 'epoch': 2.37}
192.168.0.25: {'loss': 3.1848, 'grad_norm': 2.61960110182996, 'learning_rate': 9.986888940080468e-07, 'epoch': 2.39}
192.168.0.25: {'loss': 3.1781, 'grad_norm': 3.5988653514166034, 'learning_rate': 9.381288589124877e-07, 'epoch': 2.41}
192.168.0.25: {'loss': 3.2034, 'grad_norm': 2.0435547435009074, 'learning_rate': 8.792723938365599e-07, 'epoch': 2.43}
192.168.0.25: {'loss': 3.182, 'grad_norm': 2.511282733193126, 'learning_rate': 8.221441834898175e-07, 'epoch': 2.45}
192.168.0.25: {'loss': 3.1701, 'grad_norm': 2.120114488569217, 'learning_rate': 7.667681877427363e-07, 'epoch': 2.47}
192.168.0.25: {'loss': 3.1738, 'grad_norm': 2.02178274707729, 'learning_rate': 7.131676315778136e-07, 'epoch': 2.49}
192.168.0.25: {'loss': 3.177, 'grad_norm': 1.9749774231404957, 'learning_rate': 6.613649953488921e-07, 'epoch': 2.51}
192.168.0.25: {'loss': 3.1538, 'grad_norm': 1.9816146635328458, 'learning_rate': 6.113820053527835e-07, 'epoch': 2.53}
192.168.0.25: {'loss': 3.199, 'grad_norm': 1.9853643519209616, 'learning_rate': 5.632396247171429e-07, 'epoch': 2.55}
192.168.0.25: {'loss': 3.1615, 'grad_norm': 2.059221609883414, 'learning_rate': 5.169580446084226e-07, 'epoch': 2.57}
192.168.0.25: {'loss': 3.1668, 'grad_norm': 1.9815848852703077, 'learning_rate': 4.7255667576359687e-07, 'epoch': 2.59}
192.168.0.25: {'loss': 3.1693, 'grad_norm': 2.0177445024574507, 'learning_rate': 4.300541403491909e-07, 'epoch': 2.61}
192.168.0.25: {'loss': 3.1529, 'grad_norm': 2.006579662582889, 'learning_rate': 3.894682641510478e-07, 'epoch': 2.63}
192.168.0.25: {'loss': 3.1809, 'grad_norm': 1.8123722031362117, 'learning_rate': 3.508160690981055e-07, 'epoch': 2.64}
192.168.0.25: {'loss': 3.1543, 'grad_norm': 1.8989714197435752, 'learning_rate': 3.1411376612330513e-07, 'epoch': 2.66}
192.168.0.25: {'loss': 3.1771, 'grad_norm': 1.9048867275573682, 'learning_rate': 2.7937674836464256e-07, 'epoch': 2.68}
192.168.0.25: {'loss': 3.1606, 'grad_norm': 1.7972999401999445, 'learning_rate': 2.4661958470920845e-07, 'epoch': 2.7}
192.168.0.25: {'loss': 3.1556, 'grad_norm': 1.8858560362812022, 'learning_rate': 2.1585601368291575e-07, 'epoch': 2.72}
192.168.0.25: {'loss': 3.1765, 'grad_norm': 1.9258684057356923, 'learning_rate': 1.8709893768849406e-07, 'epoch': 2.74}
192.168.0.25: {'loss': 3.1729, 'grad_norm': 1.8580794108187544, 'learning_rate': 1.6036041759415255e-07, 'epoch': 2.76}
192.168.0.25: {'loss': 3.17, 'grad_norm': 1.9050864123472542, 'learning_rate': 1.3565166767519012e-07, 'epoch': 2.78}
192.168.0.25: {'loss': 3.1665, 'grad_norm': 1.8263190062303767, 'learning_rate': 1.1298305091066664e-07, 'epoch': 2.8}
192.168.0.25: {'loss': 3.1585, 'grad_norm': 1.8473435635337938, 'learning_rate': 9.236407463712071e-08, 'epoch': 2.82}
192.168.0.25: {'loss': 3.1574, 'grad_norm': 1.9633982746845275, 'learning_rate': 7.38033865611365e-08, 'epoch': 2.84}
192.168.0.25: {'loss': 3.1618, 'grad_norm': 1.8336370462215255, 'learning_rate': 5.730877113245381e-08, 'epoch': 2.86}
192.168.0.25: {'loss': 3.1664, 'grad_norm': 1.7422273913550625, 'learning_rate': 4.288714627913082e-08, 'epoch': 2.88}
192.168.0.25: {'loss': 3.164, 'grad_norm': 1.7454613686151874, 'learning_rate': 3.0544560506123863e-08, 'epoch': 2.9}
192.168.0.25:  64%|██████▍   | 1001/1554 [3:39:19<57:04:52, 371.60s/it] 64%|██████▍   | 1002/1554 [3:39:31<40:24:44, 263.56s/it] 65%|██████▍   | 1003/1554 [3:39:41<28:41:00, 187.41s/it] 65%|██████▍   | 1004/1554 [3:39:50<20:29:20, 134.11s/it] 65%|██████▍   | 1005/1554 [3:40:00<14:45:33, 96.78s/it]  65%|██████▍   | 1006/1554 [3:40:10<10:45:19, 70.66s/it] 65%|██████▍   | 1007/1554 [3:40:19<7:57:19, 52.36s/it]  65%|██████▍   | 1008/1554 [3:40:29<5:59:56, 39.55s/it] 65%|██████▍   | 1009/1554 [3:40:39<4:37:53, 30.59s/it] 65%|██████▍   | 1010/1554 [3:40:48<3:40:24, 24.31s/it]                                                        65%|██████▍   | 1010/1554 [3:40:48<3:40:24, 24.31s/it] 65%|██████▌   | 1011/1554 [3:40:58<3:00:02, 19.89s/it] 65%|██████▌   | 1012/1554 [3:41:08<2:31:43, 16.80s/it] 65%|██████▌   | 1013/1554 [3:41:17<2:12:03, 14.65s/it] 65%|██████▌   | 1014/1554 [3:41:27<1:58:17, 13.14s/it] 65%|██████▌   | 1015/1554 [3:41:37<1:48:44, 12.10s/it] 65%|██████▌   | 1016/1554 [3:41:46<1:41:56, 11.37s/it] 65%|██████▌   | 1017/1554 [3:41:56<1:37:11, 10.86s/it] 66%|██████▌   | 1018/1554 [3:42:06<1:33:56, 10.52s/it] 66%|██████▌   | 1019/1554 [3:42:15<1:31:38, 10.28s/it] 66%|██████▌   | 1020/1554 [3:42:25<1:30:14, 10.14s/it]                                                        66%|██████▌   | 1020/1554 [3:42:25<1:30:14, 10.14s/it] 66%|██████▌   | 1021/1554 [3:42:35<1:29:29, 10.07s/it] 66%|██████▌   | 1022/1554 [3:42:45<1:29:04, 10.05s/it] 66%|██████▌   | 1023/1554 [3:42:55<1:28:41, 10.02s/it] 66%|██████▌   | 1024/1554 [3:43:05<1:28:42, 10.04s/it] 66%|██████▌   | 1025/1554 [3:43:15<1:28:43, 10.06s/it] 66%|██████▌   | 1026/1554 [3:43:26<1:29:15, 10.14s/it] 66%|██████▌   | 1027/1554 [3:43:36<1:29:08, 10.15s/it] 66%|██████▌   | 1028/1554 [3:43:46<1:29:10, 10.17s/it] 66%|██████▌   | 1029/1554 [3:43:56<1:29:26, 10.22s/it] 66%|██████▋   | 1030/1554 [3:44:07<1:29:50, 10.29s/it]                                                        66%|██████▋   | 1030/1554 [3:44:07<1:29:50, 10.29s/it] 66%|██████▋   | 1031/1554 [3:44:17<1:30:29, 10.38s/it] 66%|██████▋   | 1032/1554 [3:44:28<1:30:25, 10.39s/it] 66%|██████▋   | 1033/1554 [3:44:38<1:30:30, 10.42s/it] 67%|██████▋   | 1034/1554 [3:44:49<1:30:09, 10.40s/it] 67%|██████▋   | 1035/1554 [3:44:59<1:29:54, 10.39s/it] 67%|██████▋   | 1036/1554 [3:45:10<1:30:31, 10.49s/it] 67%|██████▋   | 1037/1554 [3:45:20<1:30:23, 10.49s/it] 67%|██████▋   | 1038/1554 [3:45:31<1:30:17, 10.50s/it] 67%|██████▋   | 1039/1554 [3:45:41<1:30:18, 10.52s/it] 67%|██████▋   | 1040/1554 [3:45:52<1:30:11, 10.53s/it]                                                        67%|██████▋   | 1040/1554 [3:45:52<1:30:11, 10.53s/it] 67%|██████▋   | 1041/1554 [3:46:03<1:30:34, 10.59s/it] 67%|██████▋   | 1042/1554 [3:46:13<1:30:04, 10.56s/it] 67%|██████▋   | 1043/1554 [3:46:24<1:29:54, 10.56s/it] 67%|██████▋   | 1044/1554 [3:46:34<1:29:14, 10.50s/it] 67%|██████▋   | 1045/1554 [3:46:45<1:29:32, 10.56s/it] 67%|██████▋   | 1046/1554 [3:46:55<1:29:50, 10.61s/it] 67%|██████▋   | 1047/1554 [3:47:06<1:29:39, 10.61s/it] 67%|██████▋   | 1048/1554 [3:47:17<1:29:51, 10.66s/it] 68%|██████▊   | 1049/1554 [3:47:27<1:29:01, 10.58s/it] 68%|██████▊   | 1050/1554 [3:47:38<1:28:51, 10.58s/it]                                                        68%|██████▊   | 1050/1554 [3:47:38<1:28:51, 10.58s/it] 68%|██████▊   | 1051/1554 [3:47:48<1:28:03, 10.50s/it] 68%|██████▊   | 1052/1554 [3:47:58<1:27:38, 10.47s/it] 68%|██████▊   | 1053/1554 [3:48:09<1:27:56, 10.53s/it] 68%|██████▊   | 1054/1554 [3:48:20<1:27:27, 10.50s/it] 68%|██████▊   | 1055/1554 [3:48:30<1:27:00, 10.46s/it] 68%|██████▊   | 1056/1554 [3:48:40<1:26:45, 10.45s/it] 68%|██████▊   | 1057/1554 [3:48:51<1:26:07, 10.40s/it] 68%|██████▊   | 1058/1554 [3:49:01<1:25:27, 10.34s/it] 68%|██████▊   | 1059/1554 [3:49:11<1:25:31, 10.37s/it] 68%|██████▊   | 1060/1554 [3:49:22<1:26:36, 10.52s/it]                                                        68%|██████▊   | 1060/1554 [3:49:22<1:26:36, 10.52s/it] 68%|██████▊   | 1061/1554 [3:49:32<1:25:36, 10.42s/it] 68%|██████▊   | 1062/1554 [3:49:43<1:25:27, 10.42s/it] 68%|██████▊   | 1063/1554 [3:49:53<1:25:24, 10.44s/it] 68%|██████▊   | 1064/1554 [3:50:04<1:26:31, 10.59s/it] 69%|██████▊   | 1065/1554 [3:50:15<1:25:47, 10.53s/it] 69%|██████▊   | 1066/1554 [3:50:25<1:25:35, 10.52s/it] 69%|██████▊   | 1067/1554 [3:50:36<1:25:39, 10.55s/it] 69%|██████▊   | 1068/1554 [3:50:46<1:25:07, 10.51s/it] 69%|██████▉   | 1069/1554 [3:50:56<1:24:20, 10.43s/it] 69%|██████▉   | 1070/1554 [3:51:07<1:24:15, 10.44s/it]                                                        69%|██████▉   | 1070/1554 [3:51:07<1:24:15, 10.44s/it] 69%|██████▉   | 1071/1554 [3:51:18<1:25:13, 10.59s/it] 69%|██████▉   | 1072/1554 [3:51:28<1:25:15, 10.61s/it] 69%|██████▉   | 1073/1554 [3:51:39<1:24:21, 10.52s/it] 69%|██████▉   | 1074/1554 [3:51:50<1:25:08, 10.64s/it] 69%|██████▉   | 1075/1554 [3:52:00<1:24:59, 10.65s/it] 69%|██████▉   | 1076/1554 [3:52:11<1:23:59, 10.54s/it] 69%|██████▉   | 1077/1554 [3:52:21<1:24:03, 10.57s/it] 69%|██████▉   | 1078/1554 [3:52:32<1:24:50, 10.69s/it] 69%|██████▉   | 1079/1554 [3:52:43<1:25:31, 10.80s/it] 69%|██████▉   | 1080/1554 [3:52:54<1:25:10, 10.78s/it]                                                        69%|██████▉   | 1080/1554 [3:52:54<1:25:10, 10.78s/it] 70%|██████▉   | 1081/1554 [3:53:05<1:24:43, 10.75s/it] 70%|██████▉   | 1082/1554 [3:53:16<1:24:51, 10.79s/it] 70%|██████▉   | 1083/1554 [3:53:26<1:24:37, 10.78s/it] 70%|██████▉   | 1084/1554 [3:53:37<1:24:02, 10.73s/it] 70%|██████▉   | 1085/1554 [3:53:48<1:23:34, 10.69s/it] 70%|██████▉   | 1086/1554 [3:53:59<1:25:02, 10.90s/it] 70%|██████▉   | 1087/1554 [3:54:10<1:24:52, 10.90s/it] 70%|███████   | 1088/1554 [3:54:20<1:24:01, 10.82s/it] 70%|███████   | 1089/1554 [3:54:31<1:23:56, 10.83s/it] 70%|███████   | 1090/1554 [3:54:42<1:23:05, 10.74s/it]                                                        70%|███████   | 1090/1554 [3:54:42<1:23:05, 10.74s/it] 70%|███████   | 1091/1554 [3:54:53<1:23:41, 10.85s/it] 70%|███████   | 1092/1554 [3:55:03<1:22:51, 10.76s/it] 70%|███████   | 1093/1554 [3:55:14<1:21:43, 10.64s/it] 70%|███████   | 1094/1554 [3:55:24<1:20:54, 10.55s/it] 70%|███████   | 1095/1554 [3:55:35<1:21:40, 10.68s/it] 71%|███████   | 1096/1554 [3:55:46<1:21:28, 10.67s/it] 71%|███████   | 1097/1554 [3:55:57<1:21:56, 10.76s/it] 71%|███████   | 1098/1554 [3:56:07<1:21:34, 10.73s/it] 71%|███████   | 1099/1554 [3:56:18<1:20:53, 10.67s/it] 71%|███████   | 1100/1554 [3:56:29<1:21:04, 10.71s/it]                                                        71%|███████   | 1100/1554 [3:56:29<1:21:04, 10.71s/it] 71%|███████   | 1101/1554 [3:56:40<1:21:13, 10.76s/it] 71%|███████   | 1102/1554 [3:56:50<1:21:00, 10.75s/it] 71%|███████   | 1103/1554 [3:57:01<1:19:48, 10.62s/it] 71%|███████   | 1104/1554 [3:57:11<1:19:47, 10.64s/it] 71%|███████   | 1105/1554 [3:57:22<1:19:02, 10.56s/it] 71%|███████   | 1106/1554 [3:57:32<1:18:29, 10.51s/it] 71%|███████   | 1107/1554 [3:57:43<1:19:01, 10.61s/it] 71%|███████▏  | 1108/1554 [3:57:54<1:18:41, 10.59s/it] 71%|███████▏  | 1109/1554 [3:58:04<1:18:59, 10.65s/it] 71%|███████▏  | 1110/1554 [3:58:15<1:18:00, 10.54s/it]                                                        71%|███████▏  | 1110/1554 [3:58:15<1:18:00, 10.54s/it] 71%|███████▏  | 1111/1554 [3:58:25<1:18:26, 10.62s/it] 72%|███████▏  | 1112/1554 [3:58:36<1:18:14, 10.62s/it] 72%|███████▏  | 1113/1554 [3:58:47<1:18:10, 10.64s/it] 72%|███████▏  | 1114/1554 [3:58:57<1:17:57, 10.63s/it] 72%|███████▏  | 1115/1554 [3:59:08<1:17:46, 10.63s/it] 72%|███████▏  | 1116/1554 [3:59:18<1:17:05, 10.56s/it] 72%|███████▏  | 1117/1554 [3:59:29<1:16:58, 10.57s/it] 72%|███████▏  | 1118/1554 [3:59:40<1:16:55, 10.59s/it] 72%|███████▏  | 1119/1554 [3:59:50<1:16:46, 10.59s/it] 72%|███████▏  | 1120/1554 [4:00:01<1:16:17, 10.55s/it]                                                        72%|███████▏  | 1120/1554 [4:00:01<1:16:17, 10.55s/it] 72%|███████▏  | 1121/1554 [4:00:12<1:16:50, 10.65s/it] 72%|███████▏  | 1122/1554 [4:00:22<1:16:34, 10.63s/it] 72%|███████▏  | 1123/1554 [4:00:33<1:17:00, 10.72s/it] 72%|███████▏  | 1124/1554 [4:00:44<1:16:41, 10.70s/it] 72%|███████▏  | 1125/1554 [4:00:54<1:16:21, 10.68s/it] 72%|███████▏  | 1126/1554 [4:01:05<1:15:48, 10.63s/it] 73%|███████▎  | 1127/1554 [4:01:16<1:17:31, 10.89s/it] 73%|███████▎  | 1128/1554 [4:01:27<1:16:44, 10.81s/it] 73%|███████▎  | 1129/1554 [4:01:37<1:15:49, 10.71s/it] 73%|███████▎  | 1130/1554 [4:01:48<1:15:22, 10.67s/it]                                                        73%|███████▎  | 1130/1554 [4:01:48<1:15:22, 10.67s/it] 73%|███████▎  | 1131/1554 [4:01:59<1:15:07, 10.66s/it] 73%|███████▎  | 1132/1554 [4:02:09<1:15:08, 10.68s/it] 73%|███████▎  | 1133/1554 [4:02:21<1:16:37, 10.92s/it] 73%|███████▎  | 1134/1554 [4:02:32<1:15:55, 10.85s/it] 73%|███████▎  | 1135/1554 [4:02:43<1:16:19, 10.93s/it] 73%|███████▎  | 1136/1554 [4:02:53<1:14:50, 10.74s/it] 73%|███████▎  | 1137/1554 [4:03:03<1:14:09, 10.67s/it] 73%|███████▎  | 1138/1554 [4:03:14<1:14:41, 10.77s/it] 73%|███████▎  | 1139/1554 [4:03:25<1:14:03, 10.71s/it] 73%|███████▎  | 1140/1554 [4:03:37<1:15:55, 11.00s/it]                                                        73%|███████▎  | 1140/1554 [4:03:37<1:15:55, 11.00s/it] 73%|███████▎  | 1141/1554 [4:03:47<1:14:48, 10.87s/it] 73%|███████▎  | 1142/1554 [4:03:57<1:13:17, 10.67s/it] 74%|███████▎  | 1143/1554 [4:04:08<1:12:35, 10.60s/it] 74%|███████▎  | 1144/1554 [4:04:19<1:12:55, 10.67s/it] 74%|███████▎  | 1145/1554 [4:04:29<1:11:58, 10.56s/it] 74%|███████▎  | 1146/1554 [4:04:40<1:12:20, 10.64s/it] 74%|███████▍  | 1147/1554 [4:04:51<1:12:50, 10.74s/it] 74%|███████▍  | 1148/1554 [4:05:02<1:13:00, 10.79s/it] 74%|███████▍  | 1149/1554 [4:05:13<1:12:54, 10.80s/it] 74%|███████▍  | 1150/1554 [4:05:23<1:12:56, 10.83s/it]                                                        74%|███████▍  | 1150/1554 [4:05:24<1:12:56, 10.83s/it] 74%|███████▍  | 1151/1554 [4:05:34<1:12:54, 10.85s/it] 74%|███████▍  | 1152/1554 [4:05:45<1:12:54, 10.88s/it] 74%|███████▍  | 1153/1554 [4:05:56<1:12:36, 10.86s/it] 74%|███████▍  | 1154/1554 [4:06:07<1:11:54, 10.79s/it] 74%|███████▍  | 1155/1554 [4:06:17<1:10:56, 10.67s/it] 74%|███████▍  | 1156/1554 [4:06:28<1:10:39, 10.65s/it] 74%|███████▍  | 1157/1554 [4:06:38<1:10:30, 10.66s/it] 75%|███████▍  | 1158/1554 [4:06:49<1:09:51, 10.59s/it] 75%|███████▍  | 1159/1554 [4:06:59<1:09:03, 10.49s/it] 75%|███████▍  | 1160/1554 [4:07:10<1:09:22, 10.57s/it]                                                        75%|███████▍  | 1160/1554 [4:07:10<1:09:22, 10.57s/it] 75%|███████▍  | 1161/1554 [4:07:20<1:09:12, 10.57s/it] 75%|███████▍  | 1162/1554 [4:07:31<1:08:32, 10.49s/it] 75%|███████▍  | 1163/1554 [4:07:41<1:08:33, 10.52s/it] 75%|███████▍  | 1164/1554 [4:07:52<1:08:44, 10.57s/it] 75%|███████▍  | 1165/1554 [4:08:02<1:08:09, 10.51s/it] 75%|███████▌  | 1166/1554 [4:08:13<1:07:58, 10.51s/it] 75%|███████▌  | 1167/1554 [4:08:23<1:07:32, 10.47s/it] 75%|███████▌  | 1168/1554 [4:08:34<1:07:21, 10.47s/it] 75%|███████▌  | 1169/1554 [4:08:44<1:07:15, 10.48s/it] 75%|███████▌  | 1170/1554 [4:08:55<1:06:54, 10.45s/it]                                                        75%|███████▌  | 1170/1554 [4:08:55<1:06:54, 10.45s/it] 75%|███████▌  | 1171/1554 [4:09:05<1:06:23, 10.40s/it] 75%|███████▌  | 1172/1554 [4:09:15<1:06:16, 10.41s/it] 75%|███████▌  | 1173/1554 [4:09:26<1:06:21, 10.45s/it] 76%|███████▌  | 1174/1554 [4:09:36<1:06:06, 10.44s/it] 76%|███████▌  | 1175/1554 [4:09:47<1:05:57, 10.44s/it] 76%|███████▌  | 1176/1554 [4:09:57<1:05:52, 10.46s/it] 76%|███████▌  | 1177/1554 [4:10:08<1:06:19, 10.55s/it] 76%|███████▌  | 1178/1554 [4:10:19<1:07:12, 10.73s/it] 76%|███████▌  | 1179/1554 [4:10:30<1:06:48, 10.69s/it] 76%|███████▌  | 1180/1554 [4:10:41<1:07:53, 10.89s/it]                                                        76%|███████▌  | 1180/1554 [4:10:41<1:07:53, 10.89s/it] 76%|███████▌  | 1181/1554 [4:10:52<1:06:45, 10.74s/it] 76%|███████▌  | 1182/1554 [4:11:02<1:06:05, 10.66s/it] 76%|███████▌  | 1183/1554 [4:11:12<1:05:19, 10.56s/it] 76%|███████▌  | 1184/1554 [4:11:23<1:05:04, 10.55s/it] 76%|███████▋  | 1185/1554 [4:11:33<1:04:49, 10.54s/it] 76%|███████▋  | 1186/1554 [4:11:44<1:05:00, 10.60s/it] 76%|███████▋  | 1187/1554 [4:11:55<1:04:40, 10.57s/it] 76%|███████▋  | 1188/1554 [4:12:05<1:04:00, 10.49s/it] 77%|███████▋  | 1189/1554 [4:12:15<1:03:58, 10.52s/it] 77%|███████▋  | 1190/1554 [4:12:26<1:03:59, 10.55s/it]                                                        77%|███████▋  | 1190/1554 [4:12:26<1:03:59, 10.55s/it] 77%|███████▋  | 1191/1554 [4:12:37<1:03:56, 10.57s/it] 77%|███████▋  | 1192/1554 [4:12:47<1:03:23, 10.51s/it] 77%|███████▋  | 1193/1554 [4:12:58<1:03:37, 10.57s/it] 77%|███████▋  | 1194/1554 [4:13:08<1:03:35, 10.60s/it] 77%|███████▋  | 1195/1554 [4:13:19<1:02:59, 10.53s/it] 77%|███████▋  | 1196/1554 [4:13:29<1:02:28, 10.47s/it] 77%|███████▋  | 1197/1554 [4:13:40<1:03:17, 10.64s/it] 77%|███████▋  | 1198/1554 [4:13:51<1:02:54, 10.60s/it] 77%|███████▋  | 1199/1554 [4:14:01<1:02:42, 10.60s/it] 77%|███████▋  | 1200/1554 [4:14:12<1:02:34, 10.60s/it]                                                        77%|███████▋  | 1200/1554 [4:14:12<1:02:34, 10.60s/it] 77%|███████▋  | 1201/1554 [4:14:23<1:02:22, 10.60s/it] 77%|███████▋  | 1202/1554 [4:14:33<1:02:04, 10.58s/it] 77%|███████▋  | 1203/1554 [4:14:44<1:01:48, 10.57s/it] 77%|███████▋  | 1204/1554 [4:14:54<1:01:25, 10.53s/it] 78%|███████▊  | 1205/1554 [4:15:05<1:01:19, 10.54s/it] 78%|███████▊  | 1206/1554 [4:15:15<1:00:58, 10.51s/it] 78%|███████▊  | 1207/1554 [4:15:26<1:00:43, 10.50s/it] 78%|███████▊  | 1208/1554 [4:15:36<1:00:22, 10.47s/it] 78%|███████▊  | 1209/1554 [4:15:46<59:52, 10.41s/it]   78%|███████▊  | 1210/1554 [4:15:57<59:32, 10.39s/it]                                                      78%|███████▊  | 1210/1554 [4:15:57<59:32, 10.39s/it] 78%|███████▊  | 1211/1554 [4:16:07<1:00:05, 10.51s/it] 78%|███████▊  | 1212/1554 [4:16:18<59:30, 10.44s/it]   78%|███████▊  | 1213/1554 [4:16:28<59:00, 10.38s/it] 78%|███████▊  | 1214/1554 [4:16:38<58:46, 10.37s/it] 78%|███████▊  | 1215/1554 [4:16:49<59:14, 10.49s/it] 78%|███████▊  | 1216/1554 [4:17:00<59:16, 10.52s/it] 78%|███████▊  | 1217/1554 [4:17:11<59:50, 10.65s/it] 78%|███████▊  | 1218/1554 [4:17:21<59:33, 10.63s/it] 78%|███████▊  | 1219/1554 [4:17:31<58:56, 10.56s/it] 79%|███████▊  | 1220/1554 [4:17:42<58:14, 10.46s/it]                                                      79%|███████▊  | 1220/1554 [4:17:42<58:14, 10.46s/it] 79%|███████▊  | 1221/1554 [4:17:53<58:35, 10.56s/it] 79%|███████▊  | 1222/1554 [4:18:03<58:30, 10.57s/it] 79%|███████▊  | 1223/1554 [4:18:14<58:22, 10.58s/it] 79%|███████▉  | 1224/1554 [4:18:24<57:55, 10.53s/it] 79%|███████▉  | 1225/1554 [4:18:34<57:19, 10.45s/it] 79%|███████▉  | 1226/1554 [4:18:45<56:54, 10.41s/it] 79%|███████▉  | 1227/1554 [4:18:55<56:57, 10.45s/it] 79%|███████▉  | 1228/1554 [4:19:06<57:02, 10.50s/it] 79%|███████▉  | 1229/1554 [4:19:16<56:42, 10.47s/it] 79%|███████▉  | 1230/1554 [4:19:27<56:38, 10.49s/it]                                                      79%|███████▉  | 1230/1554 [4:19:27<56:38, 10.49s/it] 79%|███████▉  | 1231/1554 [4:19:37<56:27, 10.49s/it] 79%|███████▉  | 1232/1554 [4:19:48<57:05, 10.64s/it] 79%|███████▉  | 1233/1554 [4:19:59<57:01, 10.66s/it] 79%|███████▉  | 1234/1554 [4:20:09<56:28, 10.59s/it] 79%|███████▉  | 1235/1554 [4:20:20<56:30, 10.63s/it] 80%|███████▉  | 1236/1554 [4:20:31<56:14, 10.61s/it] 80%|███████▉  | 1237/1554 [4:20:42<56:29, 10.69s/it] 80%|███████▉  | 1238/1554 [4:20:52<56:19, 10.69s/it] 80%|███████▉  | 1239/1554 [4:21:03<55:59, 10.66s/it] 80%|███████▉  | 1240/1554 [4:21:14<55:56, 10.69s/it]                                                      80%|███████▉  | 1240/1554 [4:21:14<55:56, 10.69s/it] 80%|███████▉  | 1241/1554 [4:21:24<55:40, 10.67s/it] 80%|███████▉  | 1242/1554 [4:21:35<55:36, 10.69s/it] 80%|███████▉  | 1243/1554 [4:21:46<55:14, 10.66s/it] 80%|████████  | 1244/1554 [4:21:56<54:32, 10.55s/it] 80%|████████  | 1245/1554 [4:22:06<54:21, 10.55s/it] 80%|████████  | 1246/1554 [4:22:17<54:03, 10.53s/it] 80%|████████  | 1247/1554 [4:22:27<53:51, 10.53s/it] 80%|████████  | 1248/1554 [4:22:38<53:49, 10.55s/it] 80%|████████  | 1249/1554 [4:22:49<54:01, 10.63s/it] 80%|████████  | 1250/1554 [4:23:00<54:07, 10.68s/it]                                                      80%|████████  | 1250/1554 [4:23:00<54:07, 10.68s/it] 81%|████████  | 1251/1554 [4:23:10<53:39, 10.63s/it] 81%|████████  | 1252/1554 [4:23:21<53:20, 10.60s/it] 81%|████████  | 1253/1554 [4:23:31<53:13, 10.61s/it] 81%|████████  | 1254/1554 [4:23:42<53:19, 10.66s/it] 81%|████████  | 1255/1554 [4:23:53<52:59, 10.64s/it] 81%|████████  | 1256/1554 [4:24:03<52:50, 10.64s/it] 81%|████████  | 1257/1554 [4:24:14<52:44, 10.66s/it] 81%|████████  | 1258/1554 [4:24:25<52:30, 10.64s/it] 81%|████████  | 1259/1554 [4:24:35<51:52, 10.55s/it] 81%|████████  | 1260/1554 [4:24:46<52:08, 10.64s/it]                                                      81%|████████  | 1260/1554 [4:24:46<52:08, 10.64s/it] 81%|████████  | 1261/1554 [4:24:56<51:41, 10.59s/it] 81%|████████  | 1262/1554 [4:25:07<51:55, 10.67s/it] 81%|████████▏ | 1263/1554 [4:25:18<51:55, 10.71s/it] 81%|████████▏ | 1264/1554 [4:25:28<51:08, 10.58s/it] 81%|████████▏ | 1265/1554 [4:25:38<50:29, 10.48s/it] 81%|████████▏ | 1266/1554 [4:25:49<50:49, 10.59s/it] 82%|████████▏ | 1267/1554 [4:26:00<50:15, 10.51s/it] 82%|████████▏ | 1268/1554 [4:26:10<50:25, 10.58s/it] 82%|████████▏ | 1269/1554 [4:26:21<50:43, 10.68s/it] 82%|████████▏ | 1270/1554 [4:26:32<50:31, 10.67s/it]                                                      82%|████████▏ | 1270/1554 [4:26:32<50:31, 10.67s/it] 82%|████████▏ | 1271/1554 [4:26:43<50:09, 10.63s/it] 82%|████████▏ | 1272/1554 [4:26:53<50:01, 10.64s/it] 82%|████████▏ | 1273/1554 [4:27:04<50:10, 10.71s/it] 82%|████████▏ | 1274/1554 [4:27:14<49:22, 10.58s/it] 82%|████████▏ | 1275/1554 [4:27:25<49:22, 10.62s/it] 82%|████████▏ | 1276/1554 [4:27:35<48:47, 10.53s/it] 82%|████████▏ | 1277/1554 [4:27:47<49:33, 10.74s/it] 82%|████████▏ | 1278/1554 [4:27:57<49:11, 10.69s/it] 82%|████████▏ | 1279/1554 [4:28:08<49:25, 10.78s/it] 82%|████████▏ | 1280/1554 [4:28:19<48:44, 10.67s/it]                                                      82%|████████▏ | 1280/1554 [4:28:19<48:44, 10.67s/it] 82%|████████▏ | 1281/1554 [4:28:29<48:33, 10.67s/it] 82%|████████▏ | 1282/1554 [4:28:40<47:59, 10.59s/it] 83%|████████▎ | 1283/1554 [4:28:51<48:17, 10.69s/it] 83%|████████▎ | 1284/1554 [4:29:01<48:19, 10.74s/it] 83%|████████▎ | 1285/1554 [4:29:12<48:08, 10.74s/it] 83%|████████▎ | 1286/1554 [4:29:22<47:18, 10.59s/it] 83%|████████▎ | 1287/1554 [4:29:33<46:43, 10.50s/it] 83%|████████▎ | 1288/1554 [4:29:43<46:23, 10.47s/it] 83%|████████▎ | 1289/1554 [4:29:54<46:16, 10.48s/it] 83%|████████▎ | 1290/1554 [4:30:04<46:16, 10.52s/it]                                                      83%|████████▎ | 1290/1554 [4:30:04<46:16, 10.52s/it] 83%|████████▎ | 1291/1554 [4:30:15<46:52, 10.69s/it] 83%|████████▎ | 1292/1554 [4:30:26<46:39, 10.69s/it] 83%|████████▎ | 1293/1554 [4:30:37<46:45, 10.75s/it] 83%|████████▎ | 1294/1554 [4:30:47<46:15, 10.68s/it] 83%|████████▎ | 1295/1554 [4:30:58<45:30, 10.54s/it] 83%|████████▎ | 1296/1554 [4:31:08<45:32, 10.59s/it] 83%|████████▎ | 1297/1554 [4:31:19<45:04, 10.52s/it] 84%|████████▎ | 1298/1554 [4:31:29<44:42, 10.48s/it] 84%|████████▎ | 1299/1554 [4:31:39<44:28, 10.46s/it] 84%|████████▎ | 1300/1554 [4:31:50<44:30, 10.51s/it]                                                      84%|████████▎ | 1300/1554 [4:31:50<44:30, 10.51s/it] 84%|████████▎ | 1301/1554 [4:32:00<44:10, 10.48s/it] 84%|████████▍ | 1302/1554 [4:32:12<45:11, 10.76s/it] 84%|████████▍ | 1303/1554 [4:32:22<44:44, 10.70s/it] 84%|████████▍ | 1304/1554 [4:32:33<44:09, 10.60s/it] 84%|████████▍ | 1305/1554 [4:32:43<43:55, 10.58s/it] 84%|████████▍ | 1306/1554 [4:32:54<43:22, 10.49s/it] 84%|████████▍ | 1307/1554 [4:33:04<43:25, 10.55s/it] 84%|████████▍ | 1308/1554 [4:33:15<43:08, 10.52s/it] 84%|████████▍ | 1309/1554 [4:33:25<42:58, 10.52s/it] 84%|████████▍ | 1310/1554 [4:33:36<42:48, 10.53s/it]                                                      84%|████████▍ | 1310/1554 [4:33:36<42:48, 10.53s/it] 84%|████████▍ | 1311/1554 [4:33:46<42:17, 10.44s/it] 84%|████████▍ | 1312/1554 [4:33:57<42:20, 10.50s/it] 84%|████████▍ | 1313/1554 [4:34:07<42:07, 10.49s/it] 85%|████████▍ | 1314/1554 [4:34:18<42:02, 10.51s/it] 85%|████████▍ | 1315/1554 [4:34:28<41:49, 10.50s/it] 85%|████████▍ | 1316/1554 [4:34:39<41:48, 10.54s/it] 85%|████████▍ | 1317/1554 [4:34:49<41:35, 10.53s/it] 85%|████████▍ | 1318/1554 [4:35:00<41:23, 10.52s/it] 85%|████████▍ | 1319/1554 [4:35:10<41:08, 10.50s/it] 85%|████████▍ | 1320/1554 [4:35:21<40:58, 10.51s/it]                                                      85%|████████▍ | 1320/1554 [4:35:21<40:58, 10.51s/it] 85%|████████▌ | 1321/1554 [4:35:31<40:47, 10.51s/it] 85%|████████▌ | 1322/1554 [4:35:42<40:32, 10.48s/it] 85%|████████▌ | 1323/1554 [4:35:52<40:32, 10.53s/it] 85%|████████▌ | 1324/1554 [4:36:03<40:16, 10.51s/it] 85%|████████▌ | 1325/1554 [4:36:13<40:13, 10.54s/it] 85%|████████▌ | 1326/1554 [4:36:24<39:59, 10.52s/it] 85%|████████▌ | 1327/1554 [4:36:35<39:51, 10.53s/it] 85%|████████▌ | 1328/1554 [4:36:45<39:32, 10.50s/it] 86%|████████▌ | 1329/1554 [4:36:55<39:18, 10.48s/it] 86%|████████▌ | 1330/1554 [4:37:06<38:57, 10.44s/it]                                                      86%|████████▌ | 1330/1554 [4:37:06<38:57, 10.44s/it] 86%|████████▌ | 1331/1554 [4:37:16<38:43, 10.42s/it] 86%|████████▌ | 1332/1554 [4:37:26<38:28, 10.40s/it] 86%|████████▌ | 1333/1554 [4:37:38<39:01, 10.59s/it] 86%|████████▌ | 1334/1554 [4:37:48<38:48, 10.58s/it] 86%|████████▌ | 1335/1554 [4:37:58<38:23, 10.52s/it] 86%|████████▌ | 1336/1554 [4:38:09<38:08, 10.50s/it] 86%|████████▌ | 1337/1554 [4:38:19<37:48, 10.45s/it] 86%|████████▌ | 1338/1554 [4:38:30<37:37, 10.45s/it] 86%|████████▌ | 1339/1554 [4:38:40<37:19, 10.42s/it] 86%|████████▌ | 1340/1554 [4:38:50<37:11, 10.43s/it]                                                      86%|████████▌ | 1340/1554 [4:38:50<37:11, 10.43s/it] 86%|████████▋ | 1341/1554 [4:39:01<36:58, 10.41s/it] 86%|████████▋ | 1342/1554 [4:39:11<36:51, 10.43s/it] 86%|████████▋ | 1343/1554 [4:39:22<36:44, 10.45s/it] 86%|████████▋ | 1344/1554 [4:39:32<36:33, 10.44s/it] 87%|████████▋ | 1345/1554 [4:39:43<36:23, 10.45s/it] 87%|████████▋ | 1346/1554 [4:39:53<36:15, 10.46s/it] 87%|████████▋ | 1347/1554 [4:40:04<36:06, 10.47s/it] 87%|████████▋ | 1348/1554 [4:40:14<35:52, 10.45s/it] 87%|████████▋ | 1349/1554 [4:40:24<35:35, 10.42s/it] 87%|████████▋ | 1350/1554 [4:40:35<35:27, 10.43s/it]                                                      87%|████████▋ | 1350/1554 [4:40:35<35:27, 10.43s/it] 87%|████████▋ | 1351/1554 [4:40:45<35:16, 10.43s/it] 87%|████████▋ | 1352/1554 [4:40:56<35:01, 10.40s/it] 87%|████████▋ | 1353/1554 [4:41:06<34:43, 10.37s/it] 87%|████████▋ | 1354/1554 [4:41:16<34:26, 10.33s/it] 87%|████████▋ | 1355/1554 [4:41:26<34:14, 10.32s/it] 87%|████████▋ | 1356/1554 [4:41:37<34:03, 10.32s/it] 87%|████████▋ | 1357/1554 [4:41:47<33:51, 10.31s/it] 87%|████████▋ | 1358/1554 [4:41:57<33:42, 10.32s/it] 87%|████████▋ | 1359/1554 [4:42:08<33:30, 10.31s/it] 88%|████████▊ | 1360/1554 [4:42:18<33:18, 10.30s/it]                                                      88%|████████▊ | 1360/1554 [4:42:18<33:18, 10.30s/it] 88%|████████▊ | 1361/1554 [4:42:28<33:11, 10.32s/it] 88%|████████▊ | 1362/1554 [4:42:39<33:02, 10.32s/it] 88%|████████▊ | 1363/1554 [4:42:49<32:53, 10.33s/it] 88%|████████▊ | 1364/1554 [4:42:59<32:39, 10.31s/it] 88%|████████▊ | 1365/1554 [4:43:10<32:35, 10.35s/it] 88%|████████▊ | 1366/1554 [4:43:20<32:28, 10.37s/it] 88%|████████▊ | 1367/1554 [4:43:31<32:19, 10.37s/it] 88%|████████▊ | 1368/1554 [4:43:41<32:06, 10.36s/it] 88%|████████▊ | 1369/1554 [4:43:51<31:55, 10.36s/it] 88%|████████▊ | 1370/1554 [4:44:02<31:44, 10.35s/it]                                                      88%|████████▊ | 1370/1554 [4:44:02<31:44, 10.35s/it] 88%|████████▊ | 1371/1554 [4:44:12<31:29, 10.33s/it] 88%|████████▊ | 1372/1554 [4:44:22<31:21, 10.34s/it] 88%|████████▊ | 1373/1554 [4:44:32<31:06, 10.31s/it] 88%|████████▊ | 1374/1554 [4:44:43<30:52, 10.29s/it] 88%|████████▊ | 1375/1554 [4:44:53<30:43, 10.30s/it] 89%|████████▊ | 1376/1554 [4:45:03<30:33, 10.30s/it] 89%|████████▊ | 1377/1554 [4:45:14<30:25, 10.31s/it] 89%|████████▊ | 1378/1554 [4:45:24<30:13, 10.30s/it] 89%|████████▊ | 1379/1554 [4:45:34<30:05, 10.31s/it] 89%|████████▉ | 1380/1554 [4:45:45<30:03, 10.36s/it]                                                      89%|████████▉ | 1380/1554 [4:45:45<30:03, 10.36s/it] 89%|████████▉ | 1381/1554 [4:45:55<29:54, 10.37s/it] 89%|████████▉ | 1382/1554 [4:46:05<29:39, 10.35s/it] 89%|████████▉ | 1383/1554 [4:46:16<29:34, 10.38s/it] 89%|████████▉ | 1384/1554 [4:46:26<29:23, 10.37s/it] 89%|████████▉ | 1385/1554 [4:46:37<29:10, 10.36s/it] 89%|████████▉ | 1386/1554 [4:46:47<29:03, 10.38s/it] 89%|████████▉ | 1387/1554 [4:46:57<28:57, 10.40s/it] 89%|████████▉ | 1388/1554 [4:47:08<28:40, 10.36s/it] 89%|████████▉ | 1389/1554 [4:47:18<28:33, 10.39s/it] 89%|████████▉ | 1390/1554 [4:47:29<28:26, 10.41s/it]                                                      89%|████████▉ | 1390/1554 [4:47:29<28:26, 10.41s/it] 90%|████████▉ | 1391/1554 [4:47:39<28:15, 10.40s/it] 90%|████████▉ | 1392/1554 [4:47:49<28:02, 10.39s/it] 90%|████████▉ | 1393/1554 [4:48:00<27:45, 10.34s/it] 90%|████████▉ | 1394/1554 [4:48:10<27:31, 10.32s/it] 90%|████████▉ | 1395/1554 [4:48:20<27:20, 10.32s/it] 90%|████████▉ | 1396/1554 [4:48:31<27:12, 10.33s/it] 90%|████████▉ | 1397/1554 [4:48:41<27:03, 10.34s/it] 90%|████████▉ | 1398/1554 [4:48:51<26:49, 10.32s/it] 90%|█████████ | 1399/1554 [4:49:02<26:45, 10.36s/it] 90%|█████████ | 1400/1554 [4:49:12<26:39, 10.38s/it]                                                      90%|█████████ | 1400/1554 [4:49:12<26:39, 10.38s/it] 90%|█████████ | 1401/1554 [4:49:22<26:26, 10.37s/it] 90%|█████████ | 1402/1554 [4:49:33<26:10, 10.33s/it] 90%|█████████ | 1403/1554 [4:49:43<25:57, 10.31s/it] 90%|█████████ | 1404/1554 [4:49:53<25:42, 10.28s/it] 90%|█████████ | 1405/1554 [4:50:03<25:35, 10.31s/it] 90%|█████████ | 1406/1554 [4:50:14<25:23, 10.30s/it] 91%|█████████ | 1407/1554 [4:50:24<25:13, 10.29s/it] 91%|█████████ | 1408/1554 [4:50:34<25:07, 10.32s/it] 91%|█████████ | 1409/1554 [4:50:45<24:56, 10.32s/it] 91%|█████████ | 1410/1554 [4:50:55<24:43, 10.30s/it]                                                      91%|█████████ | 1410/1554 [4:50:55<24:43, 10.30s/it] 91%|█████████ | 1411/1554 [4:51:05<24:35, 10.32s/it] 91%|█████████ | 1412/1554 [4:51:16<24:24, 10.31s/it] 91%|█████████ | 1413/1554 [4:51:26<24:20, 10.36s/it] 91%|█████████ | 1414/1554 [4:51:37<24:12, 10.38s/it] 91%|█████████ | 1415/1554 [4:51:47<23:58, 10.35s/it] 91%|█████████ | 1416/1554 [4:51:57<23:47, 10.34s/it] 91%|█████████ | 1417/1554 [4:52:07<23:35, 10.33s/it] 91%|█████████ | 1418/1554 [4:52:18<23:19, 10.29s/it] 91%|█████████▏| 1419/1554 [4:52:28<23:08, 10.29s/it] 91%|█████████▏| 1420/1554 [4:52:38<23:00, 10.30s/it]                                                      91%|█████████▏| 1420/1554 [4:52:38<23:00, 10.30s/it] 91%|█████████▏| 1421/1554 [4:52:49<22:52, 10.32s/it] 92%|█████████▏| 1422/1554 [4:52:59<22:41, 10.32s/it] 92%|█████████▏| 1423/1554 [4:53:09<22:31, 10.32s/it] 92%|█████████▏| 1424/1554 [4:53:20<22:21, 10.32s/it] 92%|█████████▏| 1425/1554 [4:53:30<22:11, 10.32s/it] 92%|█████████▏| 1426/1554 [4:53:40<21:58, 10.30s/it] 92%|█████████▏| 1427/1554 [4:53:51<21:51, 10.33s/it] 92%|█████████▏| 1428/1554 [4:54:01<21:43, 10.34s/it] 92%|█████████▏| 1429/1554 [4:54:11<21:32, 10.34s/it] 92%|█████████▏| 1430/1554 [4:54:22<21:21, 10.33s/it]                                                      92%|█████████▏| 1430/1554 [4:54:22<21:21, 10.33s/it] 92%|█████████▏| 1431/1554 [4:54:32<21:13, 10.36s/it] 92%|█████████▏| 1432/1554 [4:54:42<21:04, 10.37s/it] 92%|█████████▏| 1433/1554 [4:54:53<20:52, 10.35s/it] 92%|█████████▏| 1434/1554 [4:55:03<20:50, 10.42s/it] 92%|█████████▏| 1435/1554 [4:55:14<20:40, 10.42s/it] 92%|█████████▏| 1436/1554 [4:55:24<20:27, 10.40s/it] 92%|█████████▏| 1437/1554 [4:55:35<20:19, 10.42s/it] 93%|█████████▎| 1438/1554 [4:55:45<20:07, 10.41s/it] 93%|█████████▎| 1439/1554 [4:55:55<19:57, 10.41s/it] 93%|█████████▎| 1440/1554 [4:56:06<19:44, 10.39s/it]                                                      93%|█████████▎| 1440/1554 [4:56:06<19:44, 10.39s/it] 93%|█████████▎| 1441/1554 [4:56:16<19:34, 10.39s/it] 93%|█████████▎| 1442/1554 [4:56:26<19:23, 10.38s/it] 93%|█████████▎| 1443/1554 [4:56:37<19:11, 10.37s/it] 93%|█████████▎| 1444/1554 [4:56:47<19:05, 10.41s/it] 93%|█████████▎| 1445/1554 [4:56:58<18:51, 10.38s/it] 93%|█████████▎| 1446/1554 [4:57:08<18:41, 10.38s/it] 93%|█████████▎| 1447/1554 [4:57:18<18:27, 10.35s/it] 93%|█████████▎| 1448/1554 [4:57:29<18:19, 10.37s/it] 93%|█████████▎| 1449/1554 [4:57:39<18:08, 10.37s/it] 93%|█████████▎| 1450/1554 [4:57:49<17:57, 10.36s/it]                                                      93%|█████████▎| 1450/1554 [4:57:49<17:57, 10.36s/it] 93%|█████████▎| 1451/1554 [4:58:00<17:46, 10.35s/it] 93%|█████████▎| 1452/1554 [4:58:10<17:37, 10.37s/it] 94%|█████████▎| 1453/1554 [4:58:20<17:25, 10.35s/it] 94%|█████████▎| 1454/1554 [4:58:31<17:13, 10.34s/it] 94%|█████████▎| 1455/1554 [4:58:41<16:59, 10.29s/it] 94%|█████████▎| 1456/1554 [4:58:51<16:51, 10.32s/it] 94%|█████████▍| 1457/1554 [4:59:02<17:05, 10.57s/it] 94%|█████████▍| 1458/1554 [4:59:13<16:43, 10.46s/it] 94%|█████████▍| 1459/1554 [4:59:23<16:26, 10.38s/it] 94%|█████████▍| 1460/1554 [4:59:33<16:15, 10.38s/it]                                                      94%|█████████▍| 1460/1554 [4:59:33<16:15, 10.38s/it] 94%|█████████▍| 1461/1554 [4:59:43<16:02, 10.35s/it] 94%|█████████▍| 1462/1554 [4:59:54<15:50, 10.33s/it] 94%|█████████▍| 1463/1554 [5:00:04<15:36, 10.29s/it] 94%|█████████▍| 1464/1554 [5:00:14<15:23, 10.26s/it] 94%|█████████▍| 1465/1554 [5:00:24<15:13, 10.27s/it] 94%|█████████▍| 1466/1554 [5:00:35<15:07, 10.32s/it] 94%|█████████▍| 1467/1554 [5:00:45<14:54, 10.28s/it] 94%|█████████▍| 1468/1554 [5:00:55<14:46, 10.31s/it] 95%|█████████▍| 1469/1554 [5:01:06<14:38, 10.33s/it] 95%|█████████▍| 1470/1554 [5:01:16<14:31, 10.38s/it]                                                      95%|█████████▍| 1470/1554 [5:01:16<14:31, 10.38s/it] 95%|█████████▍| 1471/1554 [5:01:27<14:22, 10.39s/it] 95%|█████████▍| 1472/1554 [5:01:37<14:15, 10.43s/it] 95%|█████████▍| 1473/1554 [5:01:48<14:01, 10.39s/it] 95%|█████████▍| 1474/1554 [5:01:58<13:50, 10.38s/it] 95%|█████████▍| 1475/1554 [5:02:08<13:41, 10.39s/it] 95%|█████████▍| 1476/1554 [5:02:19<13:29, 10.38s/it] 95%|█████████▌| 1477/1554 [5:02:29<13:22, 10.42s/it] 95%|█████████▌| 1478/1554 [5:02:39<13:08, 10.37s/it] 95%|█████████▌| 1479/1554 [5:02:50<13:02, 10.43s/it] 95%|█████████▌| 1480/1554 [5:03:00<12:51, 10.43s/it]                                                      95%|█████████▌| 1480/1554 [5:03:00<12:51, 10.43s/it] 95%|█████████▌| 1481/1554 [5:03:11<12:39, 10.40s/it] 95%|█████████▌| 1482/1554 [5:03:21<12:23, 10.32s/it] 95%|█████████▌| 1483/1554 [5:03:31<12:13, 10.34s/it] 95%|█████████▌| 1484/1554 [5:03:42<12:05, 10.36s/it] 96%|█████████▌| 1485/1554 [5:03:52<11:55, 10.37s/it] 96%|█████████▌| 1486/1554 [5:04:02<11:45, 10.38s/it] 96%|█████████▌| 1487/1554 [5:04:13<11:34, 10.37s/it] 96%|█████████▌| 1488/1554 [5:04:23<11:25, 10.38s/it] 96%|█████████▌| 1489/1554 [5:04:34<11:15, 10.39s/it] 96%|█████████▌| 1490/1554 [5:04:44<11:06, 10.42s/it]                                                      96%|█████████▌| 1490/1554 [5:04:44<11:06, 10.42s/it] 96%|█████████▌| 1491/1554 [5:04:54<10:52, 10.36s/it] 96%|█████████▌| 1492/1554 [5:05:05<10:43, 10.38s/it] 96%|█████████▌| 1493/1554 [5:05:16<10:47, 10.62s/it] 96%|█████████▌| 1494/1554 [5:05:26<10:34, 10.58s/it] 96%|█████████▌| 1495/1554 [5:05:37<10:19, 10.49s/it] 96%|█████████▋| 1496/1554 [5:05:47<10:05, 10.44s/it] 96%|█████████▋| 1497/1554 [5:05:57<09:53, 10.41s/it] 96%|█████████▋| 1498/1554 [5:06:08<09:41, 10.39s/it] 96%|█████████▋| 1499/1554 [5:06:18<09:29, 10.36s/it] 97%|█████████▋| 1500/1554 [5:06:28<09:18, 10.35s/it]                                                      97%|█████████▋| 1500/1554 [5:06:28<09:18, 10.35s/it]<frozen importlib._bootstrap>:914: ImportWarning: TEMetaPathFinder.find_spec() not found; falling back to find_module()
192.168.0.13: [INFO|trainer.py:3819] 2024-09-29 16:38:53,191 >> 
192.168.0.13: ***** Running Evaluation *****
192.168.0.13: [INFO|trainer.py:3821] 2024-09-29 16:38:53,192 >>   Num examples = 14708
192.168.0.13: [INFO|trainer.py:3824] 2024-09-29 16:38:53,192 >>   Batch size = 1
192.168.0.149: [INFO|trainer.py:3819] 2024-09-29 16:38:55,789 >> 
192.168.0.149: ***** Running Evaluation *****
192.168.0.149: [INFO|trainer.py:3821] 2024-09-29 16:38:55,789 >>   Num examples = 14708
192.168.0.149: [INFO|trainer.py:3824] 2024-09-29 16:38:55,789 >>   Batch size = 1
192.168.0.25: [INFO|trainer.py:3819] 2024-09-29 16:38:52,749 >> 
192.168.0.25: ***** Running Evaluation *****
192.168.0.25: [INFO|trainer.py:3821] 2024-09-29 16:38:52,750 >>   Num examples = 14708
192.168.0.25: [INFO|trainer.py:3824] 2024-09-29 16:38:52,750 >>   Batch size = 1
192.168.0.89: [INFO|trainer.py:3819] 2024-09-29 16:38:37,061 >> 
192.168.0.89: ***** Running Evaluation *****
192.168.0.89: [INFO|trainer.py:3821] 2024-09-29 16:38:37,062 >>   Num examples = 14708
192.168.0.89: [INFO|trainer.py:3824] 2024-09-29 16:38:37,062 >>   Batch size = 1
192.168.0.25: 
192.168.0.25:   0%|          | 0/460 [00:00<?, ?it/s][A
192.168.0.25:   0%|          | 2/460 [00:02<09:33,  1.25s/it][A
192.168.0.25:   1%|          | 3/460 [00:05<13:30,  1.77s/it][A
192.168.0.25:   1%|          | 4/460 [00:07<15:32,  2.05s/it][A
192.168.0.25:   1%|          | 5/460 [00:10<16:43,  2.20s/it][A
192.168.0.25:   1%|▏         | 6/460 [00:12<17:25,  2.30s/it][A
192.168.0.25:   2%|▏         | 7/460 [00:15<17:53,  2.37s/it][A
192.168.0.25:   2%|▏         | 8/460 [00:17<18:10,  2.41s/it][A
192.168.0.25:   2%|▏         | 9/460 [00:20<18:20,  2.44s/it][A
192.168.0.25:   2%|▏         | 10/460 [00:22<18:26,  2.46s/it][A
192.168.0.25:   2%|▏         | 11/460 [00:25<18:30,  2.47s/it][A
192.168.0.25:   3%|▎         | 12/460 [00:27<18:32,  2.48s/it][A
192.168.0.25:   3%|▎         | 13/460 [00:30<18:32,  2.49s/it][A
192.168.0.25:   3%|▎         | 14/460 [00:32<18:31,  2.49s/it][A
192.168.0.25:   3%|▎         | 15/460 [00:35<18:30,  2.50s/it][A
192.168.0.25:   3%|▎         | 16/460 [00:37<18:29,  2.50s/it][A
192.168.0.25:   4%|▎         | 17/460 [00:40<18:27,  2.50s/it][A
192.168.0.25:   4%|▍         | 18/460 [00:42<18:25,  2.50s/it][A
192.168.0.25:   4%|▍         | 19/460 [00:45<18:23,  2.50s/it][A
192.168.0.25:   4%|▍         | 20/460 [00:47<18:21,  2.50s/it][A
192.168.0.25:   5%|▍         | 21/460 [00:50<18:18,  2.50s/it][A
192.168.0.25:   5%|▍         | 22/460 [00:52<18:16,  2.50s/it][A
192.168.0.25:   5%|▌         | 23/460 [00:55<18:13,  2.50s/it][A
192.168.0.25:   5%|▌         | 24/460 [00:57<18:11,  2.50s/it][A
192.168.0.25:   5%|▌         | 25/460 [01:00<18:08,  2.50s/it][A
192.168.0.25:   6%|▌         | 26/460 [01:02<18:06,  2.50s/it][A
192.168.0.25:   6%|▌         | 27/460 [01:05<18:03,  2.50s/it][A
192.168.0.25:   6%|▌         | 28/460 [01:07<18:01,  2.50s/it][A
192.168.0.25:   6%|▋         | 29/460 [01:10<17:58,  2.50s/it][A
192.168.0.25:   7%|▋         | 30/460 [01:12<17:56,  2.50s/it][A
192.168.0.25:   7%|▋         | 31/460 [01:15<17:53,  2.50s/it][A
192.168.0.25:   7%|▋         | 32/460 [01:17<17:51,  2.50s/it][A
192.168.0.25:   7%|▋         | 33/460 [01:20<17:48,  2.50s/it][A
192.168.0.25:   7%|▋         | 34/460 [01:22<17:45,  2.50s/it][A
192.168.0.25:   8%|▊         | 35/460 [01:25<17:43,  2.50s/it][A
192.168.0.25:   8%|▊         | 36/460 [01:27<17:40,  2.50s/it][A
192.168.0.25:   8%|▊         | 37/460 [01:30<17:38,  2.50s/it][A
192.168.0.25:   8%|▊         | 38/460 [01:32<17:35,  2.50s/it][A
192.168.0.25:   8%|▊         | 39/460 [01:35<17:33,  2.50s/it][A
192.168.0.25:   9%|▊         | 40/460 [01:37<17:30,  2.50s/it][A
192.168.0.25:   9%|▉         | 41/460 [01:40<17:28,  2.50s/it][A
192.168.0.25:   9%|▉         | 42/460 [01:42<17:26,  2.50s/it][A
192.168.0.25:   9%|▉         | 43/460 [01:45<17:23,  2.50s/it][A
192.168.0.25:  10%|▉         | 44/460 [01:47<17:21,  2.50s/it][A
192.168.0.25:  10%|▉         | 45/460 [01:50<17:18,  2.50s/it][A
192.168.0.25:  10%|█         | 46/460 [01:52<17:16,  2.50s/it][A
192.168.0.25:  10%|█         | 47/460 [01:55<17:13,  2.50s/it][A
192.168.0.25:  10%|█         | 48/460 [01:57<17:11,  2.50s/it][A
192.168.0.25:  11%|█         | 49/460 [02:00<17:08,  2.50s/it][A
192.168.0.25:  11%|█         | 50/460 [02:02<17:06,  2.50s/it][A
192.168.0.25:  11%|█         | 51/460 [02:05<17:03,  2.50s/it][A
192.168.0.25:  11%|█▏        | 52/460 [02:07<17:01,  2.50s/it][A
192.168.0.25:  12%|█▏        | 53/460 [02:10<16:58,  2.50s/it][A
192.168.0.25:  12%|█▏        | 54/460 [02:12<16:56,  2.50s/it][A
192.168.0.25:  12%|█▏        | 55/460 [02:15<16:53,  2.50s/it][A
192.168.0.25:  12%|█▏        | 56/460 [02:17<16:51,  2.50s/it][A
192.168.0.25:  12%|█▏        | 57/460 [02:20<16:48,  2.50s/it][A
192.168.0.25:  13%|█▎        | 58/460 [02:22<16:45,  2.50s/it][A
192.168.0.25:  13%|█▎        | 59/460 [02:25<16:43,  2.50s/it][A
192.168.0.25:  13%|█▎        | 60/460 [02:27<16:40,  2.50s/it][A
192.168.0.25:  13%|█▎        | 61/460 [02:30<16:38,  2.50s/it][A
192.168.0.25:  13%|█▎        | 62/460 [02:32<16:36,  2.50s/it][A
192.168.0.25:  14%|█▎        | 63/460 [02:35<16:33,  2.50s/it][A
192.168.0.25:  14%|█▍        | 64/460 [02:37<16:30,  2.50s/it][A
192.168.0.25:  14%|█▍        | 65/460 [02:40<16:28,  2.50s/it][A
192.168.0.25:  14%|█▍        | 66/460 [02:42<16:25,  2.50s/it][A
192.168.0.25:  15%|█▍        | 67/460 [02:45<16:23,  2.50s/it][A
192.168.0.25:  15%|█▍        | 68/460 [02:47<16:20,  2.50s/it][A
192.168.0.25:  15%|█▌        | 69/460 [02:50<16:18,  2.50s/it][A
192.168.0.25:  15%|█▌        | 70/460 [02:52<16:15,  2.50s/it][A
192.168.0.25:  15%|█▌        | 71/460 [02:55<16:13,  2.50s/it][A
192.168.0.25:  16%|█▌        | 72/460 [02:57<16:10,  2.50s/it][A
192.168.0.25:  16%|█▌        | 73/460 [03:00<16:08,  2.50s/it][A
192.168.0.25:  16%|█▌        | 74/460 [03:02<16:05,  2.50s/it][A
192.168.0.25:  16%|█▋        | 75/460 [03:05<16:03,  2.50s/it][A
192.168.0.25:  17%|█▋        | 76/460 [03:07<16:00,  2.50s/it][A
192.168.0.25:  17%|█▋        | 77/460 [03:10<15:58,  2.50s/it][A
192.168.0.25:  17%|█▋        | 78/460 [03:12<15:56,  2.50s/it][A
192.168.0.25:  17%|█▋        | 79/460 [03:15<15:53,  2.50s/it][A
192.168.0.25:  17%|█▋        | 80/460 [03:17<15:51,  2.50s/it][A
192.168.0.25:  18%|█▊        | 81/460 [03:20<15:48,  2.50s/it][A
192.168.0.25:  18%|█▊        | 82/460 [03:22<15:46,  2.50s/it][A
192.168.0.25:  18%|█▊        | 83/460 [03:25<15:44,  2.51s/it][A
192.168.0.25:  18%|█▊        | 84/460 [03:27<15:41,  2.50s/it][A
192.168.0.25:  18%|█▊        | 85/460 [03:30<15:38,  2.50s/it][A
192.168.0.25:  19%|█▊        | 86/460 [03:32<15:36,  2.50s/it][A
192.168.0.25:  19%|█▉        | 87/460 [03:35<15:33,  2.50s/it][A
192.168.0.25:  19%|█▉        | 88/460 [03:37<15:31,  2.50s/it][A
192.168.0.25:  19%|█▉        | 89/460 [03:40<15:28,  2.50s/it][A
192.168.0.25:  20%|█▉        | 90/460 [03:42<15:26,  2.50s/it][A
192.168.0.25:  20%|█▉        | 91/460 [03:45<15:23,  2.50s/it][A
192.168.0.25:  20%|██        | 92/460 [03:47<15:20,  2.50s/it][A
192.168.0.25:  20%|██        | 93/460 [03:50<15:18,  2.50s/it][A
192.168.0.25:  20%|██        | 94/460 [03:52<15:15,  2.50s/it][A
192.168.0.25:  21%|██        | 95/460 [03:55<15:13,  2.50s/it][A
192.168.0.25:  21%|██        | 96/460 [03:57<15:10,  2.50s/it][A
192.168.0.25:  21%|██        | 97/460 [04:00<15:08,  2.50s/it][A
192.168.0.25:  21%|██▏       | 98/460 [04:02<15:05,  2.50s/it][A
192.168.0.25:  22%|██▏       | 99/460 [04:05<15:03,  2.50s/it][A
192.168.0.25:  22%|██▏       | 100/460 [04:07<15:00,  2.50s/it][A
192.168.0.25:  22%|██▏       | 101/460 [04:10<14:58,  2.50s/it][A
192.168.0.25:  22%|██▏       | 102/460 [04:12<14:55,  2.50s/it][A
192.168.0.25:  22%|██▏       | 103/460 [04:15<14:53,  2.50s/it][A
192.168.0.25:  23%|██▎       | 104/460 [04:17<14:50,  2.50s/it][A
192.168.0.25:  23%|██▎       | 105/460 [04:20<14:48,  2.50s/it][A
192.168.0.25:  23%|██▎       | 106/460 [04:22<14:45,  2.50s/it][A
192.168.0.25:  23%|██▎       | 107/460 [04:25<14:43,  2.50s/it][A
192.168.0.25:  23%|██▎       | 108/460 [04:27<14:40,  2.50s/it][A
192.168.0.25:  24%|██▎       | 109/460 [04:30<14:38,  2.50s/it][A
192.168.0.25:  24%|██▍       | 110/460 [04:32<14:35,  2.50s/it][A
192.168.0.25:  24%|██▍       | 111/460 [04:35<14:33,  2.50s/it][A
192.168.0.25:  24%|██▍       | 112/460 [04:37<14:30,  2.50s/it][A
192.168.0.25:  25%|██▍       | 113/460 [04:40<14:29,  2.50s/it][A
192.168.0.25:  25%|██▍       | 114/460 [04:42<14:26,  2.50s/it][A
192.168.0.25:  25%|██▌       | 115/460 [04:45<14:23,  2.50s/it][A
192.168.0.25:  25%|██▌       | 116/460 [04:47<14:21,  2.50s/it][A
192.168.0.25:  25%|██▌       | 117/460 [04:50<14:18,  2.50s/it][A
192.168.0.25:  26%|██▌       | 118/460 [04:52<14:16,  2.50s/it][A
192.168.0.25:  26%|██▌       | 119/460 [04:55<14:13,  2.50s/it][A
192.168.0.25:  26%|██▌       | 120/460 [04:57<14:11,  2.50s/it][A
192.168.0.25:  26%|██▋       | 121/460 [05:00<14:08,  2.50s/it][A
192.168.0.25:  27%|██▋       | 122/460 [05:02<14:05,  2.50s/it][A
192.168.0.25:  27%|██▋       | 123/460 [05:05<14:03,  2.50s/it][A
192.168.0.25:  27%|██▋       | 124/460 [05:07<14:00,  2.50s/it][A
192.168.0.25:  27%|██▋       | 125/460 [05:10<13:58,  2.50s/it][A
192.168.0.25:  27%|██▋       | 126/460 [05:12<13:55,  2.50s/it][A
192.168.0.25:  28%|██▊       | 127/460 [05:15<13:53,  2.50s/it][A
192.168.0.25:  28%|██▊       | 128/460 [05:17<13:50,  2.50s/it][A
192.168.0.25:  28%|██▊       | 129/460 [05:20<13:48,  2.50s/it][A
192.168.0.25:  28%|██▊       | 130/460 [05:22<13:45,  2.50s/it][A
192.168.0.25:  28%|██▊       | 131/460 [05:25<13:43,  2.50s/it][A
192.168.0.25:  29%|██▊       | 132/460 [05:27<13:40,  2.50s/it][A
192.168.0.25:  29%|██▉       | 133/460 [05:30<13:38,  2.50s/it][A
192.168.0.25:  29%|██▉       | 134/460 [05:32<13:35,  2.50s/it][A
192.168.0.25:  29%|██▉       | 135/460 [05:35<13:33,  2.50s/it][A
192.168.0.25:  30%|██▉       | 136/460 [05:37<13:30,  2.50s/it][A
192.168.0.25:  30%|██▉       | 137/460 [05:40<13:28,  2.50s/it][A
192.168.0.25:  30%|███       | 138/460 [05:42<13:25,  2.50s/it][A
192.168.0.25:  30%|███       | 139/460 [05:45<13:23,  2.50s/it][A
192.168.0.25:  30%|███       | 140/460 [05:47<13:21,  2.50s/it][A
192.168.0.25:  31%|███       | 141/460 [05:50<13:18,  2.50s/it][A
192.168.0.25:  31%|███       | 142/460 [05:52<13:16,  2.50s/it][A
192.168.0.25:  31%|███       | 143/460 [05:55<13:13,  2.50s/it][A
192.168.0.25:  31%|███▏      | 144/460 [05:57<13:10,  2.50s/it][A
192.168.0.25:  32%|███▏      | 145/460 [06:00<13:08,  2.50s/it][A
192.168.0.25:  32%|███▏      | 146/460 [06:02<13:05,  2.50s/it][A
192.168.0.25:  32%|███▏      | 147/460 [06:05<13:03,  2.50s/it][A
192.168.0.25:  32%|███▏      | 148/460 [06:07<13:00,  2.50s/it][A
192.168.0.25:  32%|███▏      | 149/460 [06:10<12:58,  2.50s/it][A
192.168.0.25:  33%|███▎      | 150/460 [06:12<12:55,  2.50s/it][A
192.168.0.25:  33%|███▎      | 151/460 [06:15<12:53,  2.50s/it][A
192.168.0.25:  33%|███▎      | 152/460 [06:17<12:51,  2.50s/it][A
192.168.0.25:  33%|███▎      | 153/460 [06:20<12:48,  2.50s/it][A
192.168.0.25:  33%|███▎      | 154/460 [06:22<12:45,  2.50s/it][A
192.168.0.25:  34%|███▎      | 155/460 [06:25<12:43,  2.50s/it][A
192.168.0.25:  34%|███▍      | 156/460 [06:27<12:41,  2.50s/it][A
192.168.0.25:  34%|███▍      | 157/460 [06:30<12:38,  2.50s/it][A
192.168.0.25:  34%|███▍      | 158/460 [06:32<12:36,  2.50s/it][A
192.168.0.25:  35%|███▍      | 159/460 [06:35<12:33,  2.50s/it][A
192.168.0.25:  35%|███▍      | 160/460 [06:37<12:31,  2.51s/it][A
192.168.0.25:  35%|███▌      | 161/460 [06:40<12:28,  2.50s/it][A
192.168.0.25:  35%|███▌      | 162/460 [06:42<12:26,  2.50s/it][A
192.168.0.25:  35%|███▌      | 163/460 [06:45<12:23,  2.50s/it][A
192.168.0.25:  36%|███▌      | 164/460 [06:47<12:21,  2.50s/it][A
192.168.0.25:  36%|███▌      | 165/460 [06:50<12:18,  2.50s/it][A
192.168.0.25:  36%|███▌      | 166/460 [06:52<12:16,  2.50s/it][A
192.168.0.25:  36%|███▋      | 167/460 [06:55<12:13,  2.50s/it][A
192.168.0.25:  37%|███▋      | 168/460 [06:57<12:10,  2.50s/it][A
192.168.0.25:  37%|███▋      | 169/460 [07:00<12:08,  2.50s/it][A
192.168.0.25:  37%|███▋      | 170/460 [07:02<12:05,  2.50s/it][A
192.168.0.25:  37%|███▋      | 171/460 [07:05<12:03,  2.50s/it][A
192.168.0.25:  37%|███▋      | 172/460 [07:08<12:00,  2.50s/it][A
192.168.0.25:  38%|███▊      | 173/460 [07:10<11:58,  2.50s/it][A
192.168.0.25:  38%|███▊      | 174/460 [07:13<11:55,  2.50s/it][A
192.168.0.25:  38%|███▊      | 175/460 [07:15<11:53,  2.50s/it][A
192.168.0.25:  38%|███▊      | 176/460 [07:18<11:50,  2.50s/it][A
192.168.0.25:  38%|███▊      | 177/460 [07:20<11:48,  2.50s/it][A
192.168.0.25:  39%|███▊      | 178/460 [07:23<11:45,  2.50s/it][A
192.168.0.25:  39%|███▉      | 179/460 [07:25<11:43,  2.50s/it][A
192.168.0.25:  39%|███▉      | 180/460 [07:28<11:40,  2.50s/it][A
192.168.0.25:  39%|███▉      | 181/460 [07:30<11:37,  2.50s/it][A
192.168.0.25:  40%|███▉      | 182/460 [07:33<11:35,  2.50s/it][A
192.168.0.25:  40%|███▉      | 183/460 [07:35<11:32,  2.50s/it][A
192.168.0.25:  40%|████      | 184/460 [07:38<11:30,  2.50s/it][A
192.168.0.25:  40%|████      | 185/460 [07:40<11:27,  2.50s/it][A
192.168.0.25:  40%|████      | 186/460 [07:43<11:25,  2.50s/it][A
192.168.0.25:  41%|████      | 187/460 [07:45<11:23,  2.50s/it][A
192.168.0.25:  41%|████      | 188/460 [07:48<11:21,  2.50s/it][A
192.168.0.25:  41%|████      | 189/460 [07:50<11:18,  2.50s/it][A
192.168.0.25:  41%|████▏     | 190/460 [07:53<11:15,  2.50s/it][A
192.168.0.25:  42%|████▏     | 191/460 [07:55<11:13,  2.50s/it][A
192.168.0.25:  42%|████▏     | 192/460 [07:58<11:10,  2.50s/it][A
192.168.0.25:  42%|████▏     | 193/460 [08:00<11:08,  2.50s/it][A
192.168.0.25:  42%|████▏     | 194/460 [08:03<11:05,  2.50s/it][A
192.168.0.25:  42%|████▏     | 195/460 [08:05<11:03,  2.50s/it][A
192.168.0.25:  43%|████▎     | 196/460 [08:08<11:00,  2.50s/it][A
192.168.0.25:  43%|████▎     | 197/460 [08:10<10:58,  2.50s/it][A
192.168.0.25:  43%|████▎     | 198/460 [08:13<10:55,  2.50s/it][A
192.168.0.25:  43%|████▎     | 199/460 [08:15<10:53,  2.50s/it][A
192.168.0.25:  43%|████▎     | 200/460 [08:18<10:50,  2.50s/it][A
192.168.0.25:  44%|████▎     | 201/460 [08:20<10:48,  2.50s/it][A
192.168.0.25:  44%|████▍     | 202/460 [08:23<10:45,  2.50s/it][A
192.168.0.25:  44%|████▍     | 203/460 [08:25<10:43,  2.50s/it][A
192.168.0.25:  44%|████▍     | 204/460 [08:28<10:41,  2.50s/it][A
192.168.0.25:  45%|████▍     | 205/460 [08:30<10:38,  2.50s/it][A
192.168.0.25:  45%|████▍     | 206/460 [08:33<10:35,  2.50s/it][A
192.168.0.25:  45%|████▌     | 207/460 [08:35<10:33,  2.50s/it][A
192.168.0.25:  45%|████▌     | 208/460 [08:38<10:30,  2.50s/it][A
192.168.0.25:  45%|████▌     | 209/460 [08:40<10:28,  2.50s/it][A
192.168.0.25:  46%|████▌     | 210/460 [08:43<10:25,  2.50s/it][A
192.168.0.25:  46%|████▌     | 211/460 [08:45<10:23,  2.50s/it][A
192.168.0.25:  46%|████▌     | 212/460 [08:48<10:21,  2.50s/it][A
192.168.0.25:  46%|████▋     | 213/460 [08:50<10:19,  2.51s/it][A
192.168.0.25:  47%|████▋     | 214/460 [08:53<10:16,  2.51s/it][A
192.168.0.25:  47%|████▋     | 215/460 [08:55<10:13,  2.51s/it][A
192.168.0.25:  47%|████▋     | 216/460 [08:58<10:10,  2.50s/it][A
192.168.0.25:  47%|████▋     | 217/460 [09:00<10:08,  2.50s/it][A
192.168.0.25:  47%|████▋     | 218/460 [09:03<10:05,  2.50s/it][A
192.168.0.25:  48%|████▊     | 219/460 [09:05<10:02,  2.50s/it][A
192.168.0.25:  48%|████▊     | 220/460 [09:08<10:00,  2.50s/it][A
192.168.0.25:  48%|████▊     | 221/460 [09:10<09:57,  2.50s/it][A
192.168.0.25:  48%|████▊     | 222/460 [09:13<09:55,  2.50s/it][A
192.168.0.25:  48%|████▊     | 223/460 [09:15<09:52,  2.50s/it][A
192.168.0.25:  49%|████▊     | 224/460 [09:18<09:50,  2.50s/it][A
192.168.0.25:  49%|████▉     | 225/460 [09:20<09:48,  2.50s/it][A
192.168.0.25:  49%|████▉     | 226/460 [09:23<09:45,  2.50s/it][A
192.168.0.25:  49%|████▉     | 227/460 [09:25<09:43,  2.50s/it][A
192.168.0.25:  50%|████▉     | 228/460 [09:28<09:40,  2.50s/it][A
192.168.0.25:  50%|████▉     | 229/460 [09:30<09:38,  2.50s/it][A
192.168.0.25:  50%|█████     | 230/460 [09:33<09:35,  2.50s/it][A
192.168.0.25:  50%|█████     | 231/460 [09:35<09:32,  2.50s/it][A
192.168.0.25:  50%|█████     | 232/460 [09:38<09:30,  2.50s/it][A
192.168.0.25:  51%|█████     | 233/460 [09:40<09:27,  2.50s/it][A
192.168.0.25:  51%|█████     | 234/460 [09:43<09:25,  2.50s/it][A
192.168.0.25:  51%|█████     | 235/460 [09:45<09:23,  2.50s/it][A
192.168.0.25:  51%|█████▏    | 236/460 [09:48<09:20,  2.50s/it][A
192.168.0.25:  52%|█████▏    | 237/460 [09:50<09:18,  2.50s/it][A
192.168.0.25:  52%|█████▏    | 238/460 [09:53<09:15,  2.50s/it][A
192.168.0.25:  52%|█████▏    | 239/460 [09:55<09:13,  2.50s/it][A
192.168.0.25:  52%|█████▏    | 240/460 [09:58<09:10,  2.50s/it][A
192.168.0.25:  52%|█████▏    | 241/460 [10:00<09:08,  2.50s/it][A
192.168.0.25:  53%|█████▎    | 242/460 [10:03<09:05,  2.50s/it][A
192.168.0.25:  53%|█████▎    | 243/460 [10:05<09:03,  2.50s/it][A
192.168.0.25:  53%|█████▎    | 244/460 [10:08<09:00,  2.50s/it][A
192.168.0.25:  53%|█████▎    | 245/460 [10:10<08:58,  2.50s/it][A
192.168.0.25:  53%|█████▎    | 246/460 [10:13<08:55,  2.50s/it][A
192.168.0.25:  54%|█████▎    | 247/460 [10:15<08:52,  2.50s/it][A
192.168.0.25:  54%|█████▍    | 248/460 [10:18<08:50,  2.50s/it][A
192.168.0.25:  54%|█████▍    | 249/460 [10:20<08:48,  2.50s/it][A
192.168.0.25:  54%|█████▍    | 250/460 [10:23<08:45,  2.50s/it][A
192.168.0.25:  55%|█████▍    | 251/460 [10:25<08:43,  2.50s/it][A
192.168.0.25:  55%|█████▍    | 252/460 [10:28<08:40,  2.50s/it][A
192.168.0.25:  55%|█████▌    | 253/460 [10:30<08:38,  2.50s/it][A
192.168.0.25:  55%|█████▌    | 254/460 [10:33<08:35,  2.50s/it][A
192.168.0.25:  55%|█████▌    | 255/460 [10:35<08:33,  2.50s/it][A
192.168.0.25:  56%|█████▌    | 256/460 [10:38<08:30,  2.50s/it][A
192.168.0.25:  56%|█████▌    | 257/460 [10:40<08:28,  2.50s/it][A
192.168.0.25:  56%|█████▌    | 258/460 [10:43<08:25,  2.50s/it][A
192.168.0.25:  56%|█████▋    | 259/460 [10:45<08:23,  2.50s/it][A
192.168.0.25:  57%|█████▋    | 260/460 [10:48<08:20,  2.50s/it][A
192.168.0.25:  57%|█████▋    | 261/460 [10:50<08:18,  2.50s/it][A
192.168.0.25:  57%|█████▋    | 262/460 [10:53<08:15,  2.50s/it][A
192.168.0.25:  57%|█████▋    | 263/460 [10:55<08:13,  2.50s/it][A
192.168.0.25:  57%|█████▋    | 264/460 [10:58<08:10,  2.50s/it][A
192.168.0.25:  58%|█████▊    | 265/460 [11:00<08:08,  2.50s/it][A
192.168.0.25:  58%|█████▊    | 266/460 [11:03<08:05,  2.50s/it][A
192.168.0.25:  58%|█████▊    | 267/460 [11:05<08:03,  2.50s/it][A
192.168.0.25:  58%|█████▊    | 268/460 [11:08<08:00,  2.50s/it][A
192.168.0.25:  58%|█████▊    | 269/460 [11:10<07:57,  2.50s/it][A
192.168.0.25:  59%|█████▊    | 270/460 [11:13<07:55,  2.50s/it][A
192.168.0.25:  59%|█████▉    | 271/460 [11:15<07:52,  2.50s/it][A
192.168.0.25:  59%|█████▉    | 272/460 [11:18<07:50,  2.50s/it][A
192.168.0.25:  59%|█████▉    | 273/460 [11:20<07:47,  2.50s/it][A
192.168.0.25:  60%|█████▉    | 274/460 [11:23<07:45,  2.50s/it][A
192.168.0.25:  60%|█████▉    | 275/460 [11:25<07:43,  2.50s/it][A
192.168.0.25:  60%|██████    | 276/460 [11:28<07:40,  2.50s/it][A
192.168.0.25:  60%|██████    | 277/460 [11:30<07:38,  2.50s/it][A
192.168.0.25:  60%|██████    | 278/460 [11:33<07:35,  2.50s/it][A
192.168.0.25:  61%|██████    | 279/460 [11:35<07:33,  2.50s/it][A
192.168.0.25:  61%|██████    | 280/460 [11:38<07:30,  2.50s/it][A
192.168.0.25:  61%|██████    | 281/460 [11:40<07:27,  2.50s/it][A
192.168.0.25:  61%|██████▏   | 282/460 [11:43<07:25,  2.50s/it][A
192.168.0.25:  62%|██████▏   | 283/460 [11:45<07:22,  2.50s/it][A
192.168.0.25:  62%|██████▏   | 284/460 [11:48<07:20,  2.50s/it][A
192.168.0.25:  62%|██████▏   | 285/460 [11:50<07:17,  2.50s/it][A
192.168.0.25:  62%|██████▏   | 286/460 [11:53<07:15,  2.50s/it][A
192.168.0.25:  62%|██████▏   | 287/460 [11:55<07:12,  2.50s/it][A
192.168.0.25:  63%|██████▎   | 288/460 [11:58<07:10,  2.50s/it][A
192.168.0.25:  63%|██████▎   | 289/460 [12:00<07:07,  2.50s/it][A
192.168.0.25:  63%|██████▎   | 290/460 [12:03<07:05,  2.50s/it][A
192.168.0.25:  63%|██████▎   | 291/460 [12:05<07:02,  2.50s/it][A
192.168.0.25:  63%|██████▎   | 292/460 [12:08<07:00,  2.50s/it][A
192.168.0.25:  64%|██████▎   | 293/460 [12:10<06:57,  2.50s/it][A
192.168.0.25:  64%|██████▍   | 294/460 [12:13<06:55,  2.50s/it][A
192.168.0.25:  64%|██████▍   | 295/460 [12:15<06:52,  2.50s/it][A
192.168.0.25:  64%|██████▍   | 296/460 [12:18<06:50,  2.50s/it][A
192.168.0.25:  65%|██████▍   | 297/460 [12:20<06:47,  2.50s/it][A
192.168.0.25:  65%|██████▍   | 298/460 [12:23<06:45,  2.50s/it][A
192.168.0.25:  65%|██████▌   | 299/460 [12:25<06:43,  2.50s/it][A
192.168.0.25:  65%|██████▌   | 300/460 [12:28<06:40,  2.50s/it][A
192.168.0.25:  65%|██████▌   | 301/460 [12:30<06:38,  2.50s/it][A
192.168.0.25:  66%|██████▌   | 302/460 [12:33<06:35,  2.50s/it][A
192.168.0.25:  66%|██████▌   | 303/460 [12:35<06:32,  2.50s/it][A
192.168.0.25:  66%|██████▌   | 304/460 [12:38<06:30,  2.50s/it][A
192.168.0.25:  66%|██████▋   | 305/460 [12:40<06:27,  2.50s/it][A
192.168.0.25:  67%|██████▋   | 306/460 [12:43<06:25,  2.50s/it][A
192.168.0.25:  67%|██████▋   | 307/460 [12:45<06:23,  2.50s/it][A
192.168.0.25:  67%|██████▋   | 308/460 [12:48<06:20,  2.50s/it][A
192.168.0.25:  67%|██████▋   | 309/460 [12:50<06:17,  2.50s/it][A
192.168.0.25:  67%|██████▋   | 310/460 [12:53<06:15,  2.50s/it][A
192.168.0.25:  68%|██████▊   | 311/460 [12:55<06:13,  2.50s/it][A
192.168.0.25:  68%|██████▊   | 312/460 [12:58<06:10,  2.50s/it][A
192.168.0.25:  68%|██████▊   | 313/460 [13:00<06:07,  2.50s/it][A
192.168.0.25:  68%|██████▊   | 314/460 [13:03<06:05,  2.50s/it][A
192.168.0.25:  68%|██████▊   | 315/460 [13:05<06:03,  2.50s/it][A
192.168.0.25:  69%|██████▊   | 316/460 [13:08<06:00,  2.50s/it][A
192.168.0.25:  69%|██████▉   | 317/460 [13:10<05:58,  2.50s/it][A
192.168.0.25:  69%|██████▉   | 318/460 [13:13<05:55,  2.50s/it][A
192.168.0.25:  69%|██████▉   | 319/460 [13:15<05:52,  2.50s/it][A
192.168.0.25:  70%|██████▉   | 320/460 [13:18<05:50,  2.50s/it][A
192.168.0.25:  70%|██████▉   | 321/460 [13:20<05:47,  2.50s/it][A
192.168.0.25:  70%|███████   | 322/460 [13:23<05:45,  2.50s/it][A
192.168.0.25:  70%|███████   | 323/460 [13:25<05:42,  2.50s/it][A
192.168.0.25:  70%|███████   | 324/460 [13:28<05:40,  2.50s/it][A
192.168.0.25:  71%|███████   | 325/460 [13:30<05:37,  2.50s/it][A
192.168.0.25:  71%|███████   | 326/460 [13:33<05:35,  2.50s/it][A
192.168.0.25:  71%|███████   | 327/460 [13:35<05:32,  2.50s/it][A
192.168.0.25:  71%|███████▏  | 328/460 [13:38<05:30,  2.50s/it][A
192.168.0.25:  72%|███████▏  | 329/460 [13:40<05:27,  2.50s/it][A
192.168.0.25:  72%|███████▏  | 330/460 [13:43<05:25,  2.50s/it][A
192.168.0.25:  72%|███████▏  | 331/460 [13:45<05:22,  2.50s/it][A
192.168.0.25:  72%|███████▏  | 332/460 [13:48<05:20,  2.50s/it][A
192.168.0.25:  72%|███████▏  | 333/460 [13:50<05:17,  2.50s/it][A
192.168.0.25:  73%|███████▎  | 334/460 [13:53<05:15,  2.50s/it][A
192.168.0.25:  73%|███████▎  | 335/460 [13:55<05:12,  2.50s/it][A
192.168.0.25:  73%|███████▎  | 336/460 [13:58<05:10,  2.50s/it][A
192.168.0.25:  73%|███████▎  | 337/460 [14:00<05:07,  2.50s/it][A
192.168.0.25:  73%|███████▎  | 338/460 [14:03<05:05,  2.50s/it][A
192.168.0.25:  74%|███████▎  | 339/460 [14:05<05:02,  2.50s/it][A
192.168.0.25:  74%|███████▍  | 340/460 [14:08<05:00,  2.50s/it][A
192.168.0.25:  74%|███████▍  | 341/460 [14:10<04:57,  2.50s/it][A
192.168.0.25:  74%|███████▍  | 342/460 [14:13<04:55,  2.50s/it][A
192.168.0.25:  75%|███████▍  | 343/460 [14:15<04:52,  2.50s/it][A
192.168.0.25:  75%|███████▍  | 344/460 [14:18<04:50,  2.50s/it][A
192.168.0.25:  75%|███████▌  | 345/460 [14:20<04:47,  2.50s/it][A
192.168.0.25:  75%|███████▌  | 346/460 [14:23<04:45,  2.50s/it][A
192.168.0.25:  75%|███████▌  | 347/460 [14:26<04:42,  2.50s/it][A
192.168.0.25:  76%|███████▌  | 348/460 [14:28<04:40,  2.50s/it][A
192.168.0.25:  76%|███████▌  | 349/460 [14:31<04:37,  2.50s/it][A
192.168.0.25:  76%|███████▌  | 350/460 [14:33<04:35,  2.50s/it][A
192.168.0.25:  76%|███████▋  | 351/460 [14:36<04:32,  2.50s/it][A
192.168.0.25:  77%|███████▋  | 352/460 [14:38<04:30,  2.50s/it][A
192.168.0.25:  77%|███████▋  | 353/460 [14:41<04:27,  2.50s/it][A
192.168.0.25:  77%|███████▋  | 354/460 [14:43<04:25,  2.50s/it][A
192.168.0.25:  77%|███████▋  | 355/460 [14:46<04:22,  2.50s/it][A
192.168.0.25:  77%|███████▋  | 356/460 [14:48<04:20,  2.50s/it][A
192.168.0.25:  78%|███████▊  | 357/460 [14:51<04:17,  2.50s/it][A
192.168.0.25:  78%|███████▊  | 358/460 [14:53<04:15,  2.50s/it][A
192.168.0.25:  78%|███████▊  | 359/460 [14:56<04:12,  2.50s/it][A
192.168.0.25:  78%|███████▊  | 360/460 [14:58<04:10,  2.50s/it][A
192.168.0.25:  78%|███████▊  | 361/460 [15:01<04:07,  2.50s/it][A
192.168.0.25:  79%|███████▊  | 362/460 [15:03<04:05,  2.50s/it][A
192.168.0.25:  79%|███████▉  | 363/460 [15:06<04:02,  2.50s/it][A
192.168.0.25:  79%|███████▉  | 364/460 [15:08<04:00,  2.50s/it][A
192.168.0.25:  79%|███████▉  | 365/460 [15:11<03:57,  2.50s/it][A
192.168.0.25:  80%|███████▉  | 366/460 [15:13<03:55,  2.50s/it][A
192.168.0.25:  80%|███████▉  | 367/460 [15:16<03:52,  2.50s/it][A
192.168.0.25:  80%|████████  | 368/460 [15:18<03:50,  2.50s/it][A
192.168.0.25:  80%|████████  | 369/460 [15:21<03:47,  2.50s/it][A
192.168.0.25:  80%|████████  | 370/460 [15:23<03:45,  2.50s/it][A
192.168.0.25:  81%|████████  | 371/460 [15:26<03:42,  2.50s/it][A
192.168.0.25:  81%|████████  | 372/460 [15:28<03:40,  2.51s/it][A
192.168.0.25:  81%|████████  | 373/460 [15:31<03:37,  2.51s/it][A
192.168.0.25:  81%|████████▏ | 374/460 [15:33<03:35,  2.50s/it][A
192.168.0.25:  82%|████████▏ | 375/460 [15:36<03:32,  2.50s/it][A
192.168.0.25:  82%|████████▏ | 376/460 [15:38<03:30,  2.50s/it][A
192.168.0.25:  82%|████████▏ | 377/460 [15:41<03:27,  2.50s/it][A
192.168.0.25:  82%|████████▏ | 378/460 [15:43<03:25,  2.50s/it][A
192.168.0.25:  82%|████████▏ | 379/460 [15:46<03:22,  2.50s/it][A
192.168.0.25:  83%|████████▎ | 380/460 [15:48<03:20,  2.50s/it][A
192.168.0.25:  83%|████████▎ | 381/460 [15:51<03:17,  2.50s/it][A
192.168.0.25:  83%|████████▎ | 382/460 [15:53<03:15,  2.50s/it][A
192.168.0.25:  83%|████████▎ | 383/460 [15:56<03:12,  2.50s/it][A
192.168.0.25:  83%|████████▎ | 384/460 [15:58<03:10,  2.50s/it][A
192.168.0.25:  84%|████████▎ | 385/460 [16:01<03:07,  2.50s/it][A
192.168.0.25:  84%|████████▍ | 386/460 [16:03<03:05,  2.50s/it][A
192.168.0.25:  84%|████████▍ | 387/460 [16:06<03:02,  2.50s/it][A
192.168.0.25:  84%|████████▍ | 388/460 [16:08<03:00,  2.50s/it][A
192.168.0.25:  85%|████████▍ | 389/460 [16:11<02:57,  2.50s/it][A
192.168.0.25:  85%|████████▍ | 390/460 [16:13<02:55,  2.50s/it][A
192.168.0.25:  85%|████████▌ | 391/460 [16:16<02:52,  2.50s/it][A
192.168.0.25:  85%|████████▌ | 392/460 [16:18<02:50,  2.50s/it][A
192.168.0.25:  85%|████████▌ | 393/460 [16:21<02:47,  2.50s/it][A
192.168.0.25:  86%|████████▌ | 394/460 [16:23<02:45,  2.50s/it][A
192.168.0.25:  86%|████████▌ | 395/460 [16:26<02:42,  2.50s/it][A
192.168.0.25:  86%|████████▌ | 396/460 [16:28<02:40,  2.50s/it][A
192.168.0.25:  86%|████████▋ | 397/460 [16:31<02:37,  2.50s/it][A
192.168.0.25:  87%|████████▋ | 398/460 [16:33<02:35,  2.50s/it][A
192.168.0.25:  87%|████████▋ | 399/460 [16:36<02:32,  2.50s/it][A
192.168.0.25:  87%|████████▋ | 400/460 [16:38<02:30,  2.50s/it][A
192.168.0.25:  87%|████████▋ | 401/460 [16:41<02:27,  2.50s/it][A
192.168.0.25:  87%|████████▋ | 402/460 [16:43<02:25,  2.50s/it][A
192.168.0.25:  88%|████████▊ | 403/460 [16:46<02:22,  2.50s/it][A
192.168.0.25:  88%|████████▊ | 404/460 [16:48<02:20,  2.50s/it][A
192.168.0.25:  88%|████████▊ | 405/460 [16:51<02:17,  2.50s/it][A
192.168.0.25:  88%|████████▊ | 406/460 [16:53<02:15,  2.50s/it][A
192.168.0.25:  88%|████████▊ | 407/460 [16:56<02:12,  2.50s/it][A
192.168.0.25:  89%|████████▊ | 408/460 [16:58<02:10,  2.50s/it][A
192.168.0.25:  89%|████████▉ | 409/460 [17:01<02:07,  2.50s/it][A
192.168.0.25:  89%|████████▉ | 410/460 [17:03<02:05,  2.50s/it][A
192.168.0.25:  89%|████████▉ | 411/460 [17:06<02:02,  2.50s/it][A
192.168.0.25:  90%|████████▉ | 412/460 [17:08<02:00,  2.50s/it][A
192.168.0.25:  90%|████████▉ | 413/460 [17:11<01:57,  2.50s/it][A
192.168.0.25:  90%|█████████ | 414/460 [17:13<01:55,  2.50s/it][A
192.168.0.25:  90%|█████████ | 415/460 [17:16<01:52,  2.50s/it][A
192.168.0.25:  90%|█████████ | 416/460 [17:18<01:50,  2.50s/it][A
192.168.0.25:  91%|█████████ | 417/460 [17:21<01:47,  2.50s/it][A
192.168.0.25:  91%|█████████ | 418/460 [17:23<01:45,  2.50s/it][A
192.168.0.25:  91%|█████████ | 419/460 [17:26<01:42,  2.50s/it][A
192.168.0.25:  91%|█████████▏| 420/460 [17:28<01:40,  2.50s/it][A
192.168.0.25:  92%|█████████▏| 421/460 [17:31<01:37,  2.50s/it][A
192.168.0.25:  92%|█████████▏| 422/460 [17:33<01:35,  2.50s/it][A
192.168.0.25:  92%|█████████▏| 423/460 [17:36<01:32,  2.51s/it][A
192.168.0.25:  92%|█████████▏| 424/460 [17:38<01:30,  2.51s/it][A
192.168.0.25:  92%|█████████▏| 425/460 [17:41<01:27,  2.50s/it][A
192.168.0.25:  93%|█████████▎| 426/460 [17:43<01:25,  2.50s/it][A
192.168.0.25:  93%|█████████▎| 427/460 [17:46<01:22,  2.51s/it][A
192.168.0.25:  93%|█████████▎| 428/460 [17:48<01:20,  2.50s/it][A
192.168.0.25:  93%|█████████▎| 429/460 [17:51<01:17,  2.50s/it][A
192.168.0.25:  93%|█████████▎| 430/460 [17:53<01:15,  2.51s/it][A
192.168.0.25:  94%|█████████▎| 431/460 [17:56<01:12,  2.50s/it][A
192.168.0.25:  94%|█████████▍| 432/460 [17:58<01:10,  2.50s/it][A
192.168.0.25:  94%|█████████▍| 433/460 [18:01<01:07,  2.50s/it][A
192.168.0.25:  94%|█████████▍| 434/460 [18:03<01:05,  2.50s/it][A
192.168.0.25:  95%|█████████▍| 435/460 [18:06<01:02,  2.50s/it][A
192.168.0.25:  95%|█████████▍| 436/460 [18:08<01:00,  2.50s/it][A
192.168.0.25:  95%|█████████▌| 437/460 [18:11<00:57,  2.50s/it][A
192.168.0.25:  95%|█████████▌| 438/460 [18:13<00:55,  2.50s/it][A
192.168.0.25:  95%|█████████▌| 439/460 [18:16<00:52,  2.50s/it][A
192.168.0.25:  96%|█████████▌| 440/460 [18:18<00:50,  2.50s/it][A
192.168.0.25:  96%|█████████▌| 441/460 [18:21<00:47,  2.50s/it][A
192.168.0.25:  96%|█████████▌| 442/460 [18:23<00:45,  2.50s/it][A
192.168.0.25:  96%|█████████▋| 443/460 [18:26<00:42,  2.50s/it][A
192.168.0.25:  97%|█████████▋| 444/460 [18:28<00:40,  2.50s/it][A
192.168.0.25:  97%|█████████▋| 445/460 [18:31<00:37,  2.50s/it][A
192.168.0.25:  97%|█████████▋| 446/460 [18:33<00:35,  2.50s/it][A
192.168.0.25:  97%|█████████▋| 447/460 [18:36<00:32,  2.50s/it][A
192.168.0.25:  97%|█████████▋| 448/460 [18:38<00:30,  2.50s/it][A
192.168.0.25:  98%|█████████▊| 449/460 [18:41<00:27,  2.50s/it][A
192.168.0.25:  98%|█████████▊| 450/460 [18:43<00:25,  2.50s/it][A
192.168.0.25:  98%|█████████▊| 451/460 [18:46<00:22,  2.50s/it][A
192.168.0.25:  98%|█████████▊| 452/460 [18:48<00:20,  2.50s/it][A
192.168.0.25:  98%|█████████▊| 453/460 [18:51<00:17,  2.50s/it][A
192.168.0.25:  99%|█████████▊| 454/460 [18:53<00:15,  2.50s/it][A
192.168.0.25:  99%|█████████▉| 455/460 [18:56<00:12,  2.50s/it][A
192.168.0.25:  99%|█████████▉| 456/460 [18:58<00:10,  2.50s/it][A
192.168.0.25:  99%|█████████▉| 457/460 [19:01<00:07,  2.50s/it][A
192.168.0.25: 100%|█████████▉| 458/460 [19:03<00:05,  2.50s/it][A
192.168.0.25: 100%|█████████▉| 459/460 [19:06<00:02,  2.50s/it][A
192.168.0.25: 100%|██████████| 460/460 [19:08<00:00,  2.50s/it][A                                                     
192.168.0.25: {'eval_loss': 3.2334933280944824, 'eval_runtime': 1151.3384, 'eval_samples_per_second': 12.775, 'eval_steps_per_second': 0.4, 'epoch': 2.9}
192.168.0.25:                                                  [A 97%|█████████▋| 1500/1554 [5:25:40<09:18, 10.35s/it]
192.168.0.25: 100%|██████████| 460/460 [19:09<00:00,  2.50s/it][A
192.168.0.25: {'loss': 3.1478, 'grad_norm': 1.7749776328383453, 'learning_rate': 2.0286190358517754e-08, 'epoch': 2.92}
192.168.0.25: {'loss': 3.1629, 'grad_norm': 1.963583145580432, 'learning_rate': 1.2116338250452997e-08, 'epoch': 2.93}
192.168.0.25: {'loss': 3.1428, 'grad_norm': 1.7653701733368758, 'learning_rate': 6.038430660670891e-09, 'epoch': 2.95}
192.168.0.25: {'loss': 3.1688, 'grad_norm': 1.751099649710242, 'learning_rate': 2.055016695430845e-09, 'epoch': 2.97}
192.168.0.25: {'loss': 3.1746, 'grad_norm': 1.9108607079460604, 'learning_rate': 1.6776701940335317e-10, 'epoch': 2.99}
192.168.0.25:                                                  [A 97%|█████████▋| 1501/1554 [5:25:49<5:14:03, 355.54s/it] 97%|█████████▋| 1502/1554 [5:25:59<3:38:12, 251.78s/it] 97%|█████████▋| 1503/1554 [5:26:09<2:32:15, 179.13s/it] 97%|█████████▋| 1504/1554 [5:26:18<1:46:53, 128.27s/it] 97%|█████████▋| 1505/1554 [5:26:28<1:15:40, 92.66s/it]  97%|█████████▋| 1506/1554 [5:26:37<54:12, 67.75s/it]   97%|█████████▋| 1507/1554 [5:26:47<39:24, 50.31s/it] 97%|█████████▋| 1508/1554 [5:26:57<29:12, 38.10s/it] 97%|█████████▋| 1509/1554 [5:27:06<22:09, 29.55s/it] 97%|█████████▋| 1510/1554 [5:27:16<17:16, 23.56s/it]                                                      97%|█████████▋| 1510/1554 [5:27:16<17:16, 23.56s/it] 97%|█████████▋| 1511/1554 [5:27:25<13:52, 19.37s/it] 97%|█████████▋| 1512/1554 [5:27:35<11:30, 16.44s/it] 97%|█████████▋| 1513/1554 [5:27:45<09:49, 14.39s/it] 97%|█████████▋| 1514/1554 [5:27:54<08:37, 12.94s/it] 97%|█████████▋| 1515/1554 [5:28:04<07:45, 11.93s/it] 98%|█████████▊| 1516/1554 [5:28:13<07:06, 11.24s/it] 98%|█████████▊| 1517/1554 [5:28:23<06:37, 10.74s/it] 98%|█████████▊| 1518/1554 [5:28:33<06:15, 10.42s/it] 98%|█████████▊| 1519/1554 [5:28:42<05:56, 10.18s/it] 98%|█████████▊| 1520/1554 [5:28:52<05:40, 10.03s/it]                                                      98%|█████████▊| 1520/1554 [5:28:52<05:40, 10.03s/it] 98%|█████████▊| 1521/1554 [5:29:02<05:27,  9.91s/it] 98%|█████████▊| 1522/1554 [5:29:11<05:14,  9.82s/it] 98%|█████████▊| 1523/1554 [5:29:21<05:02,  9.77s/it] 98%|█████████▊| 1524/1554 [5:29:31<04:52,  9.75s/it] 98%|█████████▊| 1525/1554 [5:29:40<04:42,  9.74s/it] 98%|█████████▊| 1526/1554 [5:29:50<04:32,  9.72s/it] 98%|█████████▊| 1527/1554 [5:30:00<04:21,  9.70s/it] 98%|█████████▊| 1528/1554 [5:30:09<04:12,  9.71s/it] 98%|█████████▊| 1529/1554 [5:30:19<04:02,  9.71s/it] 98%|█████████▊| 1530/1554 [5:30:29<03:53,  9.72s/it]                                                      98%|█████████▊| 1530/1554 [5:30:29<03:53,  9.72s/it] 99%|█████████▊| 1531/1554 [5:30:39<03:43,  9.73s/it] 99%|█████████▊| 1532/1554 [5:30:48<03:34,  9.74s/it] 99%|█████████▊| 1533/1554 [5:30:58<03:24,  9.74s/it] 99%|█████████▊| 1534/1554 [5:31:08<03:15,  9.77s/it] 99%|█████████▉| 1535/1554 [5:31:18<03:05,  9.77s/it] 99%|█████████▉| 1536/1554 [5:31:28<02:56,  9.79s/it] 99%|█████████▉| 1537/1554 [5:31:37<02:46,  9.81s/it] 99%|█████████▉| 1538/1554 [5:31:47<02:37,  9.81s/it] 99%|█████████▉| 1539/1554 [5:31:57<02:27,  9.84s/it] 99%|█████████▉| 1540/1554 [5:32:07<02:17,  9.84s/it]                                                      99%|█████████▉| 1540/1554 [5:32:07<02:17,  9.84s/it] 99%|█████████▉| 1541/1554 [5:32:17<02:08,  9.87s/it] 99%|█████████▉| 1542/1554 [5:32:27<01:58,  9.88s/it] 99%|█████████▉| 1543/1554 [5:32:37<01:48,  9.89s/it] 99%|█████████▉| 1544/1554 [5:32:47<01:38,  9.89s/it] 99%|█████████▉| 1545/1554 [5:32:57<01:29,  9.91s/it] 99%|█████████▉| 1546/1554 [5:33:07<01:19,  9.94s/it]100%|█████████▉| 1547/1554 [5:33:17<01:09,  9.97s/it]100%|█████████▉| 1548/1554 [5:33:27<00:59,  9.97s/it]100%|█████████▉| 1549/1554 [5:33:37<00:49,  9.99s/it]100%|█████████▉| 1550/1554 [5:33:47<00:39,  9.99s/it]                                                     100%|█████████▉| 1550/1554 [5:33:47<00:39,  9.99s/it]100%|█████████▉| 1551/1554 [5:33:57<00:30, 10.01s/it]100%|█████████▉| 1552/1554 [5:34:07<00:20, 10.02s/it]100%|█████████▉| 1553/1554 [5:34:17<00:10, 10.03s/it]100%|██████████| 1554/1554 [5:34:27<00:00, 10.04s/it][INFO|trainer.py:3503] 2024-09-29 17:06:59,643 >> Saving model checkpoint to /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1554
192.168.0.25: [INFO|configuration_utils.py:472] 2024-09-29 17:06:59,652 >> Configuration saved in /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1554/config.json
192.168.0.25: [INFO|configuration_utils.py:807] 2024-09-29 17:06:59,654 >> Configuration saved in /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1554/generation_config.json
192.168.0.25: [INFO|modeling_utils.py:2807] 2024-09-29 17:07:34,231 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1554/model.safetensors.index.json.
192.168.0.25: [INFO|tokenization_utils_base.py:2684] 2024-09-29 17:07:34,254 >> tokenizer config file saved in /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1554/tokenizer_config.json
192.168.0.25: [INFO|tokenization_utils_base.py:2693] 2024-09-29 17:07:34,256 >> Special tokens file saved in /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1554/special_tokens_map.json
192.168.0.25: [2024-09-29 17:07:34,670] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1554 is about to be saved!
192.168.0.89: [2024-09-29 17:07:18,990] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1554/global_step1554/zero_pp_rank_16_mp_rank_00_model_states.pt...
192.168.0.149: [2024-09-29 17:07:37,736] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1554/global_step1554/zero_pp_rank_8_mp_rank_00_model_states.pt...
192.168.0.13: [2024-09-29 17:07:35,139] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1554/global_step1554/zero_pp_rank_24_mp_rank_00_model_states.pt...
192.168.0.25: [2024-09-29 17:07:34,698] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1554/global_step1554/zero_pp_rank_0_mp_rank_00_model_states.pt
192.168.0.25: [2024-09-29 17:07:34,698] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1554/global_step1554/zero_pp_rank_0_mp_rank_00_model_states.pt...
192.168.0.89: [2024-09-29 17:07:19,038] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1554/global_step1554/zero_pp_rank_16_mp_rank_00_model_states.pt.
192.168.0.149: [2024-09-29 17:07:37,784] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1554/global_step1554/zero_pp_rank_8_mp_rank_00_model_states.pt.
192.168.0.25: [2024-09-29 17:07:34,745] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1554/global_step1554/zero_pp_rank_0_mp_rank_00_model_states.pt.
192.168.0.13: [2024-09-29 17:07:35,190] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1554/global_step1554/zero_pp_rank_24_mp_rank_00_model_states.pt.
192.168.0.89: [2024-09-29 17:07:19,121] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1554/global_step1554/zero_pp_rank_16_mp_rank_00_optim_states.pt...
192.168.0.25: [2024-09-29 17:07:34,823] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1554/global_step1554/zero_pp_rank_0_mp_rank_00_optim_states.pt...
192.168.0.13: [2024-09-29 17:07:35,269] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1554/global_step1554/zero_pp_rank_24_mp_rank_00_optim_states.pt...
192.168.0.149: [2024-09-29 17:07:37,868] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1554/global_step1554/zero_pp_rank_8_mp_rank_00_optim_states.pt...
192.168.0.13: [2024-09-29 17:07:46,304] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1554/global_step1554/zero_pp_rank_24_mp_rank_00_optim_states.pt.
192.168.0.13: [2024-09-29 17:07:46,304] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1554/global_step1554/zero_pp_rank_24_mp_rank_00_optim_states.pt
192.168.0.25: [2024-09-29 17:07:46,836] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1554/global_step1554/zero_pp_rank_0_mp_rank_00_optim_states.pt.
192.168.0.25: [2024-09-29 17:07:46,837] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1554/global_step1554/zero_pp_rank_0_mp_rank_00_optim_states.pt
192.168.0.89: [2024-09-29 17:07:31,306] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1554/global_step1554/zero_pp_rank_16_mp_rank_00_optim_states.pt.
192.168.0.89: [2024-09-29 17:07:31,307] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1554/global_step1554/zero_pp_rank_16_mp_rank_00_optim_states.pt
192.168.0.149: [2024-09-29 17:07:50,110] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1554/global_step1554/zero_pp_rank_8_mp_rank_00_optim_states.pt.
192.168.0.149: [2024-09-29 17:07:50,110] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/fcl/LLaMA-Factory/saves/240929/checkpoint-1554/global_step1554/zero_pp_rank_8_mp_rank_00_optim_states.pt
192.168.0.25: [2024-09-29 17:07:47,566] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1554 is ready now!
192.168.0.89: [2024-09-29 17:07:31,864] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1554 is ready now!
192.168.0.149: [2024-09-29 17:07:50,611] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1554 is ready now!
192.168.0.13: [2024-09-29 17:07:48,013] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1554 is ready now!
192.168.0.13: [INFO|trainer.py:2394] 2024-09-29 17:07:48,023 >> 
192.168.0.13: 
192.168.0.13: Training completed. Do not forget to share your model on huggingface.co/models =)
192.168.0.13: 
192.168.0.13: 
192.168.0.89: [INFO|trainer.py:2394] 2024-09-29 17:07:31,876 >> 
192.168.0.89: 
192.168.0.89: Training completed. Do not forget to share your model on huggingface.co/models =)
192.168.0.89: 
192.168.0.89: 
192.168.0.149: [INFO|trainer.py:2394] 2024-09-29 17:07:50,622 >> 
192.168.0.149: 
192.168.0.149: Training completed. Do not forget to share your model on huggingface.co/models =)
192.168.0.149: 
192.168.0.149: 
192.168.0.25: [INFO|trainer.py:2394] 2024-09-29 17:07:47,589 >> 
192.168.0.25: 
192.168.0.25: Training completed. Do not forget to share your model on huggingface.co/models =)
192.168.0.25: 
192.168.0.25: 
192.168.0.25: {'train_runtime': 20124.1136, 'train_samples_per_second': 19.732, 'train_steps_per_second': 0.077, 'train_loss': 3.546865950526725, 'epoch': 3.0}
192.168.0.25:                                                      100%|██████████| 1554/1554 [5:35:23<00:00, 10.04s/it]100%|██████████| 1554/1554 [5:35:23<00:00, 12.95s/it]
192.168.0.25: [INFO|trainer.py:3503] 2024-09-29 17:07:56,459 >> Saving model checkpoint to /root/fcl/LLaMA-Factory/saves/240929
192.168.0.25: [INFO|configuration_utils.py:472] 2024-09-29 17:07:56,465 >> Configuration saved in /root/fcl/LLaMA-Factory/saves/240929/config.json
192.168.0.25: [INFO|configuration_utils.py:807] 2024-09-29 17:07:56,466 >> Configuration saved in /root/fcl/LLaMA-Factory/saves/240929/generation_config.json
192.168.0.89: [INFO|trainer.py:3819] 2024-09-29 17:07:40,772 >> 
192.168.0.89: ***** Running Evaluation *****
192.168.0.89: [INFO|trainer.py:3821] 2024-09-29 17:07:40,772 >>   Num examples = 14708
192.168.0.89: [INFO|trainer.py:3824] 2024-09-29 17:07:40,772 >>   Batch size = 1
192.168.0.149: [INFO|trainer.py:3819] 2024-09-29 17:07:59,520 >> 
192.168.0.149: ***** Running Evaluation *****
192.168.0.149: [INFO|trainer.py:3821] 2024-09-29 17:07:59,521 >>   Num examples = 14708
192.168.0.149: [INFO|trainer.py:3824] 2024-09-29 17:07:59,521 >>   Batch size = 1
192.168.0.13: [INFO|trainer.py:3819] 2024-09-29 17:07:56,927 >> 
192.168.0.13: ***** Running Evaluation *****
192.168.0.13: [INFO|trainer.py:3821] 2024-09-29 17:07:56,927 >>   Num examples = 14708
192.168.0.13: [INFO|trainer.py:3824] 2024-09-29 17:07:56,927 >>   Batch size = 1
192.168.0.25: [INFO|modeling_utils.py:2807] 2024-09-29 17:08:29,007 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /root/fcl/LLaMA-Factory/saves/240929/model.safetensors.index.json.
192.168.0.25: [INFO|tokenization_utils_base.py:2684] 2024-09-29 17:08:29,029 >> tokenizer config file saved in /root/fcl/LLaMA-Factory/saves/240929/tokenizer_config.json
192.168.0.25: [INFO|tokenization_utils_base.py:2693] 2024-09-29 17:08:29,031 >> Special tokens file saved in /root/fcl/LLaMA-Factory/saves/240929/special_tokens_map.json
192.168.0.25: ***** train metrics *****
192.168.0.25:   epoch                    =        3.0
192.168.0.25:   total_flos               =   606060GF
192.168.0.25:   train_loss               =     3.5469
192.168.0.25:   train_runtime            = 5:35:24.11
192.168.0.25:   train_samples_per_second =     19.732
192.168.0.25:   train_steps_per_second   =      0.077
192.168.0.25: <frozen importlib._bootstrap>:914: ImportWarning: TEMetaPathFinder.find_spec() not found; falling back to find_module()
192.168.0.25: Figure saved at: /root/fcl/LLaMA-Factory/saves/240929/training_loss.png
192.168.0.25: Figure saved at: /root/fcl/LLaMA-Factory/saves/240929/training_eval_loss.png
192.168.0.25: [INFO|trainer.py:3819] 2024-09-29 17:08:31,713 >> 
192.168.0.25: ***** Running Evaluation *****
192.168.0.25: [INFO|trainer.py:3821] 2024-09-29 17:08:31,713 >>   Num examples = 14708
192.168.0.25: [INFO|trainer.py:3824] 2024-09-29 17:08:31,713 >>   Batch size = 1
192.168.0.25:   0%|          | 0/460 [00:00<?, ?it/s]  0%|          | 2/460 [00:02<09:32,  1.25s/it]  1%|          | 3/460 [00:05<13:29,  1.77s/it]  1%|          | 4/460 [00:07<15:32,  2.04s/it]  1%|          | 5/460 [00:10<16:42,  2.20s/it]  1%|▏         | 6/460 [00:12<17:24,  2.30s/it]  2%|▏         | 7/460 [00:15<17:51,  2.37s/it]  2%|▏         | 8/460 [00:17<18:08,  2.41s/it]  2%|▏         | 9/460 [00:20<18:19,  2.44s/it]  2%|▏         | 10/460 [00:22<18:25,  2.46s/it]  2%|▏         | 11/460 [00:25<18:44,  2.50s/it]  3%|▎         | 12/460 [00:27<18:41,  2.50s/it]  3%|▎         | 13/460 [00:30<18:38,  2.50s/it]  3%|▎         | 14/460 [00:32<18:35,  2.50s/it]  3%|▎         | 15/460 [00:35<18:33,  2.50s/it]  3%|▎         | 16/460 [00:37<18:30,  2.50s/it]  4%|▎         | 17/460 [00:40<18:28,  2.50s/it]  4%|▍         | 18/460 [00:42<18:25,  2.50s/it]  4%|▍         | 19/460 [00:45<18:22,  2.50s/it]  4%|▍         | 20/460 [00:47<18:19,  2.50s/it]  5%|▍         | 21/460 [00:50<18:17,  2.50s/it]  5%|▍         | 22/460 [00:52<18:14,  2.50s/it]  5%|▌         | 23/460 [00:55<18:12,  2.50s/it]  5%|▌         | 24/460 [00:57<18:09,  2.50s/it]  5%|▌         | 25/460 [01:00<18:07,  2.50s/it]  6%|▌         | 26/460 [01:02<18:04,  2.50s/it]  6%|▌         | 27/460 [01:05<18:02,  2.50s/it]  6%|▌         | 28/460 [01:07<18:00,  2.50s/it]  6%|▋         | 29/460 [01:10<17:57,  2.50s/it]  7%|▋         | 30/460 [01:12<17:55,  2.50s/it]  7%|▋         | 31/460 [01:15<17:53,  2.50s/it]  7%|▋         | 32/460 [01:17<17:50,  2.50s/it]  7%|▋         | 33/460 [01:20<17:48,  2.50s/it]  7%|▋         | 34/460 [01:22<17:45,  2.50s/it]  8%|▊         | 35/460 [01:25<17:42,  2.50s/it]  8%|▊         | 36/460 [01:27<17:39,  2.50s/it]  8%|▊         | 37/460 [01:30<17:37,  2.50s/it]  8%|▊         | 38/460 [01:32<17:34,  2.50s/it]  8%|▊         | 39/460 [01:35<17:32,  2.50s/it]  9%|▊         | 40/460 [01:37<17:29,  2.50s/it]  9%|▉         | 41/460 [01:40<17:27,  2.50s/it]  9%|▉         | 42/460 [01:42<17:24,  2.50s/it]  9%|▉         | 43/460 [01:45<17:22,  2.50s/it] 10%|▉         | 44/460 [01:47<17:19,  2.50s/it] 10%|▉         | 45/460 [01:50<17:17,  2.50s/it] 10%|█         | 46/460 [01:52<17:15,  2.50s/it] 10%|█         | 47/460 [01:55<17:13,  2.50s/it] 10%|█         | 48/460 [01:57<17:10,  2.50s/it] 11%|█         | 49/460 [02:00<17:08,  2.50s/it] 11%|█         | 50/460 [02:02<17:05,  2.50s/it] 11%|█         | 51/460 [02:05<17:02,  2.50s/it] 11%|█▏        | 52/460 [02:07<17:00,  2.50s/it] 12%|█▏        | 53/460 [02:10<16:57,  2.50s/it] 12%|█▏        | 54/460 [02:12<16:54,  2.50s/it] 12%|█▏        | 55/460 [02:15<16:52,  2.50s/it] 12%|█▏        | 56/460 [02:17<16:50,  2.50s/it] 12%|█▏        | 57/460 [02:20<16:47,  2.50s/it] 13%|█▎        | 58/460 [02:22<16:45,  2.50s/it] 13%|█▎        | 59/460 [02:25<16:42,  2.50s/it] 13%|█▎        | 60/460 [02:27<16:40,  2.50s/it] 13%|█▎        | 61/460 [02:30<16:37,  2.50s/it] 13%|█▎        | 62/460 [02:32<16:35,  2.50s/it] 14%|█▎        | 63/460 [02:35<16:33,  2.50s/it] 14%|█▍        | 64/460 [02:37<16:30,  2.50s/it] 14%|█▍        | 65/460 [02:40<16:28,  2.50s/it] 14%|█▍        | 66/460 [02:42<16:25,  2.50s/it] 15%|█▍        | 67/460 [02:45<16:22,  2.50s/it] 15%|█▍        | 68/460 [02:47<16:20,  2.50s/it] 15%|█▌        | 69/460 [02:50<16:17,  2.50s/it] 15%|█▌        | 70/460 [02:52<16:15,  2.50s/it] 15%|█▌        | 71/460 [02:55<16:12,  2.50s/it] 16%|█▌        | 72/460 [02:57<16:09,  2.50s/it] 16%|█▌        | 73/460 [03:00<16:07,  2.50s/it] 16%|█▌        | 74/460 [03:02<16:04,  2.50s/it] 16%|█▋        | 75/460 [03:05<16:02,  2.50s/it] 17%|█▋        | 76/460 [03:07<15:59,  2.50s/it] 17%|█▋        | 77/460 [03:10<15:57,  2.50s/it] 17%|█▋        | 78/460 [03:12<15:54,  2.50s/it] 17%|█▋        | 79/460 [03:15<15:52,  2.50s/it] 17%|█▋        | 80/460 [03:17<15:49,  2.50s/it] 18%|█▊        | 81/460 [03:20<15:47,  2.50s/it] 18%|█▊        | 82/460 [03:22<15:44,  2.50s/it] 18%|█▊        | 83/460 [03:25<15:42,  2.50s/it] 18%|█▊        | 84/460 [03:27<15:40,  2.50s/it] 18%|█▊        | 85/460 [03:30<15:37,  2.50s/it] 19%|█▊        | 86/460 [03:32<15:35,  2.50s/it] 19%|█▉        | 87/460 [03:35<15:32,  2.50s/it] 19%|█▉        | 88/460 [03:37<15:30,  2.50s/it] 19%|█▉        | 89/460 [03:40<15:27,  2.50s/it] 20%|█▉        | 90/460 [03:42<15:25,  2.50s/it] 20%|█▉        | 91/460 [03:45<15:22,  2.50s/it] 20%|██        | 92/460 [03:47<15:19,  2.50s/it] 20%|██        | 93/460 [03:50<15:17,  2.50s/it] 20%|██        | 94/460 [03:52<15:15,  2.50s/it] 21%|██        | 95/460 [03:55<15:13,  2.50s/it] 21%|██        | 96/460 [03:57<15:10,  2.50s/it] 21%|██        | 97/460 [04:00<15:07,  2.50s/it] 21%|██▏       | 98/460 [04:02<15:05,  2.50s/it] 22%|██▏       | 99/460 [04:05<15:02,  2.50s/it] 22%|██▏       | 100/460 [04:07<15:00,  2.50s/it] 22%|██▏       | 101/460 [04:10<14:57,  2.50s/it] 22%|██▏       | 102/460 [04:12<14:55,  2.50s/it] 22%|██▏       | 103/460 [04:15<14:52,  2.50s/it] 23%|██▎       | 104/460 [04:17<14:50,  2.50s/it] 23%|██▎       | 105/460 [04:20<14:47,  2.50s/it] 23%|██▎       | 106/460 [04:22<14:45,  2.50s/it] 23%|██▎       | 107/460 [04:25<14:42,  2.50s/it] 23%|██▎       | 108/460 [04:27<14:40,  2.50s/it] 24%|██▎       | 109/460 [04:30<14:37,  2.50s/it] 24%|██▍       | 110/460 [04:32<14:35,  2.50s/it] 24%|██▍       | 111/460 [04:35<14:32,  2.50s/it] 24%|██▍       | 112/460 [04:37<14:30,  2.50s/it] 25%|██▍       | 113/460 [04:40<14:27,  2.50s/it] 25%|██▍       | 114/460 [04:42<14:25,  2.50s/it] 25%|██▌       | 115/460 [04:45<14:22,  2.50s/it] 25%|██▌       | 116/460 [04:47<14:20,  2.50s/it] 25%|██▌       | 117/460 [04:50<14:18,  2.50s/it] 26%|██▌       | 118/460 [04:52<14:15,  2.50s/it] 26%|██▌       | 119/460 [04:55<14:12,  2.50s/it] 26%|██▌       | 120/460 [04:57<14:10,  2.50s/it] 26%|██▋       | 121/460 [05:00<14:07,  2.50s/it] 27%|██▋       | 122/460 [05:02<14:05,  2.50s/it] 27%|██▋       | 123/460 [05:05<14:02,  2.50s/it] 27%|██▋       | 124/460 [05:07<14:00,  2.50s/it] 27%|██▋       | 125/460 [05:10<13:57,  2.50s/it] 27%|██▋       | 126/460 [05:12<13:55,  2.50s/it] 28%|██▊       | 127/460 [05:15<13:53,  2.50s/it] 28%|██▊       | 128/460 [05:17<13:50,  2.50s/it] 28%|██▊       | 129/460 [05:20<13:48,  2.50s/it] 28%|██▊       | 130/460 [05:22<13:45,  2.50s/it] 28%|██▊       | 131/460 [05:25<13:42,  2.50s/it] 29%|██▊       | 132/460 [05:27<13:40,  2.50s/it] 29%|██▉       | 133/460 [05:30<13:38,  2.50s/it] 29%|██▉       | 134/460 [05:32<13:35,  2.50s/it] 29%|██▉       | 135/460 [05:35<13:32,  2.50s/it] 30%|██▉       | 136/460 [05:37<13:30,  2.50s/it] 30%|██▉       | 137/460 [05:40<13:27,  2.50s/it] 30%|███       | 138/460 [05:42<13:25,  2.50s/it] 30%|███       | 139/460 [05:45<13:22,  2.50s/it] 30%|███       | 140/460 [05:47<13:19,  2.50s/it] 31%|███       | 141/460 [05:50<13:17,  2.50s/it] 31%|███       | 142/460 [05:52<13:14,  2.50s/it] 31%|███       | 143/460 [05:55<13:12,  2.50s/it] 31%|███▏      | 144/460 [05:57<13:09,  2.50s/it] 32%|███▏      | 145/460 [06:00<13:07,  2.50s/it] 32%|███▏      | 146/460 [06:02<13:04,  2.50s/it] 32%|███▏      | 147/460 [06:05<13:02,  2.50s/it] 32%|███▏      | 148/460 [06:07<13:00,  2.50s/it] 32%|███▏      | 149/460 [06:10<12:57,  2.50s/it] 33%|███▎      | 150/460 [06:12<12:55,  2.50s/it] 33%|███▎      | 151/460 [06:15<12:52,  2.50s/it] 33%|███▎      | 152/460 [06:17<12:50,  2.50s/it] 33%|███▎      | 153/460 [06:20<12:47,  2.50s/it] 33%|███▎      | 154/460 [06:22<12:45,  2.50s/it] 34%|███▎      | 155/460 [06:25<12:42,  2.50s/it] 34%|███▍      | 156/460 [06:27<12:40,  2.50s/it] 34%|███▍      | 157/460 [06:30<12:37,  2.50s/it] 34%|███▍      | 158/460 [06:32<12:35,  2.50s/it] 35%|███▍      | 159/460 [06:35<12:32,  2.50s/it] 35%|███▍      | 160/460 [06:37<12:30,  2.50s/it] 35%|███▌      | 161/460 [06:40<12:27,  2.50s/it] 35%|███▌      | 162/460 [06:42<12:25,  2.50s/it] 35%|███▌      | 163/460 [06:45<12:22,  2.50s/it] 36%|███▌      | 164/460 [06:47<12:20,  2.50s/it] 36%|███▌      | 165/460 [06:50<12:17,  2.50s/it] 36%|███▌      | 166/460 [06:52<12:15,  2.50s/it] 36%|███▋      | 167/460 [06:55<12:12,  2.50s/it] 37%|███▋      | 168/460 [06:57<12:10,  2.50s/it] 37%|███▋      | 169/460 [07:00<12:07,  2.50s/it] 37%|███▋      | 170/460 [07:02<12:04,  2.50s/it] 37%|███▋      | 171/460 [07:05<12:02,  2.50s/it] 37%|███▋      | 172/460 [07:07<11:59,  2.50s/it] 38%|███▊      | 173/460 [07:10<11:57,  2.50s/it] 38%|███▊      | 174/460 [07:12<11:54,  2.50s/it] 38%|███▊      | 175/460 [07:15<11:52,  2.50s/it] 38%|███▊      | 176/460 [07:17<11:50,  2.50s/it] 38%|███▊      | 177/460 [07:20<11:47,  2.50s/it] 39%|███▊      | 178/460 [07:22<11:44,  2.50s/it] 39%|███▉      | 179/460 [07:25<11:42,  2.50s/it] 39%|███▉      | 180/460 [07:27<11:39,  2.50s/it] 39%|███▉      | 181/460 [07:30<11:37,  2.50s/it] 40%|███▉      | 182/460 [07:32<11:35,  2.50s/it] 40%|███▉      | 183/460 [07:35<11:32,  2.50s/it] 40%|████      | 184/460 [07:37<11:29,  2.50s/it] 40%|████      | 185/460 [07:40<11:27,  2.50s/it] 40%|████      | 186/460 [07:42<11:24,  2.50s/it] 41%|████      | 187/460 [07:45<11:22,  2.50s/it] 41%|████      | 188/460 [07:47<11:19,  2.50s/it] 41%|████      | 189/460 [07:50<11:17,  2.50s/it] 41%|████▏     | 190/460 [07:52<11:15,  2.50s/it] 42%|████▏     | 191/460 [07:55<11:12,  2.50s/it] 42%|████▏     | 192/460 [07:57<11:10,  2.50s/it] 42%|████▏     | 193/460 [08:00<11:07,  2.50s/it] 42%|████▏     | 194/460 [08:02<11:04,  2.50s/it] 42%|████▏     | 195/460 [08:05<11:02,  2.50s/it] 43%|████▎     | 196/460 [08:07<10:59,  2.50s/it] 43%|████▎     | 197/460 [08:10<10:57,  2.50s/it] 43%|████▎     | 198/460 [08:12<10:55,  2.50s/it] 43%|████▎     | 199/460 [08:15<10:52,  2.50s/it] 43%|████▎     | 200/460 [08:17<10:50,  2.50s/it] 44%|████▎     | 201/460 [08:20<10:47,  2.50s/it] 44%|████▍     | 202/460 [08:22<10:45,  2.50s/it] 44%|████▍     | 203/460 [08:25<10:42,  2.50s/it] 44%|████▍     | 204/460 [08:27<10:40,  2.50s/it] 45%|████▍     | 205/460 [08:30<10:37,  2.50s/it] 45%|████▍     | 206/460 [08:32<10:34,  2.50s/it] 45%|████▌     | 207/460 [08:35<10:32,  2.50s/it] 45%|████▌     | 208/460 [08:37<10:29,  2.50s/it] 45%|████▌     | 209/460 [08:40<10:27,  2.50s/it] 46%|████▌     | 210/460 [08:42<10:24,  2.50s/it] 46%|████▌     | 211/460 [08:45<10:22,  2.50s/it] 46%|████▌     | 212/460 [08:47<10:19,  2.50s/it] 46%|████▋     | 213/460 [08:50<10:17,  2.50s/it] 47%|████▋     | 214/460 [08:52<10:14,  2.50s/it] 47%|████▋     | 215/460 [08:55<10:12,  2.50s/it] 47%|████▋     | 216/460 [08:57<10:09,  2.50s/it] 47%|████▋     | 217/460 [09:00<10:07,  2.50s/it] 47%|████▋     | 218/460 [09:02<10:04,  2.50s/it] 48%|████▊     | 219/460 [09:05<10:02,  2.50s/it] 48%|████▊     | 220/460 [09:07<09:59,  2.50s/it] 48%|████▊     | 221/460 [09:10<09:57,  2.50s/it] 48%|████▊     | 222/460 [09:12<09:54,  2.50s/it] 48%|████▊     | 223/460 [09:15<09:52,  2.50s/it] 49%|████▊     | 224/460 [09:17<09:50,  2.50s/it] 49%|████▉     | 225/460 [09:20<09:47,  2.50s/it] 49%|████▉     | 226/460 [09:22<09:45,  2.50s/it] 49%|████▉     | 227/460 [09:25<09:42,  2.50s/it] 50%|████▉     | 228/460 [09:27<09:40,  2.50s/it] 50%|████▉     | 229/460 [09:30<09:37,  2.50s/it] 50%|█████     | 230/460 [09:32<09:35,  2.50s/it] 50%|█████     | 231/460 [09:35<09:32,  2.50s/it] 50%|█████     | 232/460 [09:37<09:30,  2.50s/it] 51%|█████     | 233/460 [09:40<09:27,  2.50s/it] 51%|█████     | 234/460 [09:42<09:24,  2.50s/it] 51%|█████     | 235/460 [09:45<09:22,  2.50s/it] 51%|█████▏    | 236/460 [09:47<09:19,  2.50s/it] 52%|█████▏    | 237/460 [09:50<09:17,  2.50s/it] 52%|█████▏    | 238/460 [09:52<09:15,  2.50s/it] 52%|█████▏    | 239/460 [09:55<09:12,  2.50s/it] 52%|█████▏    | 240/460 [09:57<09:10,  2.50s/it] 52%|█████▏    | 241/460 [10:00<09:07,  2.50s/it] 53%|█████▎    | 242/460 [10:02<09:05,  2.50s/it] 53%|█████▎    | 243/460 [10:05<09:02,  2.50s/it] 53%|█████▎    | 244/460 [10:07<09:00,  2.50s/it] 53%|█████▎    | 245/460 [10:10<08:57,  2.50s/it] 53%|█████▎    | 246/460 [10:12<08:55,  2.50s/it] 54%|█████▎    | 247/460 [10:15<08:52,  2.50s/it] 54%|█████▍    | 248/460 [10:17<08:50,  2.50s/it] 54%|█████▍    | 249/460 [10:20<08:47,  2.50s/it] 54%|█████▍    | 250/460 [10:22<08:45,  2.50s/it] 55%|█████▍    | 251/460 [10:25<08:42,  2.50s/it] 55%|█████▍    | 252/460 [10:27<08:40,  2.50s/it] 55%|█████▌    | 253/460 [10:30<08:37,  2.50s/it] 55%|█████▌    | 254/460 [10:32<08:35,  2.50s/it] 55%|█████▌    | 255/460 [10:35<08:32,  2.50s/it] 56%|█████▌    | 256/460 [10:37<08:30,  2.50s/it] 56%|█████▌    | 257/460 [10:40<08:27,  2.50s/it] 56%|█████▌    | 258/460 [10:42<08:24,  2.50s/it] 56%|█████▋    | 259/460 [10:45<08:22,  2.50s/it] 57%|█████▋    | 260/460 [10:47<08:19,  2.50s/it] 57%|█████▋    | 261/460 [10:50<08:17,  2.50s/it] 57%|█████▋    | 262/460 [10:52<08:15,  2.50s/it] 57%|█████▋    | 263/460 [10:55<08:12,  2.50s/it] 57%|█████▋    | 264/460 [10:57<08:10,  2.50s/it] 58%|█████▊    | 265/460 [11:00<08:07,  2.50s/it] 58%|█████▊    | 266/460 [11:02<08:05,  2.50s/it] 58%|█████▊    | 267/460 [11:05<08:02,  2.50s/it] 58%|█████▊    | 268/460 [11:07<08:00,  2.50s/it] 58%|█████▊    | 269/460 [11:10<07:57,  2.50s/it] 59%|█████▊    | 270/460 [11:12<07:55,  2.50s/it] 59%|█████▉    | 271/460 [11:15<07:52,  2.50s/it] 59%|█████▉    | 272/460 [11:17<07:50,  2.50s/it] 59%|█████▉    | 273/460 [11:20<07:47,  2.50s/it] 60%|█████▉    | 274/460 [11:22<07:45,  2.50s/it] 60%|█████▉    | 275/460 [11:25<07:42,  2.50s/it] 60%|██████    | 276/460 [11:27<07:40,  2.50s/it] 60%|██████    | 277/460 [11:30<07:37,  2.50s/it] 60%|██████    | 278/460 [11:32<07:35,  2.50s/it] 61%|██████    | 279/460 [11:35<07:32,  2.50s/it] 61%|██████    | 280/460 [11:37<07:30,  2.50s/it] 61%|██████    | 281/460 [11:40<07:27,  2.50s/it] 61%|██████▏   | 282/460 [11:42<07:25,  2.50s/it] 62%|██████▏   | 283/460 [11:45<07:22,  2.50s/it] 62%|██████▏   | 284/460 [11:47<07:19,  2.50s/it] 62%|██████▏   | 285/460 [11:50<07:17,  2.50s/it] 62%|██████▏   | 286/460 [11:52<07:15,  2.50s/it] 62%|██████▏   | 287/460 [11:55<07:12,  2.50s/it] 63%|██████▎   | 288/460 [11:57<07:09,  2.50s/it] 63%|██████▎   | 289/460 [12:00<07:07,  2.50s/it] 63%|██████▎   | 290/460 [12:02<07:04,  2.50s/it] 63%|██████▎   | 291/460 [12:05<07:02,  2.50s/it] 63%|██████▎   | 292/460 [12:07<06:59,  2.50s/it] 64%|██████▎   | 293/460 [12:10<06:57,  2.50s/it] 64%|██████▍   | 294/460 [12:12<06:55,  2.50s/it] 64%|██████▍   | 295/460 [12:15<06:52,  2.50s/it] 64%|██████▍   | 296/460 [12:17<06:50,  2.50s/it] 65%|██████▍   | 297/460 [12:20<06:47,  2.50s/it] 65%|██████▍   | 298/460 [12:22<06:45,  2.50s/it] 65%|██████▌   | 299/460 [12:25<06:42,  2.50s/it] 65%|██████▌   | 300/460 [12:27<06:40,  2.50s/it] 65%|██████▌   | 301/460 [12:30<06:37,  2.50s/it] 66%|██████▌   | 302/460 [12:32<06:35,  2.50s/it] 66%|██████▌   | 303/460 [12:35<06:32,  2.50s/it] 66%|██████▌   | 304/460 [12:37<06:30,  2.50s/it] 66%|██████▋   | 305/460 [12:40<06:27,  2.50s/it] 67%|██████▋   | 306/460 [12:42<06:25,  2.50s/it] 67%|██████▋   | 307/460 [12:45<06:22,  2.50s/it] 67%|██████▋   | 308/460 [12:47<06:19,  2.50s/it] 67%|██████▋   | 309/460 [12:50<06:17,  2.50s/it] 67%|██████▋   | 310/460 [12:52<06:14,  2.50s/it] 68%|██████▊   | 311/460 [12:55<06:12,  2.50s/it] 68%|██████▊   | 312/460 [12:57<06:10,  2.50s/it] 68%|██████▊   | 313/460 [13:00<06:07,  2.50s/it] 68%|██████▊   | 314/460 [13:02<06:04,  2.50s/it] 68%|██████▊   | 315/460 [13:05<06:02,  2.50s/it] 69%|██████▊   | 316/460 [13:07<06:00,  2.50s/it] 69%|██████▉   | 317/460 [13:10<05:57,  2.50s/it] 69%|██████▉   | 318/460 [13:12<05:55,  2.50s/it] 69%|██████▉   | 319/460 [13:15<05:52,  2.50s/it] 70%|██████▉   | 320/460 [13:17<05:50,  2.50s/it] 70%|██████▉   | 321/460 [13:20<05:47,  2.50s/it] 70%|███████   | 322/460 [13:22<05:45,  2.50s/it] 70%|███████   | 323/460 [13:25<05:42,  2.50s/it] 70%|███████   | 324/460 [13:27<05:39,  2.50s/it] 71%|███████   | 325/460 [13:30<05:37,  2.50s/it] 71%|███████   | 326/460 [13:32<05:35,  2.50s/it] 71%|███████   | 327/460 [13:35<05:32,  2.50s/it] 71%|███████▏  | 328/460 [13:37<05:29,  2.50s/it] 72%|███████▏  | 329/460 [13:40<05:27,  2.50s/it] 72%|███████▏  | 330/460 [13:42<05:24,  2.50s/it] 72%|███████▏  | 331/460 [13:45<05:22,  2.50s/it] 72%|███████▏  | 332/460 [13:47<05:19,  2.50s/it] 72%|███████▏  | 333/460 [13:50<05:17,  2.50s/it] 73%|███████▎  | 334/460 [13:52<05:15,  2.50s/it] 73%|███████▎  | 335/460 [13:55<05:12,  2.50s/it] 73%|███████▎  | 336/460 [13:57<05:10,  2.50s/it] 73%|███████▎  | 337/460 [14:00<05:07,  2.50s/it] 73%|███████▎  | 338/460 [14:02<05:05,  2.50s/it] 74%|███████▎  | 339/460 [14:05<05:02,  2.50s/it] 74%|███████▍  | 340/460 [14:07<05:00,  2.50s/it] 74%|███████▍  | 341/460 [14:10<04:57,  2.50s/it] 74%|███████▍  | 342/460 [14:12<04:55,  2.50s/it] 75%|███████▍  | 343/460 [14:15<04:52,  2.50s/it] 75%|███████▍  | 344/460 [14:17<04:50,  2.50s/it] 75%|███████▌  | 345/460 [14:20<04:47,  2.50s/it] 75%|███████▌  | 346/460 [14:22<04:45,  2.50s/it] 75%|███████▌  | 347/460 [14:25<04:42,  2.50s/it] 76%|███████▌  | 348/460 [14:27<04:40,  2.50s/it] 76%|███████▌  | 349/460 [14:30<04:37,  2.50s/it] 76%|███████▌  | 350/460 [14:32<04:35,  2.50s/it] 76%|███████▋  | 351/460 [14:35<04:32,  2.50s/it] 77%|███████▋  | 352/460 [14:37<04:30,  2.50s/it] 77%|███████▋  | 353/460 [14:40<04:27,  2.50s/it] 77%|███████▋  | 354/460 [14:42<04:25,  2.50s/it] 77%|███████▋  | 355/460 [14:45<04:22,  2.50s/it] 77%|███████▋  | 356/460 [14:47<04:20,  2.50s/it] 78%|███████▊  | 357/460 [14:50<04:17,  2.50s/it] 78%|███████▊  | 358/460 [14:52<04:15,  2.50s/it] 78%|███████▊  | 359/460 [14:55<04:12,  2.50s/it] 78%|███████▊  | 360/460 [14:57<04:09,  2.50s/it] 78%|███████▊  | 361/460 [15:00<04:07,  2.50s/it] 79%|███████▊  | 362/460 [15:02<04:04,  2.50s/it] 79%|███████▉  | 363/460 [15:05<04:02,  2.50s/it] 79%|███████▉  | 364/460 [15:07<03:59,  2.50s/it] 79%|███████▉  | 365/460 [15:10<03:57,  2.50s/it] 80%|███████▉  | 366/460 [15:12<03:54,  2.50s/it] 80%|███████▉  | 367/460 [15:15<03:52,  2.50s/it] 80%|████████  | 368/460 [15:17<03:50,  2.50s/it] 80%|████████  | 369/460 [15:20<03:47,  2.50s/it] 80%|████████  | 370/460 [15:22<03:44,  2.50s/it] 81%|████████  | 371/460 [15:25<03:42,  2.50s/it] 81%|████████  | 372/460 [15:27<03:39,  2.50s/it] 81%|████████  | 373/460 [15:30<03:37,  2.50s/it] 81%|████████▏ | 374/460 [15:32<03:34,  2.50s/it] 82%|████████▏ | 375/460 [15:35<03:32,  2.50s/it] 82%|████████▏ | 376/460 [15:37<03:30,  2.50s/it] 82%|████████▏ | 377/460 [15:40<03:27,  2.50s/it] 82%|████████▏ | 378/460 [15:42<03:25,  2.50s/it] 82%|████████▏ | 379/460 [15:45<03:22,  2.50s/it] 83%|████████▎ | 380/460 [15:47<03:19,  2.50s/it] 83%|████████▎ | 381/460 [15:50<03:17,  2.50s/it] 83%|████████▎ | 382/460 [15:52<03:15,  2.50s/it] 83%|████████▎ | 383/460 [15:55<03:12,  2.50s/it] 83%|████████▎ | 384/460 [15:57<03:09,  2.50s/it] 84%|████████▎ | 385/460 [16:00<03:07,  2.50s/it] 84%|████████▍ | 386/460 [16:02<03:04,  2.50s/it] 84%|████████▍ | 387/460 [16:05<03:02,  2.50s/it] 84%|████████▍ | 388/460 [16:07<02:59,  2.50s/it] 85%|████████▍ | 389/460 [16:10<02:57,  2.50s/it] 85%|████████▍ | 390/460 [16:12<02:54,  2.50s/it] 85%|████████▌ | 391/460 [16:15<02:52,  2.50s/it] 85%|████████▌ | 392/460 [16:17<02:49,  2.50s/it] 85%|████████▌ | 393/460 [16:20<02:47,  2.50s/it] 86%|████████▌ | 394/460 [16:22<02:44,  2.50s/it] 86%|████████▌ | 395/460 [16:25<02:42,  2.50s/it] 86%|████████▌ | 396/460 [16:27<02:39,  2.50s/it] 86%|████████▋ | 397/460 [16:30<02:37,  2.50s/it] 87%|████████▋ | 398/460 [16:32<02:34,  2.50s/it] 87%|████████▋ | 399/460 [16:35<02:32,  2.50s/it] 87%|████████▋ | 400/460 [16:37<02:29,  2.50s/it] 87%|████████▋ | 401/460 [16:40<02:27,  2.50s/it] 87%|████████▋ | 402/460 [16:42<02:25,  2.50s/it] 88%|████████▊ | 403/460 [16:45<02:22,  2.50s/it] 88%|████████▊ | 404/460 [16:47<02:19,  2.50s/it] 88%|████████▊ | 405/460 [16:50<02:17,  2.50s/it] 88%|████████▊ | 406/460 [16:52<02:14,  2.50s/it] 88%|████████▊ | 407/460 [16:55<02:12,  2.50s/it] 89%|████████▊ | 408/460 [16:57<02:09,  2.50s/it] 89%|████████▉ | 409/460 [17:00<02:07,  2.50s/it] 89%|████████▉ | 410/460 [17:02<02:04,  2.50s/it] 89%|████████▉ | 411/460 [17:05<02:02,  2.50s/it] 90%|████████▉ | 412/460 [17:07<02:00,  2.50s/it] 90%|████████▉ | 413/460 [17:10<01:57,  2.50s/it] 90%|█████████ | 414/460 [17:12<01:55,  2.50s/it] 90%|█████████ | 415/460 [17:15<01:52,  2.50s/it] 90%|█████████ | 416/460 [17:17<01:50,  2.50s/it] 91%|█████████ | 417/460 [17:20<01:47,  2.50s/it] 91%|█████████ | 418/460 [17:22<01:45,  2.50s/it] 91%|█████████ | 419/460 [17:25<01:42,  2.50s/it] 91%|█████████▏| 420/460 [17:27<01:40,  2.50s/it] 92%|█████████▏| 421/460 [17:30<01:37,  2.50s/it] 92%|█████████▏| 422/460 [17:32<01:35,  2.50s/it] 92%|█████████▏| 423/460 [17:35<01:32,  2.50s/it] 92%|█████████▏| 424/460 [17:37<01:30,  2.50s/it] 92%|█████████▏| 425/460 [17:40<01:27,  2.50s/it] 93%|█████████▎| 426/460 [17:42<01:25,  2.50s/it] 93%|█████████▎| 427/460 [17:45<01:22,  2.50s/it] 93%|█████████▎| 428/460 [17:47<01:20,  2.50s/it] 93%|█████████▎| 429/460 [17:50<01:17,  2.50s/it] 93%|█████████▎| 430/460 [17:52<01:15,  2.50s/it] 94%|█████████▎| 431/460 [17:55<01:12,  2.50s/it] 94%|█████████▍| 432/460 [17:57<01:10,  2.50s/it] 94%|█████████▍| 433/460 [18:00<01:07,  2.50s/it] 94%|█████████▍| 434/460 [18:02<01:05,  2.50s/it] 95%|█████████▍| 435/460 [18:05<01:02,  2.50s/it] 95%|█████████▍| 436/460 [18:07<01:00,  2.50s/it] 95%|█████████▌| 437/460 [18:10<00:57,  2.50s/it] 95%|█████████▌| 438/460 [18:12<00:54,  2.50s/it] 95%|█████████▌| 439/460 [18:15<00:52,  2.50s/it] 96%|█████████▌| 440/460 [18:17<00:50,  2.50s/it] 96%|█████████▌| 441/460 [18:20<00:47,  2.50s/it] 96%|█████████▌| 442/460 [18:22<00:44,  2.50s/it] 96%|█████████▋| 443/460 [18:25<00:42,  2.50s/it] 97%|█████████▋| 444/460 [18:27<00:39,  2.50s/it] 97%|█████████▋| 445/460 [18:30<00:37,  2.50s/it] 97%|█████████▋| 446/460 [18:32<00:34,  2.50s/it] 97%|█████████▋| 447/460 [18:35<00:32,  2.50s/it] 97%|█████████▋| 448/460 [18:37<00:29,  2.50s/it] 98%|█████████▊| 449/460 [18:40<00:27,  2.50s/it] 98%|█████████▊| 450/460 [18:42<00:24,  2.50s/it] 98%|█████████▊| 451/460 [18:45<00:22,  2.50s/it] 98%|█████████▊| 452/460 [18:47<00:19,  2.50s/it] 98%|█████████▊| 453/460 [18:50<00:17,  2.50s/it] 99%|█████████▊| 454/460 [18:52<00:14,  2.50s/it] 99%|█████████▉| 455/460 [18:55<00:12,  2.50s/it] 99%|█████████▉| 456/460 [18:57<00:09,  2.50s/it] 99%|█████████▉| 457/460 [19:00<00:07,  2.50s/it]100%|█████████▉| 458/460 [19:02<00:04,  2.50s/it]100%|█████████▉| 459/460 [19:05<00:02,  2.50s/it]100%|██████████| 460/460 [19:07<00:00,  2.50s/it]100%|██████████| 460/460 [19:07<00:00,  2.50s/it]
192.168.0.25: ***** eval metrics *****
192.168.0.25:   epoch                   =        3.0
192.168.0.25:   eval_loss               =     3.2334
192.168.0.25:   eval_runtime            = 0:19:10.25
192.168.0.25:   eval_samples_per_second =     12.787
192.168.0.25:   eval_steps_per_second   =        0.4
192.168.0.25:   perplexity              =    25.3665
192.168.0.25: [INFO|modelcard.py:449] 2024-09-29 17:27:41,969 >> Dropping the following result as it does not have all the necessary fields:
192.168.0.25: {'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
192.168.0.25: /root/miniconda3/envs/protein/lib/python3.10/tempfile.py:869: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpng4cd1vh'>
192.168.0.25:   _warnings.warn(warn_message, ResourceWarning)
192.168.0.25: /root/miniconda3/envs/protein/lib/python3.10/tempfile.py:869: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmppfbzb09g'>
192.168.0.25:   _warnings.warn(warn_message, ResourceWarning)
192.168.0.25: /root/miniconda3/envs/protein/lib/python3.10/tempfile.py:869: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpymj30f99'>
192.168.0.25:   _warnings.warn(warn_message, ResourceWarning)
192.168.0.25: /root/miniconda3/envs/protein/lib/python3.10/tempfile.py:869: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpgl4yqvb1'>
192.168.0.25:   _warnings.warn(warn_message, ResourceWarning)
192.168.0.25: /root/miniconda3/envs/protein/lib/python3.10/tempfile.py:869: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpskl3le4h'>
192.168.0.25:   _warnings.warn(warn_message, ResourceWarning)
192.168.0.25: /root/miniconda3/envs/protein/lib/python3.10/tempfile.py:869: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpbncufmyy'>
192.168.0.25:   _warnings.warn(warn_message, ResourceWarning)
192.168.0.25: /root/miniconda3/envs/protein/lib/python3.10/tempfile.py:869: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpl35lu7jy'>
192.168.0.25:   _warnings.warn(warn_message, ResourceWarning)
192.168.0.25: /root/miniconda3/envs/protein/lib/python3.10/tempfile.py:869: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpp6ht11c1'>
192.168.0.25:   _warnings.warn(warn_message, ResourceWarning)
192.168.0.13: [2024-09-29 17:27:53,491] [INFO] [launch.py:351:main] Process 85542 exits successfully.
192.168.0.149: [2024-09-29 17:27:56,111] [INFO] [launch.py:351:main] Process 175640 exits successfully.
192.168.0.89: [2024-09-29 17:27:38,246] [INFO] [launch.py:351:main] Process 24160 exits successfully.
192.168.0.89: [2024-09-29 17:27:38,247] [INFO] [launch.py:351:main] Process 24153 exits successfully.
192.168.0.89: [2024-09-29 17:27:38,247] [INFO] [launch.py:351:main] Process 24156 exits successfully.
192.168.0.89: [2024-09-29 17:27:38,247] [INFO] [launch.py:351:main] Process 24154 exits successfully.
192.168.0.25: [2024-09-29 17:27:54,009] [INFO] [launch.py:351:main] Process 50919 exits successfully.
192.168.0.13: [2024-09-29 17:27:54,492] [INFO] [launch.py:351:main] Process 85538 exits successfully.
192.168.0.13: [2024-09-29 17:27:54,493] [INFO] [launch.py:351:main] Process 85541 exits successfully.
192.168.0.149: [2024-09-29 17:27:57,113] [INFO] [launch.py:351:main] Process 175642 exits successfully.
192.168.0.89: [2024-09-29 17:27:39,248] [INFO] [launch.py:351:main] Process 24158 exits successfully.
192.168.0.89: [2024-09-29 17:27:39,249] [INFO] [launch.py:351:main] Process 24155 exits successfully.
192.168.0.89: [2024-09-29 17:27:39,249] [INFO] [launch.py:351:main] Process 24157 exits successfully.
192.168.0.89: [2024-09-29 17:27:39,249] [INFO] [launch.py:351:main] Process 24159 exits successfully.
192.168.0.25: [2024-09-29 17:27:55,010] [INFO] [launch.py:351:main] Process 50922 exits successfully.
192.168.0.25: [2024-09-29 17:27:55,010] [INFO] [launch.py:351:main] Process 50920 exits successfully.
192.168.0.25: [2024-09-29 17:27:55,011] [INFO] [launch.py:351:main] Process 50917 exits successfully.
192.168.0.13: [2024-09-29 17:27:55,494] [INFO] [launch.py:351:main] Process 85543 exits successfully.
192.168.0.13: [2024-09-29 17:27:55,494] [INFO] [launch.py:351:main] Process 85536 exits successfully.
192.168.0.13: [2024-09-29 17:27:55,495] [INFO] [launch.py:351:main] Process 85540 exits successfully.
192.168.0.149: [2024-09-29 17:27:58,114] [INFO] [launch.py:351:main] Process 175636 exits successfully.
192.168.0.149: [2024-09-29 17:27:58,115] [INFO] [launch.py:351:main] Process 175637 exits successfully.
192.168.0.149: [2024-09-29 17:27:58,115] [INFO] [launch.py:351:main] Process 175635 exits successfully.
192.168.0.149: [2024-09-29 17:27:58,115] [INFO] [launch.py:351:main] Process 175641 exits successfully.
192.168.0.149: [2024-09-29 17:27:58,115] [INFO] [launch.py:351:main] Process 175639 exits successfully.
192.168.0.149: [2024-09-29 17:27:58,115] [INFO] [launch.py:351:main] Process 175638 exits successfully.
192.168.0.25: [2024-09-29 17:27:56,011] [INFO] [launch.py:351:main] Process 50921 exits successfully.
192.168.0.25: [2024-09-29 17:27:56,012] [INFO] [launch.py:351:main] Process 50918 exits successfully.
192.168.0.25: [2024-09-29 17:27:56,012] [INFO] [launch.py:351:main] Process 50916 exits successfully.
192.168.0.13: [2024-09-29 17:27:56,495] [INFO] [launch.py:351:main] Process 85539 exits successfully.
192.168.0.13: [2024-09-29 17:27:56,495] [INFO] [launch.py:351:main] Process 85537 exits successfully.
192.168.0.25: [2024-09-29 17:27:57,013] [INFO] [launch.py:351:main] Process 50923 exits successfully.
